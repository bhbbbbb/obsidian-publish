{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Obsidian Notes \u00b6 Check my github for more information.","title":"Obsidian Notes"},{"location":"#obsidian+notes","text":"Check my github for more information.","title":"Obsidian Notes"},{"location":"About%20Me/","text":"Personal Information \u00b6 Name \u6797\u5c11\u6069 Dept. \u570b\u7acb\u6210\u529f\u5927\u5b78\u96fb\u6a5f\u5de5\u7a0b\u5b78\u7cfb(~2023-06) Email. smark9005310@gmail.com Github https://github.com/bhbbbbb Research Interests Machine Learning Deep Learning Computer Vision Audio Recognition Natural Language Processing Social Network Analysis Competitions \u00b6 \u96fb\u6a5f\u7cfb\u8a08\u7b97\u6a5f\u6982\u8ad6\u76c3 Blokus \u7b56\u7565\u7a0b\u5f0f\u8a2d\u8a08\u7af6\u8cfd \u5b63\u8ecd (2020-05) \u5927\u5c08\u6821\u9662\u8cc7\u8a0a\u61c9\u7528\u670d\u52d9\u5275\u65b0\u7af6\u8cfd \u4f73\u4f5c (2020-10) We developed a route-recommendation system considering factors including but not limited to traffic, weather, disease(\u767b\u9769\u71b1), user preference. In this project, I was in charge of designing and implementing algorithm for shortest path problem, and data collecting and visualizing data for \u767b\u9769\u71b1. Kaggle \u2013 BirdClef (2022-03) BirdClef, audio classification task, aims to distinguish between bird chirping of various species. I was not familiar to neither NLP nor audio recognition before participating this competition. At that time, I simply adapted ViT (Vision Transformer) to audio version and thus only got average grade. However, I started to study in NLP and audio recognition and further influenced my personal research Music Popularity Prediction . AI CUP 2022 \u80ba\u817a\u764c\u75c5\u7406\u5207\u7247\u5f71\u50cf\u4e4b\u816b\u7624\u6c23\u9053\u64f4\u6563\u5075\u6e2c\u7af6\u8cfd II\uff1a\u904b\u7528\u5f71\u50cf\u5206\u5272\u4f5c\u6cd5\u65bc\u5207\u5272STAS\u8f2a\u5ed3 (2022-04) \uff0c\u7372\u5f97 \u524d\u6a19 14% Stas is an image semantic segmentation task with only two classes which are normal cells and abnormal cells. The difficulty lies in the fact that even untrained human is hard to distinguish the labels correctly. In this competition, I adapted SegFormer and proposed the method Spot Validation validating ROI with more robust CNN model instead of using the output of SegFormer directly. Projects \u00b6 Music Popularity Prediction (Jul, 2022 ~ Present) \u00b6 This project/research aims to predict the time-series popularity of music with transformer architecture, and is conducted, researched, developed by myself. See hackmd for more details. \u4e2d\u6587\uff1a https://hackmd.io/4pdMiliQSHaS5jb6J5BaDg English: https://hackmd.io/W03TtVhOSQCbceFOnd5vNQ Ahk-Doc (Feb, 2022 ~ Present) \u00b6 Ahk-Doc is a language support service for AutoHotkey which is an unpopular script language and thus the original language support lacked many useful features. Currently, I have been working on refactoring for support of LSP(Language Server Protocol) which can not only make language server available to other popular IDE but also improve performance. The original project (which I forked from) only used several regular expression as parser whereas it cannot work on complicated expression and thus make some advanced services unavailable. After studying the theorem of compiler, I'm going to implement more robust lexer and parser for the important but still lacking features of language service. Minor \u00b6 pytorch-model-utils \u2013 toolkits based on Pytorch Lernen \u2013 Social Website for learner. Skills \u00b6 Programing Languages Python Proficient Javascript / Typescript Proficient C++ Competent C Competent C# Advanced Beginner Java Advanced Beginner Autohotkey Expert Web Vue2 Competent Vue3 Advanced Beginner Express (node.js) Competent Database Management Competent Grades \u00b6 \u5b78\u671f \u79d1\u76ee\u540d\u7a31 \u6210\u7e3e 108-2 \u8cc7\u6599\u7d50\u69cb 87 108-2 \u7dda\u6027\u4ee3\u6578 72 1 109-1 \u5de5\u7a0b\u6578\u5b78\uff08\u4e00\uff09 99 109-1 \u6a5f\u7387\u8207\u7d71\u8a08 85 109-1 \u8a08\u7b97\u6a5f\u7d44\u7e54 89 109-1 \u96e2\u6563\u6578\u5b78 83 109-2 \u5de5\u7a0b\u6578\u5b78\uff08\u4e8c\uff09 86 109-2 \u4f5c\u696d\u7cfb\u7d71 91 109-2 \u8a08\u7b97\u6a5f\u6f14\u7b97\u6cd5 79 110-1 \u8a08\u7b97\u7406\u8ad6 93 110-2 \u4eba\u5de5\u667a\u6167\u5c0e\u8ad6 88 110-2 \u8cc7\u6599\u5eab\u7cfb\u7d71\u5c0e\u8ad6 84 111-1 \u591a\u8655\u7406\u6a5f\u5e73\u884c\u7a0b\u5f0f\u8a2d\u8a08 \u4fee\u8ab2\u4e2d 111-1 \u8cc7\u6599\u58d3\u7e2e \u4fee\u8ab2\u4e2d 111-1 \u6a21\u64ec\u8207\u7d71\u8a08\u8a08\u7b97 \u4fee\u8ab2\u4e2d Honors & Awards \u00b6 Contributed to Pytorch Study Plan \u00b6 Realized the most important parts are those basic or traditional theories, in addition to continue my working projects and research, I am studying data compression (highly related to information theory and signal processing), statistics, digital speech processing, and so on. Hobby \u00b6 Badminton Playing badminton at least once a week for cultivation of spirit and winding down from pressure. Understanding the importance of Linear Algebra in machine learning and various fields, I had re-studied. \u21a9","title":"About Me"},{"location":"About%20Me/#personal+information","text":"Name \u6797\u5c11\u6069 Dept. \u570b\u7acb\u6210\u529f\u5927\u5b78\u96fb\u6a5f\u5de5\u7a0b\u5b78\u7cfb(~2023-06) Email. smark9005310@gmail.com Github https://github.com/bhbbbbb Research Interests Machine Learning Deep Learning Computer Vision Audio Recognition Natural Language Processing Social Network Analysis","title":"Personal Information"},{"location":"About%20Me/#competitions","text":"\u96fb\u6a5f\u7cfb\u8a08\u7b97\u6a5f\u6982\u8ad6\u76c3 Blokus \u7b56\u7565\u7a0b\u5f0f\u8a2d\u8a08\u7af6\u8cfd \u5b63\u8ecd (2020-05) \u5927\u5c08\u6821\u9662\u8cc7\u8a0a\u61c9\u7528\u670d\u52d9\u5275\u65b0\u7af6\u8cfd \u4f73\u4f5c (2020-10) We developed a route-recommendation system considering factors including but not limited to traffic, weather, disease(\u767b\u9769\u71b1), user preference. In this project, I was in charge of designing and implementing algorithm for shortest path problem, and data collecting and visualizing data for \u767b\u9769\u71b1. Kaggle \u2013 BirdClef (2022-03) BirdClef, audio classification task, aims to distinguish between bird chirping of various species. I was not familiar to neither NLP nor audio recognition before participating this competition. At that time, I simply adapted ViT (Vision Transformer) to audio version and thus only got average grade. However, I started to study in NLP and audio recognition and further influenced my personal research Music Popularity Prediction . AI CUP 2022 \u80ba\u817a\u764c\u75c5\u7406\u5207\u7247\u5f71\u50cf\u4e4b\u816b\u7624\u6c23\u9053\u64f4\u6563\u5075\u6e2c\u7af6\u8cfd II\uff1a\u904b\u7528\u5f71\u50cf\u5206\u5272\u4f5c\u6cd5\u65bc\u5207\u5272STAS\u8f2a\u5ed3 (2022-04) \uff0c\u7372\u5f97 \u524d\u6a19 14% Stas is an image semantic segmentation task with only two classes which are normal cells and abnormal cells. The difficulty lies in the fact that even untrained human is hard to distinguish the labels correctly. In this competition, I adapted SegFormer and proposed the method Spot Validation validating ROI with more robust CNN model instead of using the output of SegFormer directly.","title":"Competitions"},{"location":"About%20Me/#projects","text":"","title":"Projects"},{"location":"About%20Me/#music+popularity+prediction+jul+2022++present","text":"This project/research aims to predict the time-series popularity of music with transformer architecture, and is conducted, researched, developed by myself. See hackmd for more details. \u4e2d\u6587\uff1a https://hackmd.io/4pdMiliQSHaS5jb6J5BaDg English: https://hackmd.io/W03TtVhOSQCbceFOnd5vNQ","title":"Music Popularity Prediction (Jul, 2022 ~ Present)"},{"location":"About%20Me/#ahk-doc+feb+2022++present","text":"Ahk-Doc is a language support service for AutoHotkey which is an unpopular script language and thus the original language support lacked many useful features. Currently, I have been working on refactoring for support of LSP(Language Server Protocol) which can not only make language server available to other popular IDE but also improve performance. The original project (which I forked from) only used several regular expression as parser whereas it cannot work on complicated expression and thus make some advanced services unavailable. After studying the theorem of compiler, I'm going to implement more robust lexer and parser for the important but still lacking features of language service.","title":"Ahk-Doc (Feb, 2022 ~ Present)"},{"location":"About%20Me/#minor","text":"pytorch-model-utils \u2013 toolkits based on Pytorch Lernen \u2013 Social Website for learner.","title":"Minor"},{"location":"About%20Me/#skills","text":"Programing Languages Python Proficient Javascript / Typescript Proficient C++ Competent C Competent C# Advanced Beginner Java Advanced Beginner Autohotkey Expert Web Vue2 Competent Vue3 Advanced Beginner Express (node.js) Competent Database Management Competent","title":"Skills"},{"location":"About%20Me/#grades","text":"\u5b78\u671f \u79d1\u76ee\u540d\u7a31 \u6210\u7e3e 108-2 \u8cc7\u6599\u7d50\u69cb 87 108-2 \u7dda\u6027\u4ee3\u6578 72 1 109-1 \u5de5\u7a0b\u6578\u5b78\uff08\u4e00\uff09 99 109-1 \u6a5f\u7387\u8207\u7d71\u8a08 85 109-1 \u8a08\u7b97\u6a5f\u7d44\u7e54 89 109-1 \u96e2\u6563\u6578\u5b78 83 109-2 \u5de5\u7a0b\u6578\u5b78\uff08\u4e8c\uff09 86 109-2 \u4f5c\u696d\u7cfb\u7d71 91 109-2 \u8a08\u7b97\u6a5f\u6f14\u7b97\u6cd5 79 110-1 \u8a08\u7b97\u7406\u8ad6 93 110-2 \u4eba\u5de5\u667a\u6167\u5c0e\u8ad6 88 110-2 \u8cc7\u6599\u5eab\u7cfb\u7d71\u5c0e\u8ad6 84 111-1 \u591a\u8655\u7406\u6a5f\u5e73\u884c\u7a0b\u5f0f\u8a2d\u8a08 \u4fee\u8ab2\u4e2d 111-1 \u8cc7\u6599\u58d3\u7e2e \u4fee\u8ab2\u4e2d 111-1 \u6a21\u64ec\u8207\u7d71\u8a08\u8a08\u7b97 \u4fee\u8ab2\u4e2d","title":"Grades"},{"location":"About%20Me/#honors++awards","text":"Contributed to Pytorch","title":"Honors &amp; Awards"},{"location":"About%20Me/#study+plan","text":"Realized the most important parts are those basic or traditional theories, in addition to continue my working projects and research, I am studying data compression (highly related to information theory and signal processing), statistics, digital speech processing, and so on.","title":"Study Plan"},{"location":"About%20Me/#hobby","text":"Badminton Playing badminton at least once a week for cultivation of spirit and winding down from pressure. Understanding the importance of Linear Algebra in machine learning and various fields, I had re-studied. \u21a9","title":"Hobby"},{"location":"Data%20Compression/","text":"Source Coding \u00b6 Shannon's Source Coding, \u4fe1\u6e90\u7de8\u78bc\u3001\u7b26\u865f\u6e90\u7de8\u78bc information mapping (bits, characters, ....) Entropy \u00b6 Self-Information \u00b6 by three assumptions \\(I(p) \\ge 0\\) \\(I(p_1 \\cdot p_2)=I(p_1) + I(p_2)\\) \\(I(p)\\) is continuous to \\(p\\) thus, gives \\(I(p) = -\\log(p)\\) proof at p.25 Entropy \u00b6 Given, \\[ \\begin{gather} S=\\{s_i \\,|\\, p(s_i) = p_i\\} \\end{gather} \\] then, \\[ \\begin{gather} H_r(S) = \\sum\\limits p_i I(s_i)=\\sum\\limits -p_i\\log_r(p_i) \\end{gather} \\] note that \\(r\\) is the base of log Gibbs' Inequality \u00b6 \\[ \\begin{gather} \\sum\\limits - p_i\\log(p_i) \\le \\sum\\limits-p_i\\log(q_i) \\end{gather} \\] the lower bound of cross entropy of \\(q_i\\) and \\(p_i\\) are \\(H(p_i)\\) . Unique Decodable & Instantaneous Code \u00b6 p32 Unique Decodable \u00b6 not unique decodable example \\[ \\begin{gather} (s_1, s_2, s_3, s_4) = (0, 01, 11, 00) \\end{gather} \\] then for the message \\(0011\\) could be decoded as \\[ 0011 = \\left\\{ \\begin{align} &s_4s_3 \\\\ &s_1s_1s_3 \\end{align}\\right. \\] Instantaneous Code \u00b6 instantaneous iff \u6c92\u6709\u4e00\u500b\u7b26\u865f\u662f\u53e6\u4e00\u500b\u7b26\u865f\u7684\u5b57\u9996 instantaneous \u21d2 unique the lengths of codes have to follow Kraft Inequality Kraft Inequality \u00b6 If instantaneous, then the code have to end at leaves. thus, considering binary case only first we cannot build a binary tree for many short path(short coding length). Therefore, in order to build an binary tree, we must follow \\[ \\begin{gather} \\sum\\limits \\frac{1}{2^{L_i}} \\le 1 \\end{gather} \\] intuitive thinking \u2013 Considering taking a branch of binary tree have prob. 0.5 and 0.5, then the summation of prob. of all leaves is \\(1\\) . intuitively, the lowest bound of average lengths of codes is its entropy. (proof at p36.) \\[ \\begin{gather} H_r(S) \\le L_{avg} \\end{gather} \\] Shannon-Fano Coding \u00b6 Since we know the lower bound of coding length is its entropy, then we can have the coding length (known as Shannon-Fano Length ) as \\[ \\begin{gather} -\\log(p_i)\\le \u2113_i < -\\log(p_i) + 1 \\\\\\\\ \\iff \u2113_i = \\bigg\\lceil -\\log(p_i) \\bigg\\rceil \\end{gather} \\] drawback example Given \\(S=\\{s_1, s_2\\}\\) , and \\(k\\) is large \\[ \\begin{gather} p_1 = 2^{-k}, \\qquad p_2=1-p_1 \\\\\\\\ \\implies l_1 = k, \\qquad l_2=1 \\end{gather} \\] in Huffman coding, both \\(l_1, l_2 = 1\\) , but since \\(p_1\\) is small when \\(k\\) is large, this drawback isn't very critical. Extension Code \u00b6 quick concept \u2013 if we have an source code \\(S\\) , then we can define an new source code \\(T = S^{n}\\) (then the # of symbol in \\(T\\) would be \\(|S|^{n}\\) ) The \\(H(T) = nH(S)\\) , and denote \\(L_n\\) (average length of \\(n\\) -order S.F. code), then \\[ \\begin{gather} H(T) \\le L_n < H(T) + 1 \\\\\\\\ H(S) \\le \\frac{L_n}{n} < H(S) + \\frac{1}{n} \\end{gather} \\] thus when \\(n\\to \\infty\\) , then \\(\\frac{L_n}{n} \\to H(S)\\) aka Shannon's noiseless coding theorem. Adaptive Huffman \u00b6 start with one EOF and one ESC (always one in Huffman tree). EOF \u2013 send when end of file. ESC \u2013 send when new symbol is added to tree (send following with ascii of that symbol so that decoder can know what to decode). JBIG & JBIG2 \u00b6 code for binary data directly ( \\(S \\in \\{0, 1\\}\\) ) JBIG \u2013 high order adaptive arithmetic (for \\(P(s_t \\,|\\, s_{(t-n):(t-1)})\\) . since the target is binary, the high order coding table would be \\(2^{n}\\) , much less than high order Huffman. JBIG2 \u2013 Define decoding protocol only. That is the encoder side can be any algorithm, even lossy. LZ \u00b6 LZ77 \u00b6 sliding window and look-ahead buffer find phrase in window, and encode text in look-ahead buffer send (displacement, length, next_char) in no match case, send (0, 0, next_char) pros fast decode cons slow encode \\(O(n)\\) , ( \\(n\\) is windows size) \\(O(m)\\) , ( \\(m\\) is look-ahead buffer size) worst case when no match prefer larger window size but cost too much time efficiency \u2193 compressed phrases size \u2191 loss memory (after dictionary is full of phrases, some of them have to be removed) LZSS \u00b6 improving version of LZ77 send \\(1\\) bit indicating whether is no match instead of send (0, 0 next_char) Circular queue with WINDOW_SIZE = \\(2^{n}\\) Binary Search Tree for storing phrases (actually storing pointer to particular position of window). Node stores fixed-size length e.g. ( \"LZSS is better than LZ77\" ) with fixed length \\(5\\) , (search by comparing char-wise order) graph TD root(LZSS^) l(^is^b) ll(^bett) lr(is^be) r(ZSS^i) rl(SS^is) rr( ) rll(S^is^) rlr(tter^) root --- l root --- r l --- ll l --- lr r --- rl r --- rr rl --- rll rl --- rlr LZ78 \u00b6 create new phrase in dictionary at both encoder size and decoder size whenever no match initially, there is only null string in dictionary. thus whenever create a new phrase with length \\(n\\) , there must exists a phrase with length \\(n-1\\) . therefore, using multiway search tree to store phrases graph TD r('\\0' <br> 0) p1(\"'D' <br> 1\") p2(\"'A' <br> 2\") p3(\"'^' <br> 3\") p4(\"'A' <br> 4\") p5(\"'^' <br> 5\") p6(\"'D' <br> 6\") p7(\"'Y' <br> 7\") p8(\"'^' <br> 8\") p9(\"'O' <br> 9\") r --- p8 r --- p2 r --- p1 p1 --- p3 p1 --- p4 p1 --- p7 p4 --- p5 p4 --- p6 p6 --- p9 - in this case p1 = D , p3 = D^ , p5=DA^ , and so on... example at p119 thus encoder side only have to send phrase code (phrase id), without phrase's length. i.e. (phrase_id, next_char) cons slow decoding (have to maintain dictionary tree) LZW \u00b6 improving version LZ78 defined every characters as phrases first, then send only (phrase_id, ) examples: PNG, ... algorithm p122 encoder: i := 0 ; Dictionary phrases ; string in_buff ; in_buff . Add ( str [ i ]) while in_buff . Add ( str [++ i ]) if in_buff not in phrases phrases . Add ( in_buff ) OUTPUT << phrases . IndexOf ( in_buff . PopLast ()) in_buff := string ( in_buff [- 1 ]) decoder i := 0 Dictionary phrases ; int in_buff ; string last_phrase // init INPUT >> in_buff last_phrase := phrases [ in_buff ] OUTPUT << last_phrase while true INPUT >> in_buff OUTPUT << phrases [ in_buff ] phrases . Add ( last_phrase + phrases [ in_buff ][ 0 ]) last_phrase = phrase [ in_buff ] Lossy \u00b6 RMSE (Root Mean Square Error) SNR (Signal-to-Noise Ratio) \\[ \\begin{gather} \\text{SNR} = \\frac{E\\big[S^{2}\\big]}{E\\big[(X-\\mu)^{2}\\big]} = \\frac{E\\big[S^{2}\\big]}{\\sigma_r^{2}} \\\\\\\\ \\text{SNR}_{dB} = 10\\log_{10}{\\text{SNR}} \\end{gather} \\] in 2d media case \\[ \\begin{gather} \\text{SNR} = \\frac{Q^{2}}{\\sigma_r^{2}} \\end{gather} \\] in which \\(Q = 255\\) in 8-bits case. DM \u00b6 Delta Modulation Adaptive DM (ADM) adaptive for the magnitude of \\(\\Delta\\) DPCM \u00b6 Differential Pulse Code Modulation Predictor Optimization \u00b6 objective \\[ \\begin{align} \\hat x_{m}^* &= \\arg\\min_{\\hat x_m} \\,\\sigma_e^{2} \\\\\\\\ &= \\arg\\min_{\\hat x_m} E\\bigg[(x_m - {\\hat x_m})^{2}\\bigg] \\end{align} \\] in which, \\[ \\begin{gather} \\hat x_m = \\sum\\limits_{i \\in [0, m)}{\\alpha_i}x_i \\end{gather} \\] then solve by differentiation, we have \\[ \\begin{gather} E\\big[(x_m-\\hat x_m)x_i\\big] = 0 \\end{gather} \\] thus \\[ \\begin{gather} E[x_mx_i] = E[\\hat x_m x_i] \\\\\\\\ R_{mi} = E[\\hat x_m x_i] \\end{gather} \\] and further when \\(i=m\\) , \\[ \\begin{gather} E\\left[x_m^{2}\\right]=E\\left[\\hat x_m x_m\\right] \\\\\\\\ \\implies \\sigma_e^{2} = E\\bigg[(x_m -\\hat x_m)^{2}\\bigg]= \\\\\\\\ E\\big[(x_m-\\hat x_m)\\hat x_m\\big]=E\\big[x_m^{2}\\big] - E\\big[\\hat x_m^{2}\\big] \\\\\\\\ \\implies \\sigma^{2} < E\\big[x_m^{2}\\big] \\end{gather} \\] p141 Quantizer Optimization \u00b6 objective ( \\(N\\) is number of order of quantizer) \\[ \\begin{gather} D=\\sum\\limits_{i \\in [0, N)}{\\int_{d_i}^{d_{i+1}}p(e)\\,(e-r_i)^{2}de} \\end{gather} \\] p143 Adaptive DPCM (ADPCM) \u00b6 \u6620\u5c04\u91cf\u5316\u5668 \\(e=x_m - \\hat x_m\\) \\(x_m \\in [0, 2^{n})\\) , thus normally \\(e \\in(-2^{n}, 2^{n})\\) however with known \\(\\hat x_m\\) , then \\(e \\in [-\\hat x_m,\\, 2^{n} - \\hat x_m)\\) \u66ff\u63db\u91cf\u5316\u5668 use multi quantizers, and encode with the best quantizer, and send which of quantizers. Lossless DPCM \u00b6 without quantizers, send the \\(e\\) directly (or further using other lossless algorithm). thus can be used as preprocessor for others like Huffman , Arith . (since quite popular). more efficient then using adaptive like AHuff , AArith , but perform closely. Non-Redundant Sample Coding \u00b6 aka adaptive sampling coding Polynomial Predictor \u00b6 \\[ \\begin{gather} x_t = x_{t-1} + \\Delta x_{t-1}+\\Delta^{2}x_{t-1}+\\cdots \\end{gather} \\] in which \\[ \\begin{gather} \\Delta^{2}x_{t-1} = \\Delta x_{t-1} - \\Delta x_{t-2} \\end{gather} \\] Polynomial Interpolator \u00b6 1\u6b21\u5167\u63d2\u6cd5 = \u6247\u5f62\u6f14\u7b97\u6cd5 = SAPA2 AZTEC \u00b6 p173 rules use horizontal line if \\(\\ge 3\\) successive samples satisfy \\(x_{max} - x_{min} < \\lambda\\) otherwise, use slope. Further, if next sample have the same signed of slope and \\(|x_m - x_{m-1}| > \\lambda\\) , then keep redundant. CORNER \u00b6 CORCER > SAPA2 > AZTEC algorithm 2-order diff, \\(x''(i)=x(i+1)+x(i-1)-2x(i)\\) \\(\\forall i\\) , if \\(x(i) > \\lambda_1\\) and \\(x''(i)\\) is local maximum, then make \\(x(i)\\) then now have redundant samples \\(x(m_1), x(m_2), x(m_3), \\dots\\) for all redundant, find if \\(x\\left(\\frac{m_1+m_2}{2}\\right)- \\frac{x(m_1)+x(m_2)}{2} > \\lambda_2\\) (that is, if the \\(x\\) very concave (\u51f9)) if not, add \\(x(\\frac{m_1 + m_2}{2})\\) , as \\(x(m_{1\\_2})\\) , and do \\(\\big\\{x(m_1)\\) , \\(x(m_{1\\_2})\\big\\}\\) and \\(\\big\\{x(m_{1\\_2}), x(m_2)\\big\\}\\) as step 4. if step4. so, continue BTC \u00b6 -Block Truncation Coding Moment-Preserving Quantizer \u00b6 target \u2013 find quantizer that make 1st and 2nd moment unchanged, and since \\[ \\begin{gather} \\sigma^{2} = E\\big[X^{2}\\big] - E\\big[X\\big]^{2} \\end{gather} \\] the variance \\(\\sigma^{2}\\) would also unchanged. solution \u2013 Given \\(X_{th}\\) as the threshold (normally, mean of all pixels), and \\(q\\) is # of \\(b\\) cases \\[ \\hat x_i = \\begin{dcases*} a & if $x_i < X_{th}$ \\\\\\\\ b & otherwise \\end{dcases*} \\] \\[ \\begin{align} \\\\ E\\big[X\\big]&=\\frac{(m-q)\\cdot b+q\\cdot a}{m} \\\\\\\\ E\\big[X^{2}\\big]&=\\frac{(m-q)\\cdot b^{2} + q\\cdot a^{2}}{m} \\end{align} \\] then, \\[ \\begin{align} a &= E\\big[X\\big] - \\sigma \\sqrt{\\frac{q}{m-q}} \\\\\\\\ a &= E\\big[X\\big] + \\sigma \\sqrt{\\frac{m-q}{q}} \\end{align} \\] solution2 (Absolute Moment BTC, AMBTC) since the square root is hard to compute, thus we maintain the absolute moment instead of 2nd moment. \\[ \\begin{gather} \\alpha=E\\bigg[|X_i - \\mu|\\bigg] \\end{gather} \\] then, we can have, \\[ \\begin{align} a &= E[X] - \\frac{m\\alpha}{2(m-q)} \\\\\\\\ b &= E[X] + \\frac{m\\alpha}{2q} \\end{align} \\] Transform Coding \u00b6 Rotation Matrix \\[ M(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin \\theta \\\\ \\sin\\theta & \\cos \\theta \\end{bmatrix} \\] ### Zonal Sampling - \u5340\u57df\u53d6\u6a23 1. \u4fdd\u7559\u4f4e\u983b\uff0c\u9ad8\u983b\u901a\u5e38\u5c0f\uff0c\u4e0d\u7559 2. \u4f4e\u983b\u7528\u6bd4\u8f03\u591a bits 3. \u56fa\u5b9a bits per block \u4f46\u8b93 variance \u5927(\u5c0d\u5176\u4ed6 blocks \u7684\u540c\u4f4d\u7f6e)\u7684\u4fc2\u6578\u6709\u6bd4\u8f03\u591a bits - cons - \u53ef\u80fd\u6709\u5f88\u5927\u96e3\u4ee5\u5ffd\u7565\u7684\u4fc2\u6578 Threshold Sampling \u00b6 \u81e8\u754c\u53d6\u6a23 \u8a2d threshold \uff0c\u4ee5\u4e0b\u70ba 0\uff0c\u4ee5\u4e0a\u9001\u4f4d\u7f6e\u8207\u503c JPEG encode \u00b6 get \\(F^{*}(u, v)\\) scan with z order First coefficient(DC) encode with DPCM and Huffman. encode remains coefficients(AC) with the following law omit \\(0\\) lookup category \\(k\\) at p220 according to category \\(k\\) and # of \\(0\\) before this coefficient, lookup table in p224. append \\(k\\) bits which indicates the index of AC coefficient in that category. DCT \u00b6 p209 Vector Quantization \u00b6 Cost of encoding \\(O(nN_c)\\) , where \\(n\\) is dimension of vector, \\(N_c\\) is number of codebooks. need memory \\(n N_c\\) LBG \u00b6 LBG\u6f14\u7b97\u6cd5\u662f\u7531Linde,Buzo,Gray\u4e09\u4eba\u57281980\u5e74\u63d0\u51fa\u7684\u3002\u5176\u7b97\u6cd5\u8207K-means\u96f7\u540c\uff0c\u6839\u64da\u7576\u524d\u5283\u5206\u4e4b\u7fa4\u96c6\u8a08\u7b97\u8aa4\u5dee\u91cf\uff0c\u4e0d\u65b7\u8abf\u6574\u6620\u5c04\u5340\u9593(Mapping Region)\u53ca\u91cf\u5316\u5411\u91cf\u4e4b\u91cf\u5316\u9ede: \u7d66\u5b9a\u8a13\u7df4\u6a23\u672c\u4ee5\u53ca\u8aa4\u5dee\u95be\u503c \u8a02\u5b9a\u521d\u59cb\u78bc\u5411\u91cf \u5c07\u758a\u4ee3\u8a08\u6578\u5668\u6b78\u96f6 \u8a08\u7b97\u7e3d\u8aa4\u5dee\u503c\uff0c\u82e5\u4e0d\u70ba\u7b2c\u4e00\u6b21\uff0c\u5247\u6aa2\u67e5\u8207\u524d\u4e00\u6b21\u8aa4\u5dee\u503c\u5dee\u8ddd\u662f\u5426\u5c0f\u65bc\u95be\u503c\u3002 \u6839\u64da\u6bcf\u4e00\u500b\u8a13\u7df4\u6a23\u672c\u8207\u78bc\u5411\u91cf\u7684\u8ddd\u96e2d\uff0c\u627e\u5176\u6700\u5c0f\u503c\uff0c\u5b9a\u7fa9\u70ba\u6620\u5c04\u51fd\u6578Q \u66f4\u65b0\u78bc\u5411\u91cf\uff1a\u5c07\u5c0d\u61c9\u5230\u540c\u4e00\u500b\u78bc\u5411\u91cf\u7684\u5168\u6578\u8a13\u7df4\u6a23\u672c\u505a\u5e73\u5747\u4ee5\u66f4\u65b0\u78bc\u5411\u91cf\u3002 i\u70ba\u758a\u4ee3\u8a08\u6578\u5668\uff0cC\u70ba\u8a72\u7fa4\u96c6\u4e4b\u4ee3\u8868\uff0cx\u70ba\u8cc7\u6599\u9ede\uff0cQ(x)\u70bax\u91cf\u5316\u5f8c\u4e4b\u7fa4\u96c6\u4ee3\u8868C \u758a\u4ee3\u8a08\u6578\u5668\u52a0\u4e00 \u6703\u5230\u6b65\u9a5f\u56db\uff0c\u76f4\u81f3\u8aa4\u5dee\u503c\u5c0f\u65bc\u95a5\u503c LBG\u6f14\u7b97\u6cd5\u5341\u5206\u4f9d\u8cf4\u8d77\u59cb\u7de8\u78bc\u7c3f\uff0c\u7522\u751f\u8d77\u59cb\u7de8\u78bc\u7c3f\u7684\u65b9\u6cd5\u6709\u4ee5\u4e0b\u5e7e\u7a2e\uff1a Tree-Structured Codebooks \u00b6 \\(m\\) -ways tree Cost now decrease to \\(O(n \\cdot m\\log_m{N_c})\\) Memory increase to \\(n m \\frac{N_c - 1}{m - 1}\\) Because there are \\(N_n=(m + m^{2} + m^{3} +\\cdots)\\) nodes, and that \\(N_n = m\\frac{m^{\\log_m N_c} -1}{m-1}=m\\frac{N_c - 1}{m-1}\\) cons perform worse then full-search, since taking branch. Product Code \u00b6 use codebook represent vector direction with size \\(N_1\\) , and codebooks represent vector length with size \\(N_2\\) . this case, we can represent \\(N_1 N_2\\) vectors with only size \\(N_1 + N_2\\) therefore, in same time complexity and bit rate, perform better the full-search M/RVQ \u00b6 \u5e73\u5747/\u9918\u503c VQ \u5c0d\u6bcf\u500b blocks (e.g. \\(n=4\\times 4=16\\) ) \u6e1b\u53bb\u5e73\u5747 \u50b3\u9001\u5e73\u5747 (with DPCM or something) do VQ and send I/RVQ \u00b6 \u5167\u63d2/\u9918\u503c VQ do subsampling to original image (assume \\(N\\times N\\) ) and would get sub-image ( \\(N/\u2113 \\times N/\u2113\\) , normally \\(\u2113=8\\) ). do up-sampling by interpolation, and get the residual image. split residual image to blocks and do VQ. pros perform better (less blocking artifacts) than M/RVQ. G/SVQ \u00b6 Gain/Shape VQ find and send the vector that match the most (has greatest dot product value) CVQ \u00b6 Classification VQ split the image to blocks classify blocks to categories for every categories, there are some specific codebooks normally use many small codebooks, but can reach similar performance with the normal VQ using large codebooks. FSVQ \u00b6 Finite State VQ Given codebooks per state \\(C(s_i)\\) transition function \\(s_i = f(s_{i-1}, Y_{i-1})\\) given \\(s_0\\) , and find \\(Y_0\\) by \\(C(s_0)\\) get \\(s_1 = f(s_0, Y_0)\\) , and find \\(Y_1\\) until all done cons one connection drop can cause serious consequence.","title":"Data Compression"},{"location":"Data%20Compression/#source+coding","text":"Shannon's Source Coding, \u4fe1\u6e90\u7de8\u78bc\u3001\u7b26\u865f\u6e90\u7de8\u78bc information mapping (bits, characters, ....)","title":"Source Coding"},{"location":"Data%20Compression/#entropy","text":"","title":"Entropy"},{"location":"Data%20Compression/#self-information","text":"by three assumptions \\(I(p) \\ge 0\\) \\(I(p_1 \\cdot p_2)=I(p_1) + I(p_2)\\) \\(I(p)\\) is continuous to \\(p\\) thus, gives \\(I(p) = -\\log(p)\\) proof at p.25","title":"Self-Information"},{"location":"Data%20Compression/#entropy_1","text":"Given, \\[ \\begin{gather} S=\\{s_i \\,|\\, p(s_i) = p_i\\} \\end{gather} \\] then, \\[ \\begin{gather} H_r(S) = \\sum\\limits p_i I(s_i)=\\sum\\limits -p_i\\log_r(p_i) \\end{gather} \\] note that \\(r\\) is the base of log","title":"Entropy"},{"location":"Data%20Compression/#gibbs+inequality","text":"\\[ \\begin{gather} \\sum\\limits - p_i\\log(p_i) \\le \\sum\\limits-p_i\\log(q_i) \\end{gather} \\] the lower bound of cross entropy of \\(q_i\\) and \\(p_i\\) are \\(H(p_i)\\) .","title":"Gibbs' Inequality"},{"location":"Data%20Compression/#unique+decodable++instantaneous+code","text":"p32","title":"Unique Decodable &amp; Instantaneous Code"},{"location":"Data%20Compression/#unique+decodable","text":"not unique decodable example \\[ \\begin{gather} (s_1, s_2, s_3, s_4) = (0, 01, 11, 00) \\end{gather} \\] then for the message \\(0011\\) could be decoded as \\[ 0011 = \\left\\{ \\begin{align} &s_4s_3 \\\\ &s_1s_1s_3 \\end{align}\\right. \\]","title":"Unique Decodable"},{"location":"Data%20Compression/#instantaneous+code","text":"instantaneous iff \u6c92\u6709\u4e00\u500b\u7b26\u865f\u662f\u53e6\u4e00\u500b\u7b26\u865f\u7684\u5b57\u9996 instantaneous \u21d2 unique the lengths of codes have to follow Kraft Inequality","title":"Instantaneous Code"},{"location":"Data%20Compression/#kraft+inequality","text":"If instantaneous, then the code have to end at leaves. thus, considering binary case only first we cannot build a binary tree for many short path(short coding length). Therefore, in order to build an binary tree, we must follow \\[ \\begin{gather} \\sum\\limits \\frac{1}{2^{L_i}} \\le 1 \\end{gather} \\] intuitive thinking \u2013 Considering taking a branch of binary tree have prob. 0.5 and 0.5, then the summation of prob. of all leaves is \\(1\\) . intuitively, the lowest bound of average lengths of codes is its entropy. (proof at p36.) \\[ \\begin{gather} H_r(S) \\le L_{avg} \\end{gather} \\]","title":"Kraft Inequality"},{"location":"Data%20Compression/#shannon-fano+coding","text":"Since we know the lower bound of coding length is its entropy, then we can have the coding length (known as Shannon-Fano Length ) as \\[ \\begin{gather} -\\log(p_i)\\le \u2113_i < -\\log(p_i) + 1 \\\\\\\\ \\iff \u2113_i = \\bigg\\lceil -\\log(p_i) \\bigg\\rceil \\end{gather} \\] drawback example Given \\(S=\\{s_1, s_2\\}\\) , and \\(k\\) is large \\[ \\begin{gather} p_1 = 2^{-k}, \\qquad p_2=1-p_1 \\\\\\\\ \\implies l_1 = k, \\qquad l_2=1 \\end{gather} \\] in Huffman coding, both \\(l_1, l_2 = 1\\) , but since \\(p_1\\) is small when \\(k\\) is large, this drawback isn't very critical.","title":"Shannon-Fano Coding"},{"location":"Data%20Compression/#extension+code","text":"quick concept \u2013 if we have an source code \\(S\\) , then we can define an new source code \\(T = S^{n}\\) (then the # of symbol in \\(T\\) would be \\(|S|^{n}\\) ) The \\(H(T) = nH(S)\\) , and denote \\(L_n\\) (average length of \\(n\\) -order S.F. code), then \\[ \\begin{gather} H(T) \\le L_n < H(T) + 1 \\\\\\\\ H(S) \\le \\frac{L_n}{n} < H(S) + \\frac{1}{n} \\end{gather} \\] thus when \\(n\\to \\infty\\) , then \\(\\frac{L_n}{n} \\to H(S)\\) aka Shannon's noiseless coding theorem.","title":"Extension Code"},{"location":"Data%20Compression/#adaptive+huffman","text":"start with one EOF and one ESC (always one in Huffman tree). EOF \u2013 send when end of file. ESC \u2013 send when new symbol is added to tree (send following with ascii of that symbol so that decoder can know what to decode).","title":"Adaptive Huffman"},{"location":"Data%20Compression/#jbig++jbig2","text":"code for binary data directly ( \\(S \\in \\{0, 1\\}\\) ) JBIG \u2013 high order adaptive arithmetic (for \\(P(s_t \\,|\\, s_{(t-n):(t-1)})\\) . since the target is binary, the high order coding table would be \\(2^{n}\\) , much less than high order Huffman. JBIG2 \u2013 Define decoding protocol only. That is the encoder side can be any algorithm, even lossy.","title":"JBIG &amp; JBIG2"},{"location":"Data%20Compression/#lz","text":"","title":"LZ"},{"location":"Data%20Compression/#lz77","text":"sliding window and look-ahead buffer find phrase in window, and encode text in look-ahead buffer send (displacement, length, next_char) in no match case, send (0, 0, next_char) pros fast decode cons slow encode \\(O(n)\\) , ( \\(n\\) is windows size) \\(O(m)\\) , ( \\(m\\) is look-ahead buffer size) worst case when no match prefer larger window size but cost too much time efficiency \u2193 compressed phrases size \u2191 loss memory (after dictionary is full of phrases, some of them have to be removed)","title":"LZ77"},{"location":"Data%20Compression/#lzss","text":"improving version of LZ77 send \\(1\\) bit indicating whether is no match instead of send (0, 0 next_char) Circular queue with WINDOW_SIZE = \\(2^{n}\\) Binary Search Tree for storing phrases (actually storing pointer to particular position of window). Node stores fixed-size length e.g. ( \"LZSS is better than LZ77\" ) with fixed length \\(5\\) , (search by comparing char-wise order) graph TD root(LZSS^) l(^is^b) ll(^bett) lr(is^be) r(ZSS^i) rl(SS^is) rr( ) rll(S^is^) rlr(tter^) root --- l root --- r l --- ll l --- lr r --- rl r --- rr rl --- rll rl --- rlr","title":"LZSS"},{"location":"Data%20Compression/#lz78","text":"create new phrase in dictionary at both encoder size and decoder size whenever no match initially, there is only null string in dictionary. thus whenever create a new phrase with length \\(n\\) , there must exists a phrase with length \\(n-1\\) . therefore, using multiway search tree to store phrases graph TD r('\\0' <br> 0) p1(\"'D' <br> 1\") p2(\"'A' <br> 2\") p3(\"'^' <br> 3\") p4(\"'A' <br> 4\") p5(\"'^' <br> 5\") p6(\"'D' <br> 6\") p7(\"'Y' <br> 7\") p8(\"'^' <br> 8\") p9(\"'O' <br> 9\") r --- p8 r --- p2 r --- p1 p1 --- p3 p1 --- p4 p1 --- p7 p4 --- p5 p4 --- p6 p6 --- p9 - in this case p1 = D , p3 = D^ , p5=DA^ , and so on... example at p119 thus encoder side only have to send phrase code (phrase id), without phrase's length. i.e. (phrase_id, next_char) cons slow decoding (have to maintain dictionary tree)","title":"LZ78"},{"location":"Data%20Compression/#lzw","text":"improving version LZ78 defined every characters as phrases first, then send only (phrase_id, ) examples: PNG, ... algorithm p122 encoder: i := 0 ; Dictionary phrases ; string in_buff ; in_buff . Add ( str [ i ]) while in_buff . Add ( str [++ i ]) if in_buff not in phrases phrases . Add ( in_buff ) OUTPUT << phrases . IndexOf ( in_buff . PopLast ()) in_buff := string ( in_buff [- 1 ]) decoder i := 0 Dictionary phrases ; int in_buff ; string last_phrase // init INPUT >> in_buff last_phrase := phrases [ in_buff ] OUTPUT << last_phrase while true INPUT >> in_buff OUTPUT << phrases [ in_buff ] phrases . Add ( last_phrase + phrases [ in_buff ][ 0 ]) last_phrase = phrase [ in_buff ]","title":"LZW"},{"location":"Data%20Compression/#lossy","text":"RMSE (Root Mean Square Error) SNR (Signal-to-Noise Ratio) \\[ \\begin{gather} \\text{SNR} = \\frac{E\\big[S^{2}\\big]}{E\\big[(X-\\mu)^{2}\\big]} = \\frac{E\\big[S^{2}\\big]}{\\sigma_r^{2}} \\\\\\\\ \\text{SNR}_{dB} = 10\\log_{10}{\\text{SNR}} \\end{gather} \\] in 2d media case \\[ \\begin{gather} \\text{SNR} = \\frac{Q^{2}}{\\sigma_r^{2}} \\end{gather} \\] in which \\(Q = 255\\) in 8-bits case.","title":"Lossy"},{"location":"Data%20Compression/#dm","text":"Delta Modulation Adaptive DM (ADM) adaptive for the magnitude of \\(\\Delta\\)","title":"DM"},{"location":"Data%20Compression/#dpcm","text":"Differential Pulse Code Modulation","title":"DPCM"},{"location":"Data%20Compression/#predictor+optimization","text":"objective \\[ \\begin{align} \\hat x_{m}^* &= \\arg\\min_{\\hat x_m} \\,\\sigma_e^{2} \\\\\\\\ &= \\arg\\min_{\\hat x_m} E\\bigg[(x_m - {\\hat x_m})^{2}\\bigg] \\end{align} \\] in which, \\[ \\begin{gather} \\hat x_m = \\sum\\limits_{i \\in [0, m)}{\\alpha_i}x_i \\end{gather} \\] then solve by differentiation, we have \\[ \\begin{gather} E\\big[(x_m-\\hat x_m)x_i\\big] = 0 \\end{gather} \\] thus \\[ \\begin{gather} E[x_mx_i] = E[\\hat x_m x_i] \\\\\\\\ R_{mi} = E[\\hat x_m x_i] \\end{gather} \\] and further when \\(i=m\\) , \\[ \\begin{gather} E\\left[x_m^{2}\\right]=E\\left[\\hat x_m x_m\\right] \\\\\\\\ \\implies \\sigma_e^{2} = E\\bigg[(x_m -\\hat x_m)^{2}\\bigg]= \\\\\\\\ E\\big[(x_m-\\hat x_m)\\hat x_m\\big]=E\\big[x_m^{2}\\big] - E\\big[\\hat x_m^{2}\\big] \\\\\\\\ \\implies \\sigma^{2} < E\\big[x_m^{2}\\big] \\end{gather} \\] p141","title":"Predictor Optimization"},{"location":"Data%20Compression/#quantizer+optimization","text":"objective ( \\(N\\) is number of order of quantizer) \\[ \\begin{gather} D=\\sum\\limits_{i \\in [0, N)}{\\int_{d_i}^{d_{i+1}}p(e)\\,(e-r_i)^{2}de} \\end{gather} \\] p143","title":"Quantizer Optimization"},{"location":"Data%20Compression/#adaptive+dpcm+adpcm","text":"\u6620\u5c04\u91cf\u5316\u5668 \\(e=x_m - \\hat x_m\\) \\(x_m \\in [0, 2^{n})\\) , thus normally \\(e \\in(-2^{n}, 2^{n})\\) however with known \\(\\hat x_m\\) , then \\(e \\in [-\\hat x_m,\\, 2^{n} - \\hat x_m)\\) \u66ff\u63db\u91cf\u5316\u5668 use multi quantizers, and encode with the best quantizer, and send which of quantizers.","title":"Adaptive DPCM (ADPCM)"},{"location":"Data%20Compression/#lossless+dpcm","text":"without quantizers, send the \\(e\\) directly (or further using other lossless algorithm). thus can be used as preprocessor for others like Huffman , Arith . (since quite popular). more efficient then using adaptive like AHuff , AArith , but perform closely.","title":"Lossless DPCM"},{"location":"Data%20Compression/#non-redundant+sample+coding","text":"aka adaptive sampling coding","title":"Non-Redundant Sample Coding"},{"location":"Data%20Compression/#polynomial+predictor","text":"\\[ \\begin{gather} x_t = x_{t-1} + \\Delta x_{t-1}+\\Delta^{2}x_{t-1}+\\cdots \\end{gather} \\] in which \\[ \\begin{gather} \\Delta^{2}x_{t-1} = \\Delta x_{t-1} - \\Delta x_{t-2} \\end{gather} \\]","title":"Polynomial Predictor"},{"location":"Data%20Compression/#polynomial+interpolator","text":"1\u6b21\u5167\u63d2\u6cd5 = \u6247\u5f62\u6f14\u7b97\u6cd5 = SAPA2","title":"Polynomial Interpolator"},{"location":"Data%20Compression/#aztec","text":"p173 rules use horizontal line if \\(\\ge 3\\) successive samples satisfy \\(x_{max} - x_{min} < \\lambda\\) otherwise, use slope. Further, if next sample have the same signed of slope and \\(|x_m - x_{m-1}| > \\lambda\\) , then keep redundant.","title":"AZTEC"},{"location":"Data%20Compression/#corner","text":"CORCER > SAPA2 > AZTEC algorithm 2-order diff, \\(x''(i)=x(i+1)+x(i-1)-2x(i)\\) \\(\\forall i\\) , if \\(x(i) > \\lambda_1\\) and \\(x''(i)\\) is local maximum, then make \\(x(i)\\) then now have redundant samples \\(x(m_1), x(m_2), x(m_3), \\dots\\) for all redundant, find if \\(x\\left(\\frac{m_1+m_2}{2}\\right)- \\frac{x(m_1)+x(m_2)}{2} > \\lambda_2\\) (that is, if the \\(x\\) very concave (\u51f9)) if not, add \\(x(\\frac{m_1 + m_2}{2})\\) , as \\(x(m_{1\\_2})\\) , and do \\(\\big\\{x(m_1)\\) , \\(x(m_{1\\_2})\\big\\}\\) and \\(\\big\\{x(m_{1\\_2}), x(m_2)\\big\\}\\) as step 4. if step4. so, continue","title":"CORNER"},{"location":"Data%20Compression/#btc","text":"-Block Truncation Coding","title":"BTC"},{"location":"Data%20Compression/#moment-preserving+quantizer","text":"target \u2013 find quantizer that make 1st and 2nd moment unchanged, and since \\[ \\begin{gather} \\sigma^{2} = E\\big[X^{2}\\big] - E\\big[X\\big]^{2} \\end{gather} \\] the variance \\(\\sigma^{2}\\) would also unchanged. solution \u2013 Given \\(X_{th}\\) as the threshold (normally, mean of all pixels), and \\(q\\) is # of \\(b\\) cases \\[ \\hat x_i = \\begin{dcases*} a & if $x_i < X_{th}$ \\\\\\\\ b & otherwise \\end{dcases*} \\] \\[ \\begin{align} \\\\ E\\big[X\\big]&=\\frac{(m-q)\\cdot b+q\\cdot a}{m} \\\\\\\\ E\\big[X^{2}\\big]&=\\frac{(m-q)\\cdot b^{2} + q\\cdot a^{2}}{m} \\end{align} \\] then, \\[ \\begin{align} a &= E\\big[X\\big] - \\sigma \\sqrt{\\frac{q}{m-q}} \\\\\\\\ a &= E\\big[X\\big] + \\sigma \\sqrt{\\frac{m-q}{q}} \\end{align} \\] solution2 (Absolute Moment BTC, AMBTC) since the square root is hard to compute, thus we maintain the absolute moment instead of 2nd moment. \\[ \\begin{gather} \\alpha=E\\bigg[|X_i - \\mu|\\bigg] \\end{gather} \\] then, we can have, \\[ \\begin{align} a &= E[X] - \\frac{m\\alpha}{2(m-q)} \\\\\\\\ b &= E[X] + \\frac{m\\alpha}{2q} \\end{align} \\]","title":"Moment-Preserving Quantizer"},{"location":"Data%20Compression/#transform+coding","text":"Rotation Matrix \\[ M(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin \\theta \\\\ \\sin\\theta & \\cos \\theta \\end{bmatrix} \\] ### Zonal Sampling - \u5340\u57df\u53d6\u6a23 1. \u4fdd\u7559\u4f4e\u983b\uff0c\u9ad8\u983b\u901a\u5e38\u5c0f\uff0c\u4e0d\u7559 2. \u4f4e\u983b\u7528\u6bd4\u8f03\u591a bits 3. \u56fa\u5b9a bits per block \u4f46\u8b93 variance \u5927(\u5c0d\u5176\u4ed6 blocks \u7684\u540c\u4f4d\u7f6e)\u7684\u4fc2\u6578\u6709\u6bd4\u8f03\u591a bits - cons - \u53ef\u80fd\u6709\u5f88\u5927\u96e3\u4ee5\u5ffd\u7565\u7684\u4fc2\u6578","title":"Transform Coding"},{"location":"Data%20Compression/#threshold+sampling","text":"\u81e8\u754c\u53d6\u6a23 \u8a2d threshold \uff0c\u4ee5\u4e0b\u70ba 0\uff0c\u4ee5\u4e0a\u9001\u4f4d\u7f6e\u8207\u503c","title":"Threshold Sampling"},{"location":"Data%20Compression/#jpeg+encode","text":"get \\(F^{*}(u, v)\\) scan with z order First coefficient(DC) encode with DPCM and Huffman. encode remains coefficients(AC) with the following law omit \\(0\\) lookup category \\(k\\) at p220 according to category \\(k\\) and # of \\(0\\) before this coefficient, lookup table in p224. append \\(k\\) bits which indicates the index of AC coefficient in that category.","title":"JPEG encode"},{"location":"Data%20Compression/#dct","text":"p209","title":"DCT"},{"location":"Data%20Compression/#vector+quantization","text":"Cost of encoding \\(O(nN_c)\\) , where \\(n\\) is dimension of vector, \\(N_c\\) is number of codebooks. need memory \\(n N_c\\)","title":"Vector Quantization"},{"location":"Data%20Compression/#lbg","text":"LBG\u6f14\u7b97\u6cd5\u662f\u7531Linde,Buzo,Gray\u4e09\u4eba\u57281980\u5e74\u63d0\u51fa\u7684\u3002\u5176\u7b97\u6cd5\u8207K-means\u96f7\u540c\uff0c\u6839\u64da\u7576\u524d\u5283\u5206\u4e4b\u7fa4\u96c6\u8a08\u7b97\u8aa4\u5dee\u91cf\uff0c\u4e0d\u65b7\u8abf\u6574\u6620\u5c04\u5340\u9593(Mapping Region)\u53ca\u91cf\u5316\u5411\u91cf\u4e4b\u91cf\u5316\u9ede: \u7d66\u5b9a\u8a13\u7df4\u6a23\u672c\u4ee5\u53ca\u8aa4\u5dee\u95be\u503c \u8a02\u5b9a\u521d\u59cb\u78bc\u5411\u91cf \u5c07\u758a\u4ee3\u8a08\u6578\u5668\u6b78\u96f6 \u8a08\u7b97\u7e3d\u8aa4\u5dee\u503c\uff0c\u82e5\u4e0d\u70ba\u7b2c\u4e00\u6b21\uff0c\u5247\u6aa2\u67e5\u8207\u524d\u4e00\u6b21\u8aa4\u5dee\u503c\u5dee\u8ddd\u662f\u5426\u5c0f\u65bc\u95be\u503c\u3002 \u6839\u64da\u6bcf\u4e00\u500b\u8a13\u7df4\u6a23\u672c\u8207\u78bc\u5411\u91cf\u7684\u8ddd\u96e2d\uff0c\u627e\u5176\u6700\u5c0f\u503c\uff0c\u5b9a\u7fa9\u70ba\u6620\u5c04\u51fd\u6578Q \u66f4\u65b0\u78bc\u5411\u91cf\uff1a\u5c07\u5c0d\u61c9\u5230\u540c\u4e00\u500b\u78bc\u5411\u91cf\u7684\u5168\u6578\u8a13\u7df4\u6a23\u672c\u505a\u5e73\u5747\u4ee5\u66f4\u65b0\u78bc\u5411\u91cf\u3002 i\u70ba\u758a\u4ee3\u8a08\u6578\u5668\uff0cC\u70ba\u8a72\u7fa4\u96c6\u4e4b\u4ee3\u8868\uff0cx\u70ba\u8cc7\u6599\u9ede\uff0cQ(x)\u70bax\u91cf\u5316\u5f8c\u4e4b\u7fa4\u96c6\u4ee3\u8868C \u758a\u4ee3\u8a08\u6578\u5668\u52a0\u4e00 \u6703\u5230\u6b65\u9a5f\u56db\uff0c\u76f4\u81f3\u8aa4\u5dee\u503c\u5c0f\u65bc\u95a5\u503c LBG\u6f14\u7b97\u6cd5\u5341\u5206\u4f9d\u8cf4\u8d77\u59cb\u7de8\u78bc\u7c3f\uff0c\u7522\u751f\u8d77\u59cb\u7de8\u78bc\u7c3f\u7684\u65b9\u6cd5\u6709\u4ee5\u4e0b\u5e7e\u7a2e\uff1a","title":"LBG"},{"location":"Data%20Compression/#tree-structured+codebooks","text":"\\(m\\) -ways tree Cost now decrease to \\(O(n \\cdot m\\log_m{N_c})\\) Memory increase to \\(n m \\frac{N_c - 1}{m - 1}\\) Because there are \\(N_n=(m + m^{2} + m^{3} +\\cdots)\\) nodes, and that \\(N_n = m\\frac{m^{\\log_m N_c} -1}{m-1}=m\\frac{N_c - 1}{m-1}\\) cons perform worse then full-search, since taking branch.","title":"Tree-Structured Codebooks"},{"location":"Data%20Compression/#product+code","text":"use codebook represent vector direction with size \\(N_1\\) , and codebooks represent vector length with size \\(N_2\\) . this case, we can represent \\(N_1 N_2\\) vectors with only size \\(N_1 + N_2\\) therefore, in same time complexity and bit rate, perform better the full-search","title":"Product Code"},{"location":"Data%20Compression/#mrvq","text":"\u5e73\u5747/\u9918\u503c VQ \u5c0d\u6bcf\u500b blocks (e.g. \\(n=4\\times 4=16\\) ) \u6e1b\u53bb\u5e73\u5747 \u50b3\u9001\u5e73\u5747 (with DPCM or something) do VQ and send","title":"M/RVQ"},{"location":"Data%20Compression/#irvq","text":"\u5167\u63d2/\u9918\u503c VQ do subsampling to original image (assume \\(N\\times N\\) ) and would get sub-image ( \\(N/\u2113 \\times N/\u2113\\) , normally \\(\u2113=8\\) ). do up-sampling by interpolation, and get the residual image. split residual image to blocks and do VQ. pros perform better (less blocking artifacts) than M/RVQ.","title":"I/RVQ"},{"location":"Data%20Compression/#gsvq","text":"Gain/Shape VQ find and send the vector that match the most (has greatest dot product value)","title":"G/SVQ"},{"location":"Data%20Compression/#cvq","text":"Classification VQ split the image to blocks classify blocks to categories for every categories, there are some specific codebooks normally use many small codebooks, but can reach similar performance with the normal VQ using large codebooks.","title":"CVQ"},{"location":"Data%20Compression/#fsvq","text":"Finite State VQ Given codebooks per state \\(C(s_i)\\) transition function \\(s_i = f(s_{i-1}, Y_{i-1})\\) given \\(s_0\\) , and find \\(Y_0\\) by \\(C(s_0)\\) get \\(s_1 = f(s_0, Y_0)\\) , and find \\(Y_1\\) until all done cons one connection drop can cause serious consequence.","title":"FSVQ"},{"location":"Linear_Algebra/","text":"Vector Spaces \u00b6 Subspace \u00b6 Definition: skip due to complication That's say a vector space \\(\\mathbf V = \\text{Span}\\set \\mathbf {v_1, v_2, v_3}\\) , then the subspace \\(\\mathbf H=\\text{Span}\\set\\mathbf{v_1, v_2},\\quad\\text{Span}\\set\\mathbf{v_2, v_3}, \\dots\\) Thus it's intuitive that subspace \\(\\mathbf H\\) have the property: \\(\\set\\mathbf 0 \\in \\mathbf H\\) Null Space \u00b6 Definition for homogeneous equation \\[ \\begin{gather} \\mathbf {Ax}= \\mathbf 0 \\\\\\\\ \\text{Nul } \\mathbf A = \\set {\\mathbf x | \\mathbf x \\in \\mathbf R^{n} \\,\\cap\\, \\mathbf {Ax=0}} \\end{gather} \\] example \\[ \\begin{gather} A= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\qquad u= \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} \\\\\\\\ \\mathbf {Au}= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{gather} \\] Thus \\(\\mathbf u\\) is in \\(\\text{Nul }\\mathbf A\\) . Properties \u00b6 \\(\\text{Nul }\\mathbf A\\) is namely the solution space of \\(\\mathbf x\\) for the matrix \\(\\mathbf A_{m\\times n}\\) , \\(\\text{Nul }\\mathbf A\\) is a subspace of \\(\u211d^{n}\\) since \\(\\mathbf {Ax=0}\\) , thus \\(\\mathbf x\\) can be written as \\[ \\begin{gather} \\mathbf x= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\end{gather} \\] so this property is intuitive. Column Space \u00b6 Definition: For an \\(m\\times n\\) matrix \\(\\mathbf A=[a_1, a_2, \\dots, a_n]\\) \\[ \\begin{gather} \\text{Col }\\mathbf A= \\text{Span}\\set{a_1, a_2, \\dots, a_n} \\\\\\\\ a_i \\in \u211d^{m} \\end{gather} \\] or equivalently, \\[ \\begin{gather} \\text{Col }\\mathbf A=\\set{\\mathbf {Ax}|\\mathbf x \\in \u211d^{n}} \\end{gather} \\] Properties \u00b6 \\(\\text{Col }\\mathbf A\\) is a subspace of \\(\u211d^{m}\\) range of linear transform \\(\\mathbf {Ax}\\) \\(\\text{rank }\\mathbf A = \\text{dim}(\\text{Col } \\mathbf A)\\) \\(\\text{rank }\\mathbf A+\\text{dim}(\\text{Nul }\\mathbf A)=n\\) this is quite intuitive since when we lost \\(1\\) dimension in \\(\\text{Col }\\mathbf A\\) , we got \\(1\\) dimension in solution space. e.g. \\(3\\) equations in \\(\u211d^{3}\\) can yield an unique solution. However, \\(2\\) equations can only yield a line (i.e. one dimension) at most. Characteristic Equation \u00b6 Eigenvector & Eigenvalue \u00b6 Definition \\[ \\begin{gather} \\mathbf {Ax}= \\lambda \\mathbf x \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {(A-\\lambda I)\\,x}= 0 \\end{gather} \\] \\(n\\times n\\) matrix \\(\\mathbf A\\) , with the eigenvector \\(\\mathbf x\\) and eigenvalue \\(\\lambda\\) . note: \\(\\mathbf 0\\) cannot be an eigenvector (since it is trivial and by definition) 0 can be an eigenvalue. Orthogonality \u00b6 Orthogonal Complements \u00b6 A vector \\(\\mathbf x\\) is in \\(\\mathbf {W^{\\bot}}\\) iff \\(\\mathbf x\\) is orthogonal to every vector in \\(\\mathbf W\\) \\((\\text{Row }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf A\\) \\((\\text{Col }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf {A^{T}}\\) Orthogonal Projection \u00b6 Given a vector \\(\\mathbf y\\) and a subspace \\(\\mathbf W\\) in \\(\u211d^{n}\\) \\[ \\begin{align} \\mathbf y &= c_1 \\mathbf {u_1} + c_2 \\mathbf {u_2} + \\dots + c_n \\mathbf {u_n} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_n} \\\\ \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{align} \\] Let's say \\(\\mathbf W = \\text{Span }\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) ( \\(p \\le n\\) ) then the orthogonal projection of \\(\\mathbf y\\) onto \\(\\mathbf W\\) , \\(\\hat {\\mathbf y}\\) (aka \\(\\text{proj}_{\\mathbf W}\\mathbf y\\) ) \\[ \\begin{gather} \\hat {\\mathbf y} = \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\end{gather} \\] assume \\(\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) is an orthonormal basic. we can thus further write \\[ \\begin{align} \\hat {\\mathbf y} &= \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\\\\\\\ &= (\\mathbf {y \\cdot u_1})\\mathbf {u_1} + (\\mathbf {y \\cdot u_2})\\mathbf {u_2} + (\\mathbf {y \\cdot u_p})\\mathbf {u_p} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {y \\cdot u_1} \\\\ \\mathbf {y \\cdot u_2} \\\\ \\vdots \\\\ \\mathbf {y \\cdot u_p} \\\\ \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {u_1^{T}} \\\\ \\mathbf {u_2^{T}} \\\\ \\vdots \\\\ \\mathbf {u_p^{T}} \\\\ \\end{bmatrix} \\mathbf y \\\\\\\\ &= \\mathbf {UU^{T}y} \\end{align} \\] Thus, we can conclude that for an \\(n \\times p\\) matrix \\(\\mathbf U\\) which consists of \\(\\mathbf W\\) which is \\(p\\) -dimension subspace of \\(\u211d^{n}\\) , there is \\(\\mathbf {UU^{T}}\\) that project \\(\\mathbf y \\in \u211d^{n}\\) onto \\(\\mathbf W\\) . \\[ \\begin{gather} \\mathbf {UU^{T}y}=\\text{proj}_{\\text{W}}\\,\\mathbf y \\end{gather} \\] Diagonalization \u00b6 Diagonal Matrix \u00b6 \\(\\mathbf D\\) is said to be a diagonal matrix if \\(\\mathbf D\\) has the form \\[ \\begin{gather} \\mathbf D= \\begin{bmatrix} a & 0 & 0 & \\cdots \\\\ 0 & b & 0 & \\cdots \\\\ 0 & 0 & c & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\] thus \\(\\mathbf D\\) has the property \\[ \\begin{gather} \\mathbf D^{n}= \\begin{bmatrix} a^{n} & 0 & 0 & \\cdots \\\\ 0 & b^{n} & 0 & \\cdots \\\\ 0 & 0 & c^{n} & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\] Diagonalization \u00b6 Square matrix \\(\\mathbf A\\) is said to be diagonalizable if \\(\\mathbf {A=PDP^{-1}}\\) in which \\(\\mathbf {D}\\) is diagonal matrix \\(\\mathbf P\\) is invertible matrix. The Diagonalization Theorem \u00b6 An \\(n \\times n\\) matrix \\(\\mathbf A\\) is diagonalizable iff \\(\\mathbf A\\) has \\(n\\) eigenvectors and eigenvalues(may be multiple roots). Easy Proof Let's say an invertiable matrix \\(P_{n\\times n}\\) and diagnol matrix \\(D\\) \\[ \\begin{gather} \\mathbf P = [\\mathbf {v_1, v_2, \\dots , v_n}] \\qquad D = \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} \\\\\\\\ \\mathbf {AP}=\\mathbf{[Av_1, Av_2, \\dots]} \\\\\\\\ \\mathbf {PD}=[\\mathbf {v_1, v_2, \\dots , v_n}] \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} =[\\mathbf {\\lambda_1v_1,\\,\\lambda_2v_2,\\,\\dots}] \\end{gather} \\] by the definition of characteristic equation \\[ \\begin{gather} \\mathbf {AP=PD} \\\\\\\\ \\implies \\mathbf {A=PDP^{-1}} \\end{gather} \\] Orthogonally Diagonalizable \u00b6 Definition A matrix is said to be orthogonally diagonalizable if \\[ \\begin{gather} \\mathbf {A=PDP^{T}=PDP^{-1}} \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {A^{T}=P^{TT}D^{T}P^{T}=PDP^{T}=A} \\end{gather} \\] And iff \\(\\mathbf A\\) is a symmetric matrix. SVD \u00b6 S ingular V alue D iagonalization Singular Values \u00b6 Let \\(\\mathbf A\\) be an \\(m\\times n\\) matrix. Then \\[ \\begin{gather} ({\\mathbf {A^{T}A}})^{\\mathbf T}=\\mathbf {A^{T}A^{TT}}={\\mathbf {A^{T}A}} \\end{gather} \\] Thus, \\(\\mathbf {A^{T}A}\\) is symmetric and orthogonally diagonalizable. Then, Let say \\(\\{\\mathbf {v_1, v_2, \\dots, v_n}\\}\\) be an orthonormal basic for \\(\\mathbf \u211d^{n}\\) consisting of eigenvectors of \\(\\mathbf {A^T A}\\) And also, say \\(\\lambda_1, \\lambda_2, \\dots\\) be the eigenvalues of \\(\\mathbf {A^{T}A}\\) (make \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\) ) The singular values \\(\\sigma_i\\) of \\(\\mathbf A\\) are \\[ \\begin{gather} \\sigma_i = \\sqrt{\\lambda_i} \\end{gather} \\] Thinking collapse: none Singular values are the \"eigenvalues\" of non-square matrix \\(\\mathbf A\\) . Properties \u00b6 \\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_n}\\}\\) is an orthogonal set. for any \\(i\\neq j\\) , \\(\\mathbf {(Av_i)^{T}(Av_j)=v_i A^{T}Av_j=v_i\\lambda_j v_j}=0\\) \\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_r}\\}\\) is an orthogonal basic of \\(\\text{Col }\\mathbf A\\) . \\(\\iff \\text{rank }\\mathbf A = \\text{dim }\\text{Col }\\mathbf A=r\\) say \\(\\mathbf {y = Ax}\\) in \\(\\text{Col }\\mathbf A\\) , then \\[ \\begin{gather} \\begin{aligned} \\mathbf x &= c_1 \\mathbf v_1 + c_2 \\mathbf v_2 + \\dots + c_r \\mathbf v_r + \\dots + c_n \\mathbf v_n \\\\\\\\ &= c_1' \\mathbf v_1 + c_2' \\mathbf v_2 + \\dots + c_r' \\mathbf v_r \\end{aligned} \\\\\\\\ \\Downarrow \\\\\\\\ \\begin{aligned} \\mathbf y &=\\mathbf {Ax} \\\\\\\\ &= c_1' \\mathbf {Av_1} + c_2' \\mathbf {Av_2} + \\dots + c_r' \\mathbf {Av_r} \\end{aligned} \\end{gather} \\] Singular Value Decomposition \u00b6 \\(m\\times n\\) \"diagonal\" matrix \\[ \\begin{gather} \\mathbf \\Sigma_{m\\times n}= \\begin{bmatrix} \\mathbf D_{r\\times r} & \\mathbf 0_{r\\times (n-r)} \\\\ \\mathbf 0_{(m-r)\\times r} & \\mathbf 0_{(m-r)\\times (n-r)} \\\\ \\end{bmatrix} \\end{gather} \\] e.g. \\(\\mathbf A_{2\\times 3}\\) with \\(r=2\\) \\[ \\begin{gather} \\mathbf \\Sigma= \\begin{bmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ \\end{bmatrix} \\end{gather} \\] singular value decomposition \\[ \\begin{gather} \\mathbf A_{m\\times n}= \\mathbf U_{m\\times m} \\mathbf \\Sigma_{m\\times n} {\\mathbf V_{n\\times n}}^{\\mathbf T} \\end{gather} \\] Left singular vectors of \\(A\\,\\,\\) : The columns of \\(\\mathbf U\\) Right singular vectors of \\(A\\) : The columns of \\(\\mathbf V\\) Easy Proof First following the property 2. of singular value, we can obtain an orthonormal basic \\(\\{\\mathbf {u_1, u_2, \\dots, u_r}\\}\\) by \\[ \\begin{gather} \\mathbf {u_i} = \\mathbf {\\frac{Av_i}{||Av_i||}} = \\frac{\\mathbf {Av_i}}{\\sigma_i} \\end{gather} \\] then \\[ \\begin{gather} \\begin{aligned} \\mathbf {U\\Sigma} &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_m} \\end{bmatrix} \\begin{bmatrix} \\sigma_1 & 0 & \\cdots \\\\ 0 & \\sigma_2 & \\cdots \\\\ & & \\ddots \\end{bmatrix}_{m\\times n} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\\\\\\\\\\\ \\mathbf {AV} &= \\begin{bmatrix} \\mathbf {Av_1} & \\mathbf {Av_2} & \\cdots & \\mathbf {Av_n} \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\end{aligned} \\\\\\\\\\\\ \\implies \\mathbf {U\\Sigma=AV} \\implies \\mathbf {U\\Sigma V^{T}=A} \\end{gather} \\] Least-Squares Solution \u00b6 Target for \\(m \\times n\\) matrix \\(\\mathbf A\\) \\[ \\begin{gather} \\mathbf A= \\begin{bmatrix} \\mathbf {a_1} & \\mathbf {a_2} & \\cdots &\\mathbf {a_n} \\end{bmatrix} \\\\\\\\ \\mathbf {Ax=b} \\end{gather} \\] say \\(\\mathbf b\\) is a linear combination of columns of \\(\\mathbf A\\) for solution \\(\\mathbf x\\) . Most of times we cannot find the perfect solution. Thus, we try to find the Least-Squares solution \\(\\mathbf {\\hat x}\\) . Find \\[ \\begin{gather} \\mathbf {\\hat x} = \\mathop{\\arg\\min}_{\\mathbf {\\hat x}}{\\,||\\mathbf {b-A\\hat x}||}^{2} \\end{gather} \\] First, let define \\[ \\begin{gather} \\mathbf {p = A\\hat{x} \\in \\text{Col }A} \\end{gather} \\] We know that the least-square root happens at \\((\\mathbf {b-p})\\) orthogonal to \\(\\mathbf p\\) . Thus say the projection matrix \\(\\mathbf P\\) s.t. \\[\\mathbf {P b=p}\\] Back to the condition \\(\\mathbf {(b-p)\\cdot p}=0\\) . That is \\[ \\begin{align} \\mathbf p &\\in \\text{Col }\\mathbf {A} \\\\\\\\ \\mathbf {(b-p)} &\\in (\\text{Col }\\mathbf {A})^{\\bot} = \\text{Nul }\\mathbf {A^{T}} \\tag{a} \\end{align} \\] from the definition of null space and equation \\(a\\) , we have \\[ \\begin{gather} \\mathbf {A^{T}(b-p)=0} \\\\\\\\ \\implies \\mathbf {A^{T}(b-A\\hat x)=0} \\\\\\\\ \\implies \\mathbf {A^{T}A\\hat x=A^{T}\\,b} \\\\\\\\ \\implies \\mathbf {\\hat x=(A^{T}A)^{-1}A^{T}\\,b} \\end{gather} \\] Besides, we now further have \\[ \\begin{gather} \\mathbf {p=A\\hat x=A(A^{T}A)^{-1}A^{T}\\,b} \\tag{b} \\\\\\\\ \\mathbf {P = A(A^{T}A)^{-1}A^{T}} \\end{gather} \\] Pseudoinverse Aspect \u00b6 \\[ \\begin{gather} \\mathbf {A\\hat x=b} \\\\\\\\ \\implies \\mathbf {\\hat x=A^{\\dagger}\\,b} \\\\\\\\ \\implies \\mathbf {p=A\\hat x=AA^{\\dagger}\\,b} \\tag{c} \\end{gather} \\] compare equation \\(b\\) and \\(c\\) \\[ \\begin{align} \\mathbf {p}&=\\mathbf {A\\hat x=AA^{\\dagger}\\,b} \\\\\\\\ &= (\\mathbf {U_r D V_r^{T}})(\\mathbf {V_r D^{-1} U_r^{T}})\\,\\mathbf b \\\\\\\\ &= \\mathbf {U_r U_r^{T}}\\,\\mathbf {b} \\end{align} \\] by theorem of Orthogonal Projection , we know that \\[ \\begin{gather} \\mathbf {P} = \\mathbf {AA^{\\dagger}} = \\mathbf {U_r U_r^{T}} \\end{gather} \\]","title":"Linear Algebra"},{"location":"Linear_Algebra/#vector+spaces","text":"","title":"Vector Spaces"},{"location":"Linear_Algebra/#subspace","text":"Definition: skip due to complication That's say a vector space \\(\\mathbf V = \\text{Span}\\set \\mathbf {v_1, v_2, v_3}\\) , then the subspace \\(\\mathbf H=\\text{Span}\\set\\mathbf{v_1, v_2},\\quad\\text{Span}\\set\\mathbf{v_2, v_3}, \\dots\\) Thus it's intuitive that subspace \\(\\mathbf H\\) have the property: \\(\\set\\mathbf 0 \\in \\mathbf H\\)","title":"Subspace"},{"location":"Linear_Algebra/#null+space","text":"Definition for homogeneous equation \\[ \\begin{gather} \\mathbf {Ax}= \\mathbf 0 \\\\\\\\ \\text{Nul } \\mathbf A = \\set {\\mathbf x | \\mathbf x \\in \\mathbf R^{n} \\,\\cap\\, \\mathbf {Ax=0}} \\end{gather} \\] example \\[ \\begin{gather} A= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\qquad u= \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} \\\\\\\\ \\mathbf {Au}= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{gather} \\] Thus \\(\\mathbf u\\) is in \\(\\text{Nul }\\mathbf A\\) .","title":"Null Space"},{"location":"Linear_Algebra/#properties","text":"\\(\\text{Nul }\\mathbf A\\) is namely the solution space of \\(\\mathbf x\\) for the matrix \\(\\mathbf A_{m\\times n}\\) , \\(\\text{Nul }\\mathbf A\\) is a subspace of \\(\u211d^{n}\\) since \\(\\mathbf {Ax=0}\\) , thus \\(\\mathbf x\\) can be written as \\[ \\begin{gather} \\mathbf x= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\end{gather} \\] so this property is intuitive.","title":"Properties"},{"location":"Linear_Algebra/#column+space","text":"Definition: For an \\(m\\times n\\) matrix \\(\\mathbf A=[a_1, a_2, \\dots, a_n]\\) \\[ \\begin{gather} \\text{Col }\\mathbf A= \\text{Span}\\set{a_1, a_2, \\dots, a_n} \\\\\\\\ a_i \\in \u211d^{m} \\end{gather} \\] or equivalently, \\[ \\begin{gather} \\text{Col }\\mathbf A=\\set{\\mathbf {Ax}|\\mathbf x \\in \u211d^{n}} \\end{gather} \\]","title":"Column Space"},{"location":"Linear_Algebra/#properties_1","text":"\\(\\text{Col }\\mathbf A\\) is a subspace of \\(\u211d^{m}\\) range of linear transform \\(\\mathbf {Ax}\\) \\(\\text{rank }\\mathbf A = \\text{dim}(\\text{Col } \\mathbf A)\\) \\(\\text{rank }\\mathbf A+\\text{dim}(\\text{Nul }\\mathbf A)=n\\) this is quite intuitive since when we lost \\(1\\) dimension in \\(\\text{Col }\\mathbf A\\) , we got \\(1\\) dimension in solution space. e.g. \\(3\\) equations in \\(\u211d^{3}\\) can yield an unique solution. However, \\(2\\) equations can only yield a line (i.e. one dimension) at most.","title":"Properties"},{"location":"Linear_Algebra/#characteristic+equation","text":"","title":"Characteristic Equation"},{"location":"Linear_Algebra/#eigenvector++eigenvalue","text":"Definition \\[ \\begin{gather} \\mathbf {Ax}= \\lambda \\mathbf x \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {(A-\\lambda I)\\,x}= 0 \\end{gather} \\] \\(n\\times n\\) matrix \\(\\mathbf A\\) , with the eigenvector \\(\\mathbf x\\) and eigenvalue \\(\\lambda\\) . note: \\(\\mathbf 0\\) cannot be an eigenvector (since it is trivial and by definition) 0 can be an eigenvalue.","title":"Eigenvector &amp; Eigenvalue"},{"location":"Linear_Algebra/#orthogonality","text":"","title":"Orthogonality"},{"location":"Linear_Algebra/#orthogonal+complements","text":"A vector \\(\\mathbf x\\) is in \\(\\mathbf {W^{\\bot}}\\) iff \\(\\mathbf x\\) is orthogonal to every vector in \\(\\mathbf W\\) \\((\\text{Row }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf A\\) \\((\\text{Col }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf {A^{T}}\\)","title":"Orthogonal Complements"},{"location":"Linear_Algebra/#orthogonal+projection","text":"Given a vector \\(\\mathbf y\\) and a subspace \\(\\mathbf W\\) in \\(\u211d^{n}\\) \\[ \\begin{align} \\mathbf y &= c_1 \\mathbf {u_1} + c_2 \\mathbf {u_2} + \\dots + c_n \\mathbf {u_n} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_n} \\\\ \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{align} \\] Let's say \\(\\mathbf W = \\text{Span }\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) ( \\(p \\le n\\) ) then the orthogonal projection of \\(\\mathbf y\\) onto \\(\\mathbf W\\) , \\(\\hat {\\mathbf y}\\) (aka \\(\\text{proj}_{\\mathbf W}\\mathbf y\\) ) \\[ \\begin{gather} \\hat {\\mathbf y} = \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\end{gather} \\] assume \\(\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) is an orthonormal basic. we can thus further write \\[ \\begin{align} \\hat {\\mathbf y} &= \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\\\\\\\ &= (\\mathbf {y \\cdot u_1})\\mathbf {u_1} + (\\mathbf {y \\cdot u_2})\\mathbf {u_2} + (\\mathbf {y \\cdot u_p})\\mathbf {u_p} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {y \\cdot u_1} \\\\ \\mathbf {y \\cdot u_2} \\\\ \\vdots \\\\ \\mathbf {y \\cdot u_p} \\\\ \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {u_1^{T}} \\\\ \\mathbf {u_2^{T}} \\\\ \\vdots \\\\ \\mathbf {u_p^{T}} \\\\ \\end{bmatrix} \\mathbf y \\\\\\\\ &= \\mathbf {UU^{T}y} \\end{align} \\] Thus, we can conclude that for an \\(n \\times p\\) matrix \\(\\mathbf U\\) which consists of \\(\\mathbf W\\) which is \\(p\\) -dimension subspace of \\(\u211d^{n}\\) , there is \\(\\mathbf {UU^{T}}\\) that project \\(\\mathbf y \\in \u211d^{n}\\) onto \\(\\mathbf W\\) . \\[ \\begin{gather} \\mathbf {UU^{T}y}=\\text{proj}_{\\text{W}}\\,\\mathbf y \\end{gather} \\]","title":"Orthogonal Projection"},{"location":"Linear_Algebra/#diagonalization","text":"","title":"Diagonalization"},{"location":"Linear_Algebra/#diagonal+matrix","text":"\\(\\mathbf D\\) is said to be a diagonal matrix if \\(\\mathbf D\\) has the form \\[ \\begin{gather} \\mathbf D= \\begin{bmatrix} a & 0 & 0 & \\cdots \\\\ 0 & b & 0 & \\cdots \\\\ 0 & 0 & c & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\] thus \\(\\mathbf D\\) has the property \\[ \\begin{gather} \\mathbf D^{n}= \\begin{bmatrix} a^{n} & 0 & 0 & \\cdots \\\\ 0 & b^{n} & 0 & \\cdots \\\\ 0 & 0 & c^{n} & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\]","title":"Diagonal Matrix"},{"location":"Linear_Algebra/#diagonalization_1","text":"Square matrix \\(\\mathbf A\\) is said to be diagonalizable if \\(\\mathbf {A=PDP^{-1}}\\) in which \\(\\mathbf {D}\\) is diagonal matrix \\(\\mathbf P\\) is invertible matrix.","title":"Diagonalization"},{"location":"Linear_Algebra/#the+diagonalization+theorem","text":"An \\(n \\times n\\) matrix \\(\\mathbf A\\) is diagonalizable iff \\(\\mathbf A\\) has \\(n\\) eigenvectors and eigenvalues(may be multiple roots). Easy Proof Let's say an invertiable matrix \\(P_{n\\times n}\\) and diagnol matrix \\(D\\) \\[ \\begin{gather} \\mathbf P = [\\mathbf {v_1, v_2, \\dots , v_n}] \\qquad D = \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} \\\\\\\\ \\mathbf {AP}=\\mathbf{[Av_1, Av_2, \\dots]} \\\\\\\\ \\mathbf {PD}=[\\mathbf {v_1, v_2, \\dots , v_n}] \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} =[\\mathbf {\\lambda_1v_1,\\,\\lambda_2v_2,\\,\\dots}] \\end{gather} \\] by the definition of characteristic equation \\[ \\begin{gather} \\mathbf {AP=PD} \\\\\\\\ \\implies \\mathbf {A=PDP^{-1}} \\end{gather} \\]","title":"The Diagonalization Theorem"},{"location":"Linear_Algebra/#orthogonally+diagonalizable","text":"Definition A matrix is said to be orthogonally diagonalizable if \\[ \\begin{gather} \\mathbf {A=PDP^{T}=PDP^{-1}} \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {A^{T}=P^{TT}D^{T}P^{T}=PDP^{T}=A} \\end{gather} \\] And iff \\(\\mathbf A\\) is a symmetric matrix.","title":"Orthogonally Diagonalizable"},{"location":"Linear_Algebra/#svd","text":"S ingular V alue D iagonalization","title":"SVD"},{"location":"Linear_Algebra/#singular+values","text":"Let \\(\\mathbf A\\) be an \\(m\\times n\\) matrix. Then \\[ \\begin{gather} ({\\mathbf {A^{T}A}})^{\\mathbf T}=\\mathbf {A^{T}A^{TT}}={\\mathbf {A^{T}A}} \\end{gather} \\] Thus, \\(\\mathbf {A^{T}A}\\) is symmetric and orthogonally diagonalizable. Then, Let say \\(\\{\\mathbf {v_1, v_2, \\dots, v_n}\\}\\) be an orthonormal basic for \\(\\mathbf \u211d^{n}\\) consisting of eigenvectors of \\(\\mathbf {A^T A}\\) And also, say \\(\\lambda_1, \\lambda_2, \\dots\\) be the eigenvalues of \\(\\mathbf {A^{T}A}\\) (make \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\) ) The singular values \\(\\sigma_i\\) of \\(\\mathbf A\\) are \\[ \\begin{gather} \\sigma_i = \\sqrt{\\lambda_i} \\end{gather} \\] Thinking collapse: none Singular values are the \"eigenvalues\" of non-square matrix \\(\\mathbf A\\) .","title":"Singular Values"},{"location":"Linear_Algebra/#properties_2","text":"\\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_n}\\}\\) is an orthogonal set. for any \\(i\\neq j\\) , \\(\\mathbf {(Av_i)^{T}(Av_j)=v_i A^{T}Av_j=v_i\\lambda_j v_j}=0\\) \\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_r}\\}\\) is an orthogonal basic of \\(\\text{Col }\\mathbf A\\) . \\(\\iff \\text{rank }\\mathbf A = \\text{dim }\\text{Col }\\mathbf A=r\\) say \\(\\mathbf {y = Ax}\\) in \\(\\text{Col }\\mathbf A\\) , then \\[ \\begin{gather} \\begin{aligned} \\mathbf x &= c_1 \\mathbf v_1 + c_2 \\mathbf v_2 + \\dots + c_r \\mathbf v_r + \\dots + c_n \\mathbf v_n \\\\\\\\ &= c_1' \\mathbf v_1 + c_2' \\mathbf v_2 + \\dots + c_r' \\mathbf v_r \\end{aligned} \\\\\\\\ \\Downarrow \\\\\\\\ \\begin{aligned} \\mathbf y &=\\mathbf {Ax} \\\\\\\\ &= c_1' \\mathbf {Av_1} + c_2' \\mathbf {Av_2} + \\dots + c_r' \\mathbf {Av_r} \\end{aligned} \\end{gather} \\]","title":"Properties"},{"location":"Linear_Algebra/#singular+value+decomposition","text":"\\(m\\times n\\) \"diagonal\" matrix \\[ \\begin{gather} \\mathbf \\Sigma_{m\\times n}= \\begin{bmatrix} \\mathbf D_{r\\times r} & \\mathbf 0_{r\\times (n-r)} \\\\ \\mathbf 0_{(m-r)\\times r} & \\mathbf 0_{(m-r)\\times (n-r)} \\\\ \\end{bmatrix} \\end{gather} \\] e.g. \\(\\mathbf A_{2\\times 3}\\) with \\(r=2\\) \\[ \\begin{gather} \\mathbf \\Sigma= \\begin{bmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ \\end{bmatrix} \\end{gather} \\] singular value decomposition \\[ \\begin{gather} \\mathbf A_{m\\times n}= \\mathbf U_{m\\times m} \\mathbf \\Sigma_{m\\times n} {\\mathbf V_{n\\times n}}^{\\mathbf T} \\end{gather} \\] Left singular vectors of \\(A\\,\\,\\) : The columns of \\(\\mathbf U\\) Right singular vectors of \\(A\\) : The columns of \\(\\mathbf V\\) Easy Proof First following the property 2. of singular value, we can obtain an orthonormal basic \\(\\{\\mathbf {u_1, u_2, \\dots, u_r}\\}\\) by \\[ \\begin{gather} \\mathbf {u_i} = \\mathbf {\\frac{Av_i}{||Av_i||}} = \\frac{\\mathbf {Av_i}}{\\sigma_i} \\end{gather} \\] then \\[ \\begin{gather} \\begin{aligned} \\mathbf {U\\Sigma} &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_m} \\end{bmatrix} \\begin{bmatrix} \\sigma_1 & 0 & \\cdots \\\\ 0 & \\sigma_2 & \\cdots \\\\ & & \\ddots \\end{bmatrix}_{m\\times n} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\\\\\\\\\\\ \\mathbf {AV} &= \\begin{bmatrix} \\mathbf {Av_1} & \\mathbf {Av_2} & \\cdots & \\mathbf {Av_n} \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\end{aligned} \\\\\\\\\\\\ \\implies \\mathbf {U\\Sigma=AV} \\implies \\mathbf {U\\Sigma V^{T}=A} \\end{gather} \\]","title":"Singular Value Decomposition"},{"location":"Linear_Algebra/#least-squares+solution","text":"Target for \\(m \\times n\\) matrix \\(\\mathbf A\\) \\[ \\begin{gather} \\mathbf A= \\begin{bmatrix} \\mathbf {a_1} & \\mathbf {a_2} & \\cdots &\\mathbf {a_n} \\end{bmatrix} \\\\\\\\ \\mathbf {Ax=b} \\end{gather} \\] say \\(\\mathbf b\\) is a linear combination of columns of \\(\\mathbf A\\) for solution \\(\\mathbf x\\) . Most of times we cannot find the perfect solution. Thus, we try to find the Least-Squares solution \\(\\mathbf {\\hat x}\\) . Find \\[ \\begin{gather} \\mathbf {\\hat x} = \\mathop{\\arg\\min}_{\\mathbf {\\hat x}}{\\,||\\mathbf {b-A\\hat x}||}^{2} \\end{gather} \\] First, let define \\[ \\begin{gather} \\mathbf {p = A\\hat{x} \\in \\text{Col }A} \\end{gather} \\] We know that the least-square root happens at \\((\\mathbf {b-p})\\) orthogonal to \\(\\mathbf p\\) . Thus say the projection matrix \\(\\mathbf P\\) s.t. \\[\\mathbf {P b=p}\\] Back to the condition \\(\\mathbf {(b-p)\\cdot p}=0\\) . That is \\[ \\begin{align} \\mathbf p &\\in \\text{Col }\\mathbf {A} \\\\\\\\ \\mathbf {(b-p)} &\\in (\\text{Col }\\mathbf {A})^{\\bot} = \\text{Nul }\\mathbf {A^{T}} \\tag{a} \\end{align} \\] from the definition of null space and equation \\(a\\) , we have \\[ \\begin{gather} \\mathbf {A^{T}(b-p)=0} \\\\\\\\ \\implies \\mathbf {A^{T}(b-A\\hat x)=0} \\\\\\\\ \\implies \\mathbf {A^{T}A\\hat x=A^{T}\\,b} \\\\\\\\ \\implies \\mathbf {\\hat x=(A^{T}A)^{-1}A^{T}\\,b} \\end{gather} \\] Besides, we now further have \\[ \\begin{gather} \\mathbf {p=A\\hat x=A(A^{T}A)^{-1}A^{T}\\,b} \\tag{b} \\\\\\\\ \\mathbf {P = A(A^{T}A)^{-1}A^{T}} \\end{gather} \\]","title":"Least-Squares Solution"},{"location":"Linear_Algebra/#pseudoinverse+aspect","text":"\\[ \\begin{gather} \\mathbf {A\\hat x=b} \\\\\\\\ \\implies \\mathbf {\\hat x=A^{\\dagger}\\,b} \\\\\\\\ \\implies \\mathbf {p=A\\hat x=AA^{\\dagger}\\,b} \\tag{c} \\end{gather} \\] compare equation \\(b\\) and \\(c\\) \\[ \\begin{align} \\mathbf {p}&=\\mathbf {A\\hat x=AA^{\\dagger}\\,b} \\\\\\\\ &= (\\mathbf {U_r D V_r^{T}})(\\mathbf {V_r D^{-1} U_r^{T}})\\,\\mathbf b \\\\\\\\ &= \\mathbf {U_r U_r^{T}}\\,\\mathbf {b} \\end{align} \\] by theorem of Orthogonal Projection , we know that \\[ \\begin{gather} \\mathbf {P} = \\mathbf {AA^{\\dagger}} = \\mathbf {U_r U_r^{T}} \\end{gather} \\]","title":"Pseudoinverse Aspect"},{"location":"test/","text":"\\(x\\) -circles -- \u4e2d\u6587\u6a19\u984c \u00b6 break line \u00b6 strict break line strict break line code block \u00b6 for ( int i = 0 ; i < n ; i ++ ) printf ( \"hello world\" ); Latex \u00b6 \\[ abc \\] \\[ \\begin{gather} \\alpha\\beta\\gamma \\end{gather} \\] eq1 how \\[ \\begin{gather} how = are \\end{gather} \\] test 2. eq2 \\[ \\begin{gather} \\theta^{*} = \\arg\\max_\\theta{\\, E_{\\mathbf{X_{t}}\\sim D}\\bigg\\{\\bigg\\|\\text{Net}_\\theta(\\mathbf{X_{t0:7}}, D)- \\mathbf{X_{t8}}\\bigg\\|^{2}\\bigg\\}} \\end{gather} \\] Admonitions \u00b6 [!note]+ here is the info $$ \\begin{gather} F(x) = \\int f(x)dx \\end{gather} $$ new line $$ \\begin{gather} hello \\end{gather} $$","title":"Test"},{"location":"test/#\u4e2d\u6587\u6a19\u984c","text":"","title":"\u4e2d\u6587\u6a19\u984c"},{"location":"test/#break+line","text":"strict break line strict break line","title":"break line"},{"location":"test/#code+block","text":"for ( int i = 0 ; i < n ; i ++ ) printf ( \"hello world\" );","title":"code block"},{"location":"test/#latex","text":"\\[ abc \\] \\[ \\begin{gather} \\alpha\\beta\\gamma \\end{gather} \\] eq1 how \\[ \\begin{gather} how = are \\end{gather} \\] test 2. eq2 \\[ \\begin{gather} \\theta^{*} = \\arg\\max_\\theta{\\, E_{\\mathbf{X_{t}}\\sim D}\\bigg\\{\\bigg\\|\\text{Net}_\\theta(\\mathbf{X_{t0:7}}, D)- \\mathbf{X_{t8}}\\bigg\\|^{2}\\bigg\\}} \\end{gather} \\]","title":"Latex"},{"location":"test/#admonitions","text":"[!note]+ here is the info $$ \\begin{gather} F(x) = \\int f(x)dx \\end{gather} $$ new line $$ \\begin{gather} hello \\end{gather} $$","title":"Admonitions"},{"location":"Control%20Engineering/mid1/","text":"DC Motor \u00b6 emf voltage: \\(e\\) armature current: \\(i_a\\) viscous friction coefficient: \\(b\\) inertia: \\(J_m\\) \\[ \\begin{gather} \\tau = K_t\\ i_a \\\\\\\\ e = K_e\\ \\dot\\theta_m \\\\\\\\ K_t = K_e \\ (\\text{for same unit}) \\end{gather} \\] Modeling voltage across motor: \\(v_a\\) \\[ \\begin{gather} \\tau = J_m\\ddot\\theta + b\\dot \\theta = K_t\\ i_a \\\\\\\\ v_a = L_a\\ i_a' + r_a i_a + K_e\\ \\dot\\theta_m \\end{gather} \\] Mason's Rule \u00b6 \\[ \\begin{gather} G(s) = \\frac{V(s)}{U(s)}= \\frac{\\sum_i{G_i\\ \\Delta_i}}{\\Delta} \\end{gather} \\] \\[ \\begin{align} \\Delta = 1 &- \\text{sum of all individual loop gains} \\\\\\\\ &+ \\text{sum of gain products of all possible two loops which do not touch} \\\\\\\\ &- \\text{sum of gain products of all possible three loops which do not touch} \\\\\\\\ &+ \\dots \\end{align} \\] path gain of the \\(i\\) th forward path \\(G_i\\) value of \\(\\Delta\\) for the part of the signal-flow graph that does not touch the \\(i\\) th forward path. see example . Second System Response \u00b6 Transfer function \u00b6 \\[ \\begin{gather} H(s) = \\frac{w_{n}^{2}}{s^{2}+2\\zeta \\omega_n s + \\omega_n^{2}} \\end{gather} \\] Poles: \\(\\sigma = \\zeta \\omega_n\\) \\(\\omega_d = \\omega_n\\sqrt{1-\\zeta^{2}}\\) \\[ \\begin{gather} s = - \\zeta \\omega_n \\pm j \\omega_n\\sqrt{1-\\zeta^{2}} = -\\sigma \\pm j\\omega_d \\end{gather} \\] Unit Step Input \u00b6 \\[ \\begin{gather} Y(s) = \\frac{1}{s}H(s) = \\frac{1}{s} - \\frac{s+2\\zeta \\omega_n}{s^{2}+2\\zeta \\omega_n s + \\omega_n^{2}} \\end{gather} \\] after some trivial calculation, \\[ \\begin{gather} y(t) = 1 - K\\ e^{-\\sigma t}\\ \\sin(\\omega_dt + \\theta) \\end{gather} \\] note that \\(K\\) is not important. Peak Time \u00b6 peak Time \\(t_p\\) \\[ \\begin{align} y' &= 0 \\\\\\\\ y' &= K' e^{-\\sigma t}\\sin(\\omega_d t) \\end{align} \\] Max Overshoot \u00b6 max overshoot happens at \\(\\omega_d \\ t_p = \\pi\\) \\[ \\begin{gather} t_p = \\frac{\\pi}{\\omega_d} \\end{gather} \\] overshoot \\(M_p\\) (memorize directly) \\[ \\begin{align} y(t_p) &= 1 + M_p \\\\\\\\ &= 1 + e^{-\\sigma\\pi/\\omega_d} \\end{align} \\] \\[ \\begin{gather} \\implies M_p = e^{-\\sigma\\pi/\\omega_d} = e^{-\\sigma\\, t_p} \\end{gather} \\] rise time \u00b6 \\(t_r \\equiv t_2 - t1\\) for characteristic equation: \\(s+1/\\tau\\) \\[ \\left\\{ \\begin{gather} e^{-t_1/\\tau}=0.9 \\\\\\\\ e^{-t_2/\\tau}=0.1 \\end{gather}\\right. \\] \\[ \\begin{gather} t_r=t_2-t_1=\\tau(-\\ln0.1-(-\\ln0.9))=\\tau\\ln9\\approx2\\tau \\end{gather} \\] for characteristic equation: \\(s^{2}+2\\zeta \\omega_n+\\omega_n^{2}\\) \\[ \\begin{align} t_r &\\approx \\frac{1.8}{\\omega_n} \\\\\\\\ &\\approx \\frac{0.8+1.1\\zeta+1.4 \\zeta^{2}}{\\omega_n} \\end{align} \\] settling time \u00b6 \\(t_s\\) (for steady state error 1%) for characteristic equation: \\(s+1/\\tau\\) \\[ \\begin{gather} e^{-t_s/\\tau} = 0.01 \\\\\\\\ \\implies -t_s/\\tau = \\ln0.01 \\\\\\\\ t_s = -\\ln 0.01 \\tau \\approx 4.6 \\tau \\end{gather} \\] for characteristic equation: \\(s^{2}+2\\zeta \\omega_n+\\omega_n^{2}=(s-(\\sigma+j\\omega_d))(s-(\\sigma-j\\omega_d))\\) \\[ \\begin{gather} t_s = -\\frac{\\ln 0.01}{\\sigma} \\end{gather} \\] Extra zero \u00b6 \\[ \\begin{gather} H(s) = K\\frac{s+\\alpha\\sigma}{(s+(\\sigma+j\\omega_d))(s+(\\sigma-j\\omega_d))} \\end{gather} \\] when \\(\\alpha\\) is small ( \\(\\alpha <4\\) ), the extra zero would increase the overshoot \\(M_p\\) . when \\(\\alpha \\to \\infty\\) , zero would be trivial. Extra pole \u00b6 \\[ \\begin{gather} H(s) = K\\frac{1}{(s+\\alpha \\sigma)(s+(\\sigma+j\\omega_d))(s+(\\sigma-j\\omega_d))} \\end{gather} \\] when \\(\\alpha\\) is small ( \\(\\alpha < 4\\) ), the extra pole decrease the rising time \\(t_r\\) . when \\(\\alpha \\to \\infty\\) , the extra pole is trivial. Final Value Theorem \u00b6 poles have to be on left-half plane (converge) \\[ \\begin{gather} y(\\infty) = \\left[sY(s)\\right]_{s\\to 0} \\end{gather} \\] proof: \\[ \\begin{gather} L\\left\\{y'\\right\\}_{s\\to0} =\\left[sY(s)-y(0)\\right]_{s\\to 0}= \\int_0^{\\infty}{e^{-st}y' dt} = \\int_0^{\\infty}{y'dt} = y(\\infty) - y(0) \\\\\\\\ \\implies \\left[sY(s)\\right]_{s\\to0} = y(\\infty) \\end{gather} \\] Initial value theorem \\[ \\begin{gather} \\left[sF(s)\\right]_{s\\to \\infty} = f(0^+) \\\\\\\\ \\end{gather} \\] proof is similar to final value theorem. Routh-Hurwitz stability criterion \u00b6 \\[ \\begin{gather} s^{n} + a_1 s^{n-1} + a_2 s^{n-2} +\\dots + a_{n-1}s +a_n=0 \\end{gather} \\] The system would be stable iff the elements of the first column ( \\(a_1, b_1, c_1, \\dots\\) ) are all positive . In addition, for a stable system, the coefficients of polynomial are all positive (not even \\(0\\) ). A simple explanation is that for a stable system which have its all roots at L.H.P, we can write \\[ \\begin{gather} (s+n_1)(s+n_2)(s+n_3)\\dots \\cdot (s+n_n)=0 \\end{gather} \\] # of roots in the RHP == # of sign changes in the first column. example 3.32 \\[ \\begin{gather} a(s) = s^{6} + 4s^{5}+3s^{4}+2s^{3}+s^{2}+4s+4 \\end{gather} \\] first column \\(s^{6}\\) 1 \\(s^{5}\\) 4 \\(s^{4}\\) 5/2 \\(s^{3}\\) 2 \\(s^{2}\\) 3 \\(s^{1}\\) -76/15 \\(s^{0}\\) 4 There are two sign changes, \\(s^{2}\\) to \\(s^{1}\\) , and \\(s^{1}\\) to \\(s^{0}\\) . Thus, There are two roots in RHP","title":"Mid1"},{"location":"Control%20Engineering/mid1/#dc+motor","text":"emf voltage: \\(e\\) armature current: \\(i_a\\) viscous friction coefficient: \\(b\\) inertia: \\(J_m\\) \\[ \\begin{gather} \\tau = K_t\\ i_a \\\\\\\\ e = K_e\\ \\dot\\theta_m \\\\\\\\ K_t = K_e \\ (\\text{for same unit}) \\end{gather} \\] Modeling voltage across motor: \\(v_a\\) \\[ \\begin{gather} \\tau = J_m\\ddot\\theta + b\\dot \\theta = K_t\\ i_a \\\\\\\\ v_a = L_a\\ i_a' + r_a i_a + K_e\\ \\dot\\theta_m \\end{gather} \\]","title":"DC Motor"},{"location":"Control%20Engineering/mid1/#masons+rule","text":"\\[ \\begin{gather} G(s) = \\frac{V(s)}{U(s)}= \\frac{\\sum_i{G_i\\ \\Delta_i}}{\\Delta} \\end{gather} \\] \\[ \\begin{align} \\Delta = 1 &- \\text{sum of all individual loop gains} \\\\\\\\ &+ \\text{sum of gain products of all possible two loops which do not touch} \\\\\\\\ &- \\text{sum of gain products of all possible three loops which do not touch} \\\\\\\\ &+ \\dots \\end{align} \\] path gain of the \\(i\\) th forward path \\(G_i\\) value of \\(\\Delta\\) for the part of the signal-flow graph that does not touch the \\(i\\) th forward path. see example .","title":"Mason's Rule"},{"location":"Control%20Engineering/mid1/#second+system+response","text":"","title":"Second System Response"},{"location":"Control%20Engineering/mid1/#transfer+function","text":"\\[ \\begin{gather} H(s) = \\frac{w_{n}^{2}}{s^{2}+2\\zeta \\omega_n s + \\omega_n^{2}} \\end{gather} \\] Poles: \\(\\sigma = \\zeta \\omega_n\\) \\(\\omega_d = \\omega_n\\sqrt{1-\\zeta^{2}}\\) \\[ \\begin{gather} s = - \\zeta \\omega_n \\pm j \\omega_n\\sqrt{1-\\zeta^{2}} = -\\sigma \\pm j\\omega_d \\end{gather} \\]","title":"Transfer function"},{"location":"Control%20Engineering/mid1/#unit+step+input","text":"\\[ \\begin{gather} Y(s) = \\frac{1}{s}H(s) = \\frac{1}{s} - \\frac{s+2\\zeta \\omega_n}{s^{2}+2\\zeta \\omega_n s + \\omega_n^{2}} \\end{gather} \\] after some trivial calculation, \\[ \\begin{gather} y(t) = 1 - K\\ e^{-\\sigma t}\\ \\sin(\\omega_dt + \\theta) \\end{gather} \\] note that \\(K\\) is not important.","title":"Unit Step Input"},{"location":"Control%20Engineering/mid1/#peak+time","text":"peak Time \\(t_p\\) \\[ \\begin{align} y' &= 0 \\\\\\\\ y' &= K' e^{-\\sigma t}\\sin(\\omega_d t) \\end{align} \\]","title":"Peak Time"},{"location":"Control%20Engineering/mid1/#max+overshoot","text":"max overshoot happens at \\(\\omega_d \\ t_p = \\pi\\) \\[ \\begin{gather} t_p = \\frac{\\pi}{\\omega_d} \\end{gather} \\] overshoot \\(M_p\\) (memorize directly) \\[ \\begin{align} y(t_p) &= 1 + M_p \\\\\\\\ &= 1 + e^{-\\sigma\\pi/\\omega_d} \\end{align} \\] \\[ \\begin{gather} \\implies M_p = e^{-\\sigma\\pi/\\omega_d} = e^{-\\sigma\\, t_p} \\end{gather} \\]","title":"Max Overshoot"},{"location":"Control%20Engineering/mid1/#rise+time","text":"\\(t_r \\equiv t_2 - t1\\) for characteristic equation: \\(s+1/\\tau\\) \\[ \\left\\{ \\begin{gather} e^{-t_1/\\tau}=0.9 \\\\\\\\ e^{-t_2/\\tau}=0.1 \\end{gather}\\right. \\] \\[ \\begin{gather} t_r=t_2-t_1=\\tau(-\\ln0.1-(-\\ln0.9))=\\tau\\ln9\\approx2\\tau \\end{gather} \\] for characteristic equation: \\(s^{2}+2\\zeta \\omega_n+\\omega_n^{2}\\) \\[ \\begin{align} t_r &\\approx \\frac{1.8}{\\omega_n} \\\\\\\\ &\\approx \\frac{0.8+1.1\\zeta+1.4 \\zeta^{2}}{\\omega_n} \\end{align} \\]","title":"rise time"},{"location":"Control%20Engineering/mid1/#settling+time","text":"\\(t_s\\) (for steady state error 1%) for characteristic equation: \\(s+1/\\tau\\) \\[ \\begin{gather} e^{-t_s/\\tau} = 0.01 \\\\\\\\ \\implies -t_s/\\tau = \\ln0.01 \\\\\\\\ t_s = -\\ln 0.01 \\tau \\approx 4.6 \\tau \\end{gather} \\] for characteristic equation: \\(s^{2}+2\\zeta \\omega_n+\\omega_n^{2}=(s-(\\sigma+j\\omega_d))(s-(\\sigma-j\\omega_d))\\) \\[ \\begin{gather} t_s = -\\frac{\\ln 0.01}{\\sigma} \\end{gather} \\]","title":"settling time"},{"location":"Control%20Engineering/mid1/#extra+zero","text":"\\[ \\begin{gather} H(s) = K\\frac{s+\\alpha\\sigma}{(s+(\\sigma+j\\omega_d))(s+(\\sigma-j\\omega_d))} \\end{gather} \\] when \\(\\alpha\\) is small ( \\(\\alpha <4\\) ), the extra zero would increase the overshoot \\(M_p\\) . when \\(\\alpha \\to \\infty\\) , zero would be trivial.","title":"Extra zero"},{"location":"Control%20Engineering/mid1/#extra+pole","text":"\\[ \\begin{gather} H(s) = K\\frac{1}{(s+\\alpha \\sigma)(s+(\\sigma+j\\omega_d))(s+(\\sigma-j\\omega_d))} \\end{gather} \\] when \\(\\alpha\\) is small ( \\(\\alpha < 4\\) ), the extra pole decrease the rising time \\(t_r\\) . when \\(\\alpha \\to \\infty\\) , the extra pole is trivial.","title":"Extra pole"},{"location":"Control%20Engineering/mid1/#final+value+theorem","text":"poles have to be on left-half plane (converge) \\[ \\begin{gather} y(\\infty) = \\left[sY(s)\\right]_{s\\to 0} \\end{gather} \\] proof: \\[ \\begin{gather} L\\left\\{y'\\right\\}_{s\\to0} =\\left[sY(s)-y(0)\\right]_{s\\to 0}= \\int_0^{\\infty}{e^{-st}y' dt} = \\int_0^{\\infty}{y'dt} = y(\\infty) - y(0) \\\\\\\\ \\implies \\left[sY(s)\\right]_{s\\to0} = y(\\infty) \\end{gather} \\] Initial value theorem \\[ \\begin{gather} \\left[sF(s)\\right]_{s\\to \\infty} = f(0^+) \\\\\\\\ \\end{gather} \\] proof is similar to final value theorem.","title":"Final Value Theorem"},{"location":"Control%20Engineering/mid1/#routh-hurwitz+stability+criterion","text":"\\[ \\begin{gather} s^{n} + a_1 s^{n-1} + a_2 s^{n-2} +\\dots + a_{n-1}s +a_n=0 \\end{gather} \\] The system would be stable iff the elements of the first column ( \\(a_1, b_1, c_1, \\dots\\) ) are all positive . In addition, for a stable system, the coefficients of polynomial are all positive (not even \\(0\\) ). A simple explanation is that for a stable system which have its all roots at L.H.P, we can write \\[ \\begin{gather} (s+n_1)(s+n_2)(s+n_3)\\dots \\cdot (s+n_n)=0 \\end{gather} \\] # of roots in the RHP == # of sign changes in the first column. example 3.32 \\[ \\begin{gather} a(s) = s^{6} + 4s^{5}+3s^{4}+2s^{3}+s^{2}+4s+4 \\end{gather} \\] first column \\(s^{6}\\) 1 \\(s^{5}\\) 4 \\(s^{4}\\) 5/2 \\(s^{3}\\) 2 \\(s^{2}\\) 3 \\(s^{1}\\) -76/15 \\(s^{0}\\) 4 There are two sign changes, \\(s^{2}\\) to \\(s^{1}\\) , and \\(s^{1}\\) to \\(s^{0}\\) . Thus, There are two roots in RHP","title":"Routh-Hurwitz stability criterion"},{"location":"Control%20Engineering/mid2/","text":"Sensitivity \u00b6 sensitivity of transfer function \\(T\\) w.r.t. its parameter \\(K\\) : \\(S_K^{T}\\) \\[ \\begin{gather} S_K^{T}=\\frac{\\frac{dT}{T}}{\\frac{dK}{K}}=\\frac{d\\ln T}{d\\ln K}=\\frac{K}{T}\\frac{dT}{dK} \\end{gather} \\] System Type for Tracking \u00b6 Reference Tracking \u00b6 Tracking definition the output \\(Y(s)\\) follow any reference input \\(R(s)\\) as closely as possible. \\[ \\begin{gather} E = R - Y \\to 0 \\\\\\\\ E(s) = R\\left(1-\\frac{DG}{1+DG}\\right)=\\frac{1}{1+DG}R \\end{gather} \\] System Classification for type \\(k\\) , we have the input \\(R\\) as \\[ \\begin{gather} r(t)=\\frac{1}{k!}t^{k} 1(t) \\iff R(s)=\\frac{1}{s^{k+1}} \\end{gather} \\] such that \\[ \\begin{gather} e(\\infty) = \\text{const} \\neq 0 \\end{gather} \\] type \\(0\\) \\[ \\begin{align} e(\\infty)&= \\bigg[s E(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}R(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}\\frac{1}{s}\\bigg]_{s\\to0} \\\\\\\\ &=\\frac{1}{1+D(0)G(0)} = \\frac{1}{1+K_p} \\end{align} \\] \\(K_p\\) : position error constant type \\(1\\) \\[ \\begin{align} e(\\infty)&= \\bigg[s E(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}R(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}\\frac{1}{s^{2}}\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[\\frac{1}{s+sDG}\\bigg]_{s\\to0}= \\frac{1}{K_v} \\end{align} \\] \\[ \\begin{gather} \\\\\\\\ \\implies D(s)G(s) = \\frac{1}{s}J(s) \\\\\\\\ \\implies K_v = J(0) \\end{gather} \\] \\(K_v\\) : velocity constant type \\(2\\) \\[ \\begin{align} e(\\infty)&= \\bigg[s E(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}R(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}\\frac{1}{s^{3}}\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[\\frac{1}{s^{2}+s^{2}DG}\\bigg]_{s\\to0}= \\frac{1}{K_a} \\end{align} \\] \\[ \\begin{gather} \\\\\\\\ \\implies D(s)G(s) = \\frac{1}{s^{2}}J(s) \\\\\\\\ \\implies K_a = J(0) \\end{gather} \\] - \\(K_a\\) : acceleration constant \u00b6 Disturbance Rejection \u00b6 We hope to reject the disturbance \\(W(s)\\) . Thus, to determine the system type w.r.t. \\(W\\) , first ignoring the effect of \\(R\\) by giving \\(R(s)=0\\) . \\[ \\begin{gather} E(s)=R-Y=-Y \\\\\\\\ Y=\\frac{G}{1+GD}W \\end{gather} \\] then \\[ \\begin{gather} E(s)=\\frac{-G}{1+GD}W \\end{gather} \\] PID Controller \u00b6 P (Proportional) \\[ \\begin{gather} D_{cl}(s)=\\frac{U(s)}{E(s)}=k_p \\end{gather} \\] I (Integral) \u00b6 \\[ \\begin{gather} D_{cl}(s)=\\frac{U(s)}{E(s)}=\\frac{k_I}{s} \\end{gather} \\] fix steady-state error (with \\(K_I \\neq 0\\) , \\(e_{ss}= 0\\) ) D (Derivative) \u00b6 \\[ \\begin{gather} D_{cl}(s)=\\frac{U(s)}{E(s)}=sk_D \\end{gather} \\] make the damping coefficient \\(\\zeta\\uparrow\\) , then the stability \\(\\uparrow\\) , overshoot \\(\\downarrow\\) has no effect on stead-state error PI (P + I) \\[ \\begin{gather} U(s)=E(s)k_P + \\frac{k_I}{s} \\iff u=k_Pe+k_I\\int{e(\\tau)\\,d\\tau} \\\\\\\\ D_{cl}(s)=\\frac{U(s)}{E(s)}=sk_D \\end{gather} \\] Root Locus \u00b6 for the characteristic equation, all of these equations are equivalent. \\[ \\begin{gather} 1+KL(s)=0 \\\\\\\\ 1+K\\frac{(s-z_1)(s-z_2)(s-z_3)\\dots \\cdot}{(s-p_1)(s-p_2)(s-p_3)\\dots \\cdot}=0 \\\\\\\\ \\bigg((s-p_1)(s-p_2)\\dots \\cdot\\bigg)+K\\bigg((s-z_1)(s-z_2)\\dots \\cdot\\bigg)=0 \\\\\\\\ \\frac{1}{K}\\bigg((s-p_1)(s-p_2)\\dots \\cdot\\bigg)+\\bigg((s-z_1)(s-z_2)\\dots \\cdot\\bigg)=0 \\end{gather} \\quad \\begin{aligned} \\text{(1)} \\\\\\\\\\\\ \\text{(2)} \\\\\\\\\\\\ \\text{(3)} \\\\\\\\\\\\ \\text{(4)} \\\\\\\\ \\end{aligned} \\] Rule 1: Find zeros and poles. \\(K=0\\) , i.e. start of the root locus is located at the poles (observing (3)) \\(K=\\infty\\) , i.e. end of the root locus is located at the zeros (observing (4)) Rule 2: Find the real axis positions of the locus. Let \\(s=s_0\\) , for \\(s_0 \\in \u211d\\) \\[ \\begin{gather} \\angle L(s_0)=(\\psi_{z1}+\\psi_{z2}+\\dots+)-(\\phi_{p1}+\\phi_{p2}+\\dots +)=\\pi,\\, -\\pi, \\, 3\\pi,\\, -3\\pi, \\dots \\end{gather} \\] if none of \\(s_0\\) satisfy the equation above then the root locus is not on the real axis. Otherwise, we can solve the range of \\(s_0\\) Rule 3: Find the asymptotes for large \\(K\\) When \\(K\\to \\infty\\) , there is not only the result show in Rule 1 ( \\(s\\) is finite) Consider (2), for \\(K, s\\) is large we can modify (2) as \\[ \\begin{gather} \\left[1+K\\frac{(s-z_1)(s-z_2)(s-z_3)\\dots \\cdot}{(s-p_1)(s-p_2)(s-p_3)\\dots \\cdot}\\right]_{s\\to\\infty}=0 \\\\\\\\ \\implies 1+K\\frac{s^{(\\text{\\# of zeros})}}{s^{(\\text{\\# of poles})}}=0 \\\\\\\\ \\implies 1+Ks^{m-n}=0 \\\\\\\\ \\implies s^{n-m}+K=0 \\end{gather} \\] for \\(n-m = 0\\) , \\[ \\begin{gather} K\\neq 0 \\end{gather} \\] In this case there is no asymptote. As Rule 1 says, all root go from poles to finite zeros. - for \\(n-m =1\\) \\[ \\begin{gather} s=-K=-\\infty \\end{gather} \\] this indicates that there is an imaginary zero at \\(-\\infty\\) . for \\(n-m > 1\\) \\[ \\begin{gather} s^{n-m}=-K = K\\angle \\pi \\\\\\\\ \\implies s= k \\angle(\\frac{\\pi+\u2113\\cdot2\\pi}{n-m}) \\\\\\\\ k=K^{1/(n-m)}=\\infty \\end{gather} \\] e.g. \\(n-m=3\\) \\[ s= \\left\\{ \\begin{gather} &k\\angle \\frac{\\pi}{3} \\\\\\\\ k\\angle \\frac{3\\pi}{3} = &k\\angle\\pi \\\\\\\\ &k\\angle \\frac{-\\pi}{3} \\end{gather}\\right. \\] note that this result only tell us there are three imaginary zeros at different directions of \\(\\infty\\) . However, the source (intersection) is no need to be at the origin ( \\(s=0\\) ). This is because, for a finite \\(s_0 <0\\) . \\[ s_0 = m\\angle\\pi \\implies \\left\\{ \\begin{gather} (k\\angle \\frac{\\pi}{3}-s_0)=k\\angle \\frac{\\pi}{3} \\\\\\\\ (k\\angle \\pi-s_0)=k\\angle \\pi \\\\\\\\ (k\\angle \\frac{-\\pi}{3}-s_0)=k\\angle \\frac{-\\pi}{3} \\end{gather}\\right. \\] Thus we can write the asymptotes as \\[ \\begin{gather} \\bigg((s-p_1)(s-p_2)\\dots \\cdot\\bigg)+K\\bigg((s-z_1)(s-z_2)\\dots \\cdot\\bigg)=0 \\\\\\\\ \\iff \\bigg(s^{n}+as^{n-1}+\\dots+\\bigg)+K\\bigg(s^{m}+bs^{m-1}+\\dots+\\bigg)=0 \\\\\\\\ \\implies \\sum{p_i}=-a \\end{gather} \\] \\[ \\begin{gather} \\bigg(s^{n}+as^{n-1}+\\dots+\\bigg)+K\\bigg(s^{m}+bs^{m-1}+\\dots+\\bigg)=0 \\\\\\\\ \\iff s^{n}+as^{n-1}+\\dots+K\\bigg(s^{m}+bs^{m-1}+\\dots+\\bigg)=0 \\\\\\\\ \\iff (s-r_1)(s-r_2)\\dots\\cdot(s-r_n)=0 \\\\\\\\ \\implies \\sum{r_i}=-a \\\\\\\\ \\implies \\sum{r_i}=-a=\\sum{p_i} \\end{gather} \\] now rewrite the characteristic equation \\[ \\begin{gather} \\bigg[1+K\\frac{(s-z_1)(s-z_2)(s-z_3)\\dots \\cdot}{(s-p_1)(s-p_2)(s-p_3)\\dots \\cdot}\\bigg]_{s\\to \\infty} \\approx 1+K\\frac{1}{(s-\\alpha)^{n-m}}=0 \\\\\\\\ \\implies (s-\\alpha)^{n-m}+K=0 \\\\\\\\ \\iff (s-r_1)(s-r_2)\\dots\\cdot(s-r_{n-m})=0 \\\\\\\\ \\implies \\sum_{i=1}^{n-m}{r_i}=-(-(n-m)\\alpha)=(n-m)\\alpha \\end{gather} \\] notice that all of the roots go from poles to zeros, therefore for \\(n> m\\) case, there are some poles go to infinite imaginary zeros. Thus we have \\[ \\begin{gather} \\sum(\\text{all roots})=\\sum(\\text{roots go to finite zeros})+\\sum{(\\text{roots go to infinte img. zeros})} \\end{gather} \\] \\[ \\begin{align} \\\\ \\implies \\sum_{i=1}^{n}{r_i}&=\\sum_{i=1}^{m}{z_i}+\\sum_{i=m+1}^{n}{z_{img, i}} \\\\\\\\ &= \\sum{z_{i}}+(n-m)\\alpha \\end{align} \\] \\[ \\begin{gather} \\\\ \\implies \\alpha = \\frac{\\sum{r_i}-\\sum{z_i}}{n-m}= \\frac{\\sum{p_i}-\\sum{z_i}}{n-m} \\end{gather} \\] Rule 4: departure and arrival angle for departure angle take \\(s_0 \\to p_i\\) and solve \\(\\angle L(s_0)=n\\pi\\) for departure angle take \\(s_0 \\to z_i\\) and solve \\(\\angle L(s_0)=n\\pi\\) Rule 5: points on Image( \\(j\\omega\\) ) axis similar to rule 2, just let \\(s_0 = j\\omega_0\\) and try to solve the characteristic equation. Rule 6: find breakaway points (location of multiple roots) consider there is a multiple roots \\(r_1\\) then we can write \\[ \\begin{gather} 1+KL(s)=0 \\\\\\\\ \\implies a(s) + Kb(s) \\equiv (s-r_1)^{p}(s-r_2)\\cdot \\dots \\cdot = 0 \\\\\\\\ \\frac{d}{ds}\\bigg[a(s)+Kb(s)\\bigg]_{s=r_1}=p(s-r_1)^{p-1}(s-r_2)\\cdot \\dots \\cdot = 0 \\end{gather} \\] thus, by solving the equation \\[ \\begin{gather} \\frac{d}{ds}\\bigg[a(s)+Kb(s)\\bigg]=0 \\end{gather} \\] we can probably find the breakaway point \\(r_1\\) .","title":"Mid2"},{"location":"Control%20Engineering/mid2/#sensitivity","text":"sensitivity of transfer function \\(T\\) w.r.t. its parameter \\(K\\) : \\(S_K^{T}\\) \\[ \\begin{gather} S_K^{T}=\\frac{\\frac{dT}{T}}{\\frac{dK}{K}}=\\frac{d\\ln T}{d\\ln K}=\\frac{K}{T}\\frac{dT}{dK} \\end{gather} \\]","title":"Sensitivity"},{"location":"Control%20Engineering/mid2/#system+type+for+tracking","text":"","title":"System Type for Tracking"},{"location":"Control%20Engineering/mid2/#reference+tracking","text":"Tracking definition the output \\(Y(s)\\) follow any reference input \\(R(s)\\) as closely as possible. \\[ \\begin{gather} E = R - Y \\to 0 \\\\\\\\ E(s) = R\\left(1-\\frac{DG}{1+DG}\\right)=\\frac{1}{1+DG}R \\end{gather} \\] System Classification for type \\(k\\) , we have the input \\(R\\) as \\[ \\begin{gather} r(t)=\\frac{1}{k!}t^{k} 1(t) \\iff R(s)=\\frac{1}{s^{k+1}} \\end{gather} \\] such that \\[ \\begin{gather} e(\\infty) = \\text{const} \\neq 0 \\end{gather} \\] type \\(0\\) \\[ \\begin{align} e(\\infty)&= \\bigg[s E(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}R(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}\\frac{1}{s}\\bigg]_{s\\to0} \\\\\\\\ &=\\frac{1}{1+D(0)G(0)} = \\frac{1}{1+K_p} \\end{align} \\] \\(K_p\\) : position error constant type \\(1\\) \\[ \\begin{align} e(\\infty)&= \\bigg[s E(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}R(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}\\frac{1}{s^{2}}\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[\\frac{1}{s+sDG}\\bigg]_{s\\to0}= \\frac{1}{K_v} \\end{align} \\] \\[ \\begin{gather} \\\\\\\\ \\implies D(s)G(s) = \\frac{1}{s}J(s) \\\\\\\\ \\implies K_v = J(0) \\end{gather} \\] \\(K_v\\) : velocity constant type \\(2\\) \\[ \\begin{align} e(\\infty)&= \\bigg[s E(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}R(s)\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[s\\frac{1}{1+DG}\\frac{1}{s^{3}}\\bigg]_{s\\to0} \\\\\\\\ &=\\bigg[\\frac{1}{s^{2}+s^{2}DG}\\bigg]_{s\\to0}= \\frac{1}{K_a} \\end{align} \\] \\[ \\begin{gather} \\\\\\\\ \\implies D(s)G(s) = \\frac{1}{s^{2}}J(s) \\\\\\\\ \\implies K_a = J(0) \\end{gather} \\]","title":"Reference Tracking"},{"location":"Control%20Engineering/mid2/#-+k_a++acceleration+constant","text":"","title":"- \\(K_a\\) : acceleration constant"},{"location":"Control%20Engineering/mid2/#disturbance+rejection","text":"We hope to reject the disturbance \\(W(s)\\) . Thus, to determine the system type w.r.t. \\(W\\) , first ignoring the effect of \\(R\\) by giving \\(R(s)=0\\) . \\[ \\begin{gather} E(s)=R-Y=-Y \\\\\\\\ Y=\\frac{G}{1+GD}W \\end{gather} \\] then \\[ \\begin{gather} E(s)=\\frac{-G}{1+GD}W \\end{gather} \\]","title":"Disturbance Rejection"},{"location":"Control%20Engineering/mid2/#pid+controller","text":"P (Proportional) \\[ \\begin{gather} D_{cl}(s)=\\frac{U(s)}{E(s)}=k_p \\end{gather} \\]","title":"PID Controller"},{"location":"Control%20Engineering/mid2/#i+integral","text":"\\[ \\begin{gather} D_{cl}(s)=\\frac{U(s)}{E(s)}=\\frac{k_I}{s} \\end{gather} \\] fix steady-state error (with \\(K_I \\neq 0\\) , \\(e_{ss}= 0\\) )","title":"I (Integral)"},{"location":"Control%20Engineering/mid2/#d+derivative","text":"\\[ \\begin{gather} D_{cl}(s)=\\frac{U(s)}{E(s)}=sk_D \\end{gather} \\] make the damping coefficient \\(\\zeta\\uparrow\\) , then the stability \\(\\uparrow\\) , overshoot \\(\\downarrow\\) has no effect on stead-state error PI (P + I) \\[ \\begin{gather} U(s)=E(s)k_P + \\frac{k_I}{s} \\iff u=k_Pe+k_I\\int{e(\\tau)\\,d\\tau} \\\\\\\\ D_{cl}(s)=\\frac{U(s)}{E(s)}=sk_D \\end{gather} \\]","title":"D (Derivative)"},{"location":"Control%20Engineering/mid2/#root+locus","text":"for the characteristic equation, all of these equations are equivalent. \\[ \\begin{gather} 1+KL(s)=0 \\\\\\\\ 1+K\\frac{(s-z_1)(s-z_2)(s-z_3)\\dots \\cdot}{(s-p_1)(s-p_2)(s-p_3)\\dots \\cdot}=0 \\\\\\\\ \\bigg((s-p_1)(s-p_2)\\dots \\cdot\\bigg)+K\\bigg((s-z_1)(s-z_2)\\dots \\cdot\\bigg)=0 \\\\\\\\ \\frac{1}{K}\\bigg((s-p_1)(s-p_2)\\dots \\cdot\\bigg)+\\bigg((s-z_1)(s-z_2)\\dots \\cdot\\bigg)=0 \\end{gather} \\quad \\begin{aligned} \\text{(1)} \\\\\\\\\\\\ \\text{(2)} \\\\\\\\\\\\ \\text{(3)} \\\\\\\\\\\\ \\text{(4)} \\\\\\\\ \\end{aligned} \\] Rule 1: Find zeros and poles. \\(K=0\\) , i.e. start of the root locus is located at the poles (observing (3)) \\(K=\\infty\\) , i.e. end of the root locus is located at the zeros (observing (4)) Rule 2: Find the real axis positions of the locus. Let \\(s=s_0\\) , for \\(s_0 \\in \u211d\\) \\[ \\begin{gather} \\angle L(s_0)=(\\psi_{z1}+\\psi_{z2}+\\dots+)-(\\phi_{p1}+\\phi_{p2}+\\dots +)=\\pi,\\, -\\pi, \\, 3\\pi,\\, -3\\pi, \\dots \\end{gather} \\] if none of \\(s_0\\) satisfy the equation above then the root locus is not on the real axis. Otherwise, we can solve the range of \\(s_0\\) Rule 3: Find the asymptotes for large \\(K\\) When \\(K\\to \\infty\\) , there is not only the result show in Rule 1 ( \\(s\\) is finite) Consider (2), for \\(K, s\\) is large we can modify (2) as \\[ \\begin{gather} \\left[1+K\\frac{(s-z_1)(s-z_2)(s-z_3)\\dots \\cdot}{(s-p_1)(s-p_2)(s-p_3)\\dots \\cdot}\\right]_{s\\to\\infty}=0 \\\\\\\\ \\implies 1+K\\frac{s^{(\\text{\\# of zeros})}}{s^{(\\text{\\# of poles})}}=0 \\\\\\\\ \\implies 1+Ks^{m-n}=0 \\\\\\\\ \\implies s^{n-m}+K=0 \\end{gather} \\] for \\(n-m = 0\\) , \\[ \\begin{gather} K\\neq 0 \\end{gather} \\] In this case there is no asymptote. As Rule 1 says, all root go from poles to finite zeros. - for \\(n-m =1\\) \\[ \\begin{gather} s=-K=-\\infty \\end{gather} \\] this indicates that there is an imaginary zero at \\(-\\infty\\) . for \\(n-m > 1\\) \\[ \\begin{gather} s^{n-m}=-K = K\\angle \\pi \\\\\\\\ \\implies s= k \\angle(\\frac{\\pi+\u2113\\cdot2\\pi}{n-m}) \\\\\\\\ k=K^{1/(n-m)}=\\infty \\end{gather} \\] e.g. \\(n-m=3\\) \\[ s= \\left\\{ \\begin{gather} &k\\angle \\frac{\\pi}{3} \\\\\\\\ k\\angle \\frac{3\\pi}{3} = &k\\angle\\pi \\\\\\\\ &k\\angle \\frac{-\\pi}{3} \\end{gather}\\right. \\] note that this result only tell us there are three imaginary zeros at different directions of \\(\\infty\\) . However, the source (intersection) is no need to be at the origin ( \\(s=0\\) ). This is because, for a finite \\(s_0 <0\\) . \\[ s_0 = m\\angle\\pi \\implies \\left\\{ \\begin{gather} (k\\angle \\frac{\\pi}{3}-s_0)=k\\angle \\frac{\\pi}{3} \\\\\\\\ (k\\angle \\pi-s_0)=k\\angle \\pi \\\\\\\\ (k\\angle \\frac{-\\pi}{3}-s_0)=k\\angle \\frac{-\\pi}{3} \\end{gather}\\right. \\] Thus we can write the asymptotes as \\[ \\begin{gather} \\bigg((s-p_1)(s-p_2)\\dots \\cdot\\bigg)+K\\bigg((s-z_1)(s-z_2)\\dots \\cdot\\bigg)=0 \\\\\\\\ \\iff \\bigg(s^{n}+as^{n-1}+\\dots+\\bigg)+K\\bigg(s^{m}+bs^{m-1}+\\dots+\\bigg)=0 \\\\\\\\ \\implies \\sum{p_i}=-a \\end{gather} \\] \\[ \\begin{gather} \\bigg(s^{n}+as^{n-1}+\\dots+\\bigg)+K\\bigg(s^{m}+bs^{m-1}+\\dots+\\bigg)=0 \\\\\\\\ \\iff s^{n}+as^{n-1}+\\dots+K\\bigg(s^{m}+bs^{m-1}+\\dots+\\bigg)=0 \\\\\\\\ \\iff (s-r_1)(s-r_2)\\dots\\cdot(s-r_n)=0 \\\\\\\\ \\implies \\sum{r_i}=-a \\\\\\\\ \\implies \\sum{r_i}=-a=\\sum{p_i} \\end{gather} \\] now rewrite the characteristic equation \\[ \\begin{gather} \\bigg[1+K\\frac{(s-z_1)(s-z_2)(s-z_3)\\dots \\cdot}{(s-p_1)(s-p_2)(s-p_3)\\dots \\cdot}\\bigg]_{s\\to \\infty} \\approx 1+K\\frac{1}{(s-\\alpha)^{n-m}}=0 \\\\\\\\ \\implies (s-\\alpha)^{n-m}+K=0 \\\\\\\\ \\iff (s-r_1)(s-r_2)\\dots\\cdot(s-r_{n-m})=0 \\\\\\\\ \\implies \\sum_{i=1}^{n-m}{r_i}=-(-(n-m)\\alpha)=(n-m)\\alpha \\end{gather} \\] notice that all of the roots go from poles to zeros, therefore for \\(n> m\\) case, there are some poles go to infinite imaginary zeros. Thus we have \\[ \\begin{gather} \\sum(\\text{all roots})=\\sum(\\text{roots go to finite zeros})+\\sum{(\\text{roots go to infinte img. zeros})} \\end{gather} \\] \\[ \\begin{align} \\\\ \\implies \\sum_{i=1}^{n}{r_i}&=\\sum_{i=1}^{m}{z_i}+\\sum_{i=m+1}^{n}{z_{img, i}} \\\\\\\\ &= \\sum{z_{i}}+(n-m)\\alpha \\end{align} \\] \\[ \\begin{gather} \\\\ \\implies \\alpha = \\frac{\\sum{r_i}-\\sum{z_i}}{n-m}= \\frac{\\sum{p_i}-\\sum{z_i}}{n-m} \\end{gather} \\] Rule 4: departure and arrival angle for departure angle take \\(s_0 \\to p_i\\) and solve \\(\\angle L(s_0)=n\\pi\\) for departure angle take \\(s_0 \\to z_i\\) and solve \\(\\angle L(s_0)=n\\pi\\) Rule 5: points on Image( \\(j\\omega\\) ) axis similar to rule 2, just let \\(s_0 = j\\omega_0\\) and try to solve the characteristic equation. Rule 6: find breakaway points (location of multiple roots) consider there is a multiple roots \\(r_1\\) then we can write \\[ \\begin{gather} 1+KL(s)=0 \\\\\\\\ \\implies a(s) + Kb(s) \\equiv (s-r_1)^{p}(s-r_2)\\cdot \\dots \\cdot = 0 \\\\\\\\ \\frac{d}{ds}\\bigg[a(s)+Kb(s)\\bigg]_{s=r_1}=p(s-r_1)^{p-1}(s-r_2)\\cdot \\dots \\cdot = 0 \\end{gather} \\] thus, by solving the equation \\[ \\begin{gather} \\frac{d}{ds}\\bigg[a(s)+Kb(s)\\bigg]=0 \\end{gather} \\] we can probably find the breakaway point \\(r_1\\) .","title":"Root Locus"},{"location":"Control%20Engineering/examples/masion_rule_examples/","text":"Example 3.2 \u00b6 \\[ \\begin{gather} \\Delta = 1 - (H_1H_5 + H_2H_6 + H_3H_7 + H_4H_7H_6H_5) + H_1H_5H_3H_7 \\end{gather} \\] forward path: \\[ \\begin{gather} G_1 = H_1H_2H_3 \\\\\\\\ \\Delta_1 = 1 - 0 \\\\\\\\\\\\ G_2 = H_4 \\\\\\\\ \\Delta_2 = 1-(H_2H_6) \\end{gather} \\]","title":"Masion rule examples"},{"location":"Control%20Engineering/examples/masion_rule_examples/#example+32","text":"\\[ \\begin{gather} \\Delta = 1 - (H_1H_5 + H_2H_6 + H_3H_7 + H_4H_7H_6H_5) + H_1H_5H_3H_7 \\end{gather} \\] forward path: \\[ \\begin{gather} G_1 = H_1H_2H_3 \\\\\\\\ \\Delta_1 = 1 - 0 \\\\\\\\\\\\ G_2 = H_4 \\\\\\\\ \\Delta_2 = 1-(H_2H_6) \\end{gather} \\]","title":"Example 3.2"},{"location":"DBMS/mid1/","text":"Basic Definition \u00b6 Database \u2014 A collection of related data Data \u2014 Known facts that can be recorded and have implicit meaning. Mini-world \u2014 some part of the real world about which data is stored in a database. For example, Part of a UNIVERSITY environment. student course section department and so on Database Management System Database System \u2014 DBMS + DATA Data Models \u00b6 Categories of data models Conceptual (high level) \u2014 aka entity-base or object-base data models. e.g. OO, ER, ... Physical (low level) \u2014 Provide concepts that describe details of how data is stored in the device. Implementation (record-oriented): Provide concept between conceptual and physical. e.g. Relation, ... Three-Schema Architecture \u00b6 **Internal graph TD internal((Internal Schema)) conceptual(Conceptual schema) external(External schema) logical{Logical Data<br/>Independence} physical{Physical Data<br>Independence} external <--> logical <--> conceptual <--> physical <--> internal DBMS Languages \u00b6 Data Definition Language (DDL) storage definition language (SDL) view definition language (VDL) Data Manipulation Language (DML) e.g. SQL Entity-Relationship Data Model \u00b6 Entity-Relationship aka ER Concepts \u00b6 Entities Attributes simple composite multi-valued Relational Algebra \u00b6 SELECT \\(\\sigma\\) PROJECTION \\(\\pi\\) RENAME \\(\\rho\\) DIVISION Set Operations \u00b6 THETA JOIN \\(\\bowtie_\\theta\\) \\(\\theta\\) is comparison operators, e.g. \\(=, >, \\ge,\\dots\\) AGGREGATE \\(\u2111\\) \u2014 SUM, COUNT, AVERAGE, MIN, MAX, ... format: \\<grouping attributes> \\(\u2111\\) \\<function list> (R) eg1. R(AVGSAL) \\(\\leftarrow\\) \\(\u2111\\) AVERAGE SALARY (EMPLOYEE) eg2. R(DNO, NUMIPMS, AVGSAL) \\(\\leftarrow\\) DNO \\(\u2111\\) AVERAGE SALARY (EMPLOYEE) in which DNO is the grouping attrs.","title":"Mid1"},{"location":"DBMS/mid1/#basic+definition","text":"Database \u2014 A collection of related data Data \u2014 Known facts that can be recorded and have implicit meaning. Mini-world \u2014 some part of the real world about which data is stored in a database. For example, Part of a UNIVERSITY environment. student course section department and so on Database Management System Database System \u2014 DBMS + DATA","title":"Basic Definition"},{"location":"DBMS/mid1/#data+models","text":"Categories of data models Conceptual (high level) \u2014 aka entity-base or object-base data models. e.g. OO, ER, ... Physical (low level) \u2014 Provide concepts that describe details of how data is stored in the device. Implementation (record-oriented): Provide concept between conceptual and physical. e.g. Relation, ...","title":"Data Models"},{"location":"DBMS/mid1/#three-schema+architecture","text":"**Internal graph TD internal((Internal Schema)) conceptual(Conceptual schema) external(External schema) logical{Logical Data<br/>Independence} physical{Physical Data<br>Independence} external <--> logical <--> conceptual <--> physical <--> internal","title":"Three-Schema Architecture"},{"location":"DBMS/mid1/#dbms+languages","text":"Data Definition Language (DDL) storage definition language (SDL) view definition language (VDL) Data Manipulation Language (DML) e.g. SQL","title":"DBMS Languages"},{"location":"DBMS/mid1/#entity-relationship+data+model","text":"Entity-Relationship aka ER","title":"Entity-Relationship Data Model"},{"location":"DBMS/mid1/#concepts","text":"Entities Attributes simple composite multi-valued","title":"Concepts"},{"location":"DBMS/mid1/#relational+algebra","text":"SELECT \\(\\sigma\\) PROJECTION \\(\\pi\\) RENAME \\(\\rho\\) DIVISION","title":"Relational Algebra"},{"location":"DBMS/mid1/#set+operations","text":"THETA JOIN \\(\\bowtie_\\theta\\) \\(\\theta\\) is comparison operators, e.g. \\(=, >, \\ge,\\dots\\) AGGREGATE \\(\u2111\\) \u2014 SUM, COUNT, AVERAGE, MIN, MAX, ... format: \\<grouping attributes> \\(\u2111\\) \\<function list> (R) eg1. R(AVGSAL) \\(\\leftarrow\\) \\(\u2111\\) AVERAGE SALARY (EMPLOYEE) eg2. R(DNO, NUMIPMS, AVGSAL) \\(\\leftarrow\\) DNO \\(\u2111\\) AVERAGE SALARY (EMPLOYEE) in which DNO is the grouping attrs.","title":"Set Operations"},{"location":"DBMS/mid2/","text":"Hashing \u00b6 identifier density \\(\\alpha = \\displaystyle \\frac{n}{T}\\) number of identifiers \\(n\\) number of all distinct possible values \\(T\\) e.g. \\(n\\) is # of all people having id in country, \\(T=26 \\times 2 \\times 10^{8}\\) for id in the format like A123456789.","title":"Mid2"},{"location":"DBMS/mid2/#hashing","text":"identifier density \\(\\alpha = \\displaystyle \\frac{n}{T}\\) number of identifiers \\(n\\) number of all distinct possible values \\(T\\) e.g. \\(n\\) is # of all people having id in country, \\(T=26 \\times 2 \\times 10^{8}\\) for id in the format like A123456789.","title":"Hashing"},{"location":"Data%20Compression/DataCompression/","text":"Source Coding \u00b6 Shannon's Source Coding, \u4fe1\u6e90\u7de8\u78bc\u3001\u7b26\u865f\u6e90\u7de8\u78bc information mapping (bits, characters, ....) Entropy \u00b6 Self-Information \u00b6 by three assumptions \\(I(p) \\ge 0\\) \\(I(p_1 \\cdot p_2)=I(p_1) + I(p_2)\\) \\(I(p)\\) is continuous to \\(p\\) thus, gives \\(I(p) = -\\log(p)\\) proof at p.25 Entropy \u00b6 Given, \\[ \\begin{gather} S=\\{s_i \\,|\\, p(s_i) = p_i\\} \\end{gather} \\] then, \\[ \\begin{gather} H_r(S) = \\sum\\limits p_i I(s_i)=\\sum\\limits -p_i\\log_r(p_i) \\end{gather} \\] note that \\(r\\) is the base of log Gibbs' Inequality \u00b6 \\[ \\begin{gather} \\sum\\limits - p_i\\log(p_i) \\le \\sum\\limits-p_i\\log(q_i) \\end{gather} \\] the lower bound of cross entropy of \\(q_i\\) and \\(p_i\\) are \\(H(p_i)\\) . Unique Decodable & Instantaneous Code \u00b6 p32 Unique Decodable \u00b6 not unique decodable example \\[ \\begin{gather} (s_1, s_2, s_3, s_4) = (0, 01, 11, 00) \\end{gather} \\] then for the message \\(0011\\) could be decoded as \\[ 0011 = \\left\\{ \\begin{align} &s_4s_3 \\\\ &s_1s_1s_3 \\end{align}\\right. \\] Instantaneous Code \u00b6 instantaneous iff \u6c92\u6709\u4e00\u500b\u7b26\u865f\u662f\u53e6\u4e00\u500b\u7b26\u865f\u7684\u5b57\u9996 instantaneous \u21d2 unique the lengths of codes have to follow Kraft Inequality Kraft Inequality \u00b6 If instantaneous, then the code have to end at leaves. thus, considering binary case only first we cannot build a binary tree for many short path(short coding length). Therefore, in order to build an binary tree, we must follow \\[ \\begin{gather} \\sum\\limits \\frac{1}{2^{L_i}} \\le 1 \\end{gather} \\] intuitive thinking \u2013 Considering taking a branch of binary tree have prob. 0.5 and 0.5, then the summation of prob. of all leaves is \\(1\\) . intuitively, the lowest bound of average lengths of codes is its entropy. (proof at p36.) \\[ \\begin{gather} H_r(S) \\le L_{avg} \\end{gather} \\] Shannon-Fano Coding \u00b6 Since we know the lower bound of coding length is its entropy, then we can have the coding length (known as Shannon-Fano Length ) as \\[ \\begin{gather} -\\log(p_i)\\le \u2113_i < -\\log(p_i) + 1 \\\\\\\\ \\iff \u2113_i = \\bigg\\lceil -\\log(p_i) \\bigg\\rceil \\end{gather} \\] drawback example Given \\(S=\\{s_1, s_2\\}\\) , and \\(k\\) is large \\[ \\begin{gather} p_1 = 2^{-k}, \\qquad p_2=1-p_1 \\\\\\\\ \\implies l_1 = k, \\qquad l_2=1 \\end{gather} \\] in Huffman coding, both \\(l_1, l_2 = 1\\) , but since \\(p_1\\) is small when \\(k\\) is large, this drawback isn't very critical. Extension Code \u00b6 quick concept \u2013 if we have an source code \\(S\\) , then we can define an new source code \\(T = S^{n}\\) (then the # of symbol in \\(T\\) would be \\(|S|^{n}\\) ) The \\(H(T) = nH(S)\\) , and denote \\(L_n\\) (average length of \\(n\\) -order S.F. code), then \\[ \\begin{gather} H(T) \\le L_n < H(T) + 1 \\\\\\\\ H(S) \\le \\frac{L_n}{n} < H(S) + \\frac{1}{n} \\end{gather} \\] thus when \\(n\\to \\infty\\) , then \\(\\frac{L_n}{n} \\to H(S)\\) aka Shannon's noiseless coding theorem. Adaptive Huffman \u00b6 start with one EOF and one ESC (always one in Huffman tree). EOF \u2013 send when end of file. ESC \u2013 send when new symbol is added to tree (send following with ascii of that symbol so that decoder can know what to decode). JBIG & JBIG2 \u00b6 code for binary data directly ( \\(S \\in \\{0, 1\\}\\) ) JBIG \u2013 high order adaptive arithmetic (for \\(P(s_t \\,|\\, s_{(t-n):(t-1)})\\) . since the target is binary, the high order coding table would be \\(2^{n}\\) , much less than high order Huffman. JBIG2 \u2013 Define decoding protocol only. That is the encoder side can be any algorithm, even lossy. LZ \u00b6 LZ77 \u00b6 sliding window and look-ahead buffer find phrase in window, and encode text in look-ahead buffer send (displacement, length, next_char) in no match case, send (0, 0, next_char) pros fast decode cons slow encode \\(O(n)\\) , ( \\(n\\) is windows size) \\(O(m)\\) , ( \\(m\\) is look-ahead buffer size) worst case when no match prefer larger window size but cost too much time efficiency \u2193 compressed phrases size \u2191 loss memory (after dictionary is full of phrases, some of them have to be removed) LZSS \u00b6 improving version of LZ77 send \\(1\\) bit indicating whether is no match instead of send (0, 0 next_char) Circular queue with WINDOW_SIZE = \\(2^{n}\\) Binary Search Tree for storing phrases (actually storing pointer to particular position of window). Node stores fixed-size length e.g. ( \"LZSS is better than LZ77\" ) with fixed length \\(5\\) , (search by comparing char-wise order) graph TD root(LZSS^) l(^is^b) ll(^bett) lr(is^be) r(ZSS^i) rl(SS^is) rr( ) rll(S^is^) rlr(tter^) root --- l root --- r l --- ll l --- lr r --- rl r --- rr rl --- rll rl --- rlr LZ78 \u00b6 create new phrase in dictionary at both encoder size and decoder size whenever no match initially, there is only null string in dictionary. thus whenever create a new phrase with length \\(n\\) , there must exists a phrase with length \\(n-1\\) . therefore, using multiway search tree to store phrases graph TD r('\\0' <br> 0) p1(\"'D' <br> 1\") p2(\"'A' <br> 2\") p3(\"'^' <br> 3\") p4(\"'A' <br> 4\") p5(\"'^' <br> 5\") p6(\"'D' <br> 6\") p7(\"'Y' <br> 7\") p8(\"'^' <br> 8\") p9(\"'O' <br> 9\") r --- p8 r --- p2 r --- p1 p1 --- p3 p1 --- p4 p1 --- p7 p4 --- p5 p4 --- p6 p6 --- p9 - in this case p1 = D , p3 = D^ , p5=DA^ , and so on... example at p119 thus encoder side only have to send phrase code (phrase id), without phrase's length. i.e. (phrase_id, next_char) cons slow decoding (have to maintain dictionary tree) LZW \u00b6 improving version LZ78 defined every characters as phrases first, then send only (phrase_id, ) examples: PNG, ... algorithm p122 encoder: i := 0 ; Dictionary phrases ; string in_buff ; in_buff . Add ( str [ i ]) while in_buff . Add ( str [++ i ]) if in_buff not in phrases phrases . Add ( in_buff ) OUTPUT << phrases . IndexOf ( in_buff . PopLast ()) in_buff := string ( in_buff [- 1 ]) decoder i := 0 Dictionary phrases ; int in_buff ; string last_phrase // init INPUT >> in_buff last_phrase := phrases [ in_buff ] OUTPUT << last_phrase while true INPUT >> in_buff OUTPUT << phrases [ in_buff ] phrases . Add ( last_phrase + phrases [ in_buff ][ 0 ]) last_phrase = phrase [ in_buff ] Lossy \u00b6 RMSE (Root Mean Square Error) SNR (Signal-to-Noise Ratio) \\[ \\begin{gather} \\text{SNR} = \\frac{E\\big[S^{2}\\big]}{E\\big[(X-\\mu)^{2}\\big]} = \\frac{E\\big[S^{2}\\big]}{\\sigma_r^{2}} \\\\\\\\ \\text{SNR}_{dB} = 10\\log_{10}{\\text{SNR}} \\end{gather} \\] in 2d media case \\[ \\begin{gather} \\text{SNR} = \\frac{Q^{2}}{\\sigma_r^{2}} \\end{gather} \\] in which \\(Q = 255\\) in 8-bits case. DM \u00b6 Delta Modulation Adaptive DM (ADM) adaptive for the magnitude of \\(\\Delta\\) DPCM \u00b6 Differential Pulse Code Modulation Predictor Optimization \u00b6 objective \\[ \\begin{align} \\hat x_{m}^* &= \\arg\\min_{\\hat x_m} \\,\\sigma_e^{2} \\\\\\\\ &= \\arg\\min_{\\hat x_m} E\\bigg[(x_m - {\\hat x_m})^{2}\\bigg] \\end{align} \\] in which, \\[ \\begin{gather} \\hat x_m = \\sum\\limits_{i \\in [0, m)}{\\alpha_i}x_i \\end{gather} \\] then solve by differentiation, we have \\[ \\begin{gather} E\\big[(x_m-\\hat x_m)x_i\\big] = 0 \\end{gather} \\] thus \\[ \\begin{gather} E[x_mx_i] = E[\\hat x_m x_i] \\\\\\\\ R_{mi} = E[\\hat x_m x_i] \\end{gather} \\] and further when \\(i=m\\) , \\[ \\begin{gather} E\\left[x_m^{2}\\right]=E\\left[\\hat x_m x_m\\right] \\\\\\\\ \\implies \\sigma_e^{2} = E\\bigg[(x_m -\\hat x_m)^{2}\\bigg]= \\\\\\\\ E\\big[(x_m-\\hat x_m)\\hat x_m\\big]=E\\big[x_m^{2}\\big] - E\\big[\\hat x_m^{2}\\big] \\\\\\\\ \\implies \\sigma^{2} < E\\big[x_m^{2}\\big] \\end{gather} \\] p141 Quantizer Optimization \u00b6 objective ( \\(N\\) is number of order of quantizer) \\[ \\begin{gather} D=\\sum\\limits_{i \\in [0, N)}{\\int_{d_i}^{d_{i+1}}p(e)\\,(e-r_i)^{2}de} \\end{gather} \\] p143 Adaptive DPCM (ADPCM) \u00b6 \u6620\u5c04\u91cf\u5316\u5668 \\(e=x_m - \\hat x_m\\) \\(x_m \\in [0, 2^{n})\\) , thus normally \\(e \\in(-2^{n}, 2^{n})\\) however with known \\(\\hat x_m\\) , then \\(e \\in [-\\hat x_m,\\, 2^{n} - \\hat x_m)\\) \u66ff\u63db\u91cf\u5316\u5668 use multi quantizers, and encode with the best quantizer, and send which of quantizers. Lossless DPCM \u00b6 without quantizers, send the \\(e\\) directly (or further using other lossless algorithm). thus can be used as preprocessor for others like Huffman , Arith . (since quite popular). more efficient then using adaptive like AHuff , AArith , but perform closely. Non-Redundant Sample Coding \u00b6 aka adaptive sampling coding Polynomial Predictor \u00b6 \\[ \\begin{gather} x_t = x_{t-1} + \\Delta x_{t-1}+\\Delta^{2}x_{t-1}+\\cdots \\end{gather} \\] in which \\[ \\begin{gather} \\Delta^{2}x_{t-1} = \\Delta x_{t-1} - \\Delta x_{t-2} \\end{gather} \\] Polynomial Interpolator \u00b6 1\u6b21\u5167\u63d2\u6cd5 = \u6247\u5f62\u6f14\u7b97\u6cd5 = SAPA2 AZTEC \u00b6 p173 rules use horizontal line if \\(\\ge 3\\) successive samples satisfy \\(x_{max} - x_{min} < \\lambda\\) otherwise, use slope. Further, if next sample have the same signed of slope and \\(|x_m - x_{m-1}| > \\lambda\\) , then keep redundant. CORNER \u00b6 CORCER > SAPA2 > AZTEC algorithm 2-order diff, \\(x''(i)=x(i+1)+x(i-1)-2x(i)\\) \\(\\forall i\\) , if \\(x(i) > \\lambda_1\\) and \\(x''(i)\\) is local maximum, then make \\(x(i)\\) then now have redundant samples \\(x(m_1), x(m_2), x(m_3), \\dots\\) for all redundant, find if \\(x\\left(\\frac{m_1+m_2}{2}\\right)- \\frac{x(m_1)+x(m_2)}{2} > \\lambda_2\\) (that is, if the \\(x\\) very concave (\u51f9)) if not, add \\(x(\\frac{m_1 + m_2}{2})\\) , as \\(x(m_{1\\_2})\\) , and do \\(\\big\\{x(m_1)\\) , \\(x(m_{1\\_2})\\big\\}\\) and \\(\\big\\{x(m_{1\\_2}), x(m_2)\\big\\}\\) as step 4. if step4. so, continue BTC \u00b6 -Block Truncation Coding Moment-Preserving Quantizer \u00b6 target \u2013 find quantizer that make 1st and 2nd moment unchanged, and since \\[ \\begin{gather} \\sigma^{2} = E\\big[X^{2}\\big] - E\\big[X\\big]^{2} \\end{gather} \\] the variance \\(\\sigma^{2}\\) would also unchanged. solution \u2013 Given \\(X_{th}\\) as the threshold (normally, mean of all pixels), and \\(q\\) is # of \\(b\\) cases \\[ \\hat x_i = \\begin{dcases*} a & if $x_i < X_{th}$ \\\\\\\\ b & otherwise \\end{dcases*} \\] \\[ \\begin{align} \\\\ E\\big[X\\big]&=\\frac{(m-q)\\cdot b+q\\cdot a}{m} \\\\\\\\ E\\big[X^{2}\\big]&=\\frac{(m-q)\\cdot b^{2} + q\\cdot a^{2}}{m} \\end{align} \\] then, \\[ \\begin{align} a &= E\\big[X\\big] - \\sigma \\sqrt{\\frac{q}{m-q}} \\\\\\\\ a &= E\\big[X\\big] + \\sigma \\sqrt{\\frac{m-q}{q}} \\end{align} \\] solution2 (Absolute Moment BTC, AMBTC) since the square root is hard to compute, thus we maintain the absolute moment instead of 2nd moment. \\[ \\begin{gather} \\alpha=E\\bigg[|X_i - \\mu|\\bigg] \\end{gather} \\] then, we can have, \\[ \\begin{align} a &= E[X] - \\frac{m\\alpha}{2(m-q)} \\\\\\\\ b &= E[X] + \\frac{m\\alpha}{2q} \\end{align} \\] Transform Coding \u00b6 Rotation Matrix \\[ M(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin \\theta \\\\ \\sin\\theta & \\cos \\theta \\end{bmatrix} \\] ### Zonal Sampling - \u5340\u57df\u53d6\u6a23 1. \u4fdd\u7559\u4f4e\u983b\uff0c\u9ad8\u983b\u901a\u5e38\u5c0f\uff0c\u4e0d\u7559 2. \u4f4e\u983b\u7528\u6bd4\u8f03\u591a bits 3. \u56fa\u5b9a bits per block \u4f46\u8b93 variance \u5927(\u5c0d\u5176\u4ed6 blocks \u7684\u540c\u4f4d\u7f6e)\u7684\u4fc2\u6578\u6709\u6bd4\u8f03\u591a bits - cons - \u53ef\u80fd\u6709\u5f88\u5927\u96e3\u4ee5\u5ffd\u7565\u7684\u4fc2\u6578 Threshold Sampling \u00b6 \u81e8\u754c\u53d6\u6a23 \u8a2d threshold \uff0c\u4ee5\u4e0b\u70ba 0\uff0c\u4ee5\u4e0a\u9001\u4f4d\u7f6e\u8207\u503c JPEG encode \u00b6 get \\(F^{*}(u, v)\\) scan with z order First coefficient(DC) encode with DPCM and Huffman. encode remains coefficients(AC) with the following law omit \\(0\\) lookup category \\(k\\) at p220 according to category \\(k\\) and # of \\(0\\) before this coefficient, lookup table in p224. append \\(k\\) bits which indicates the index of AC coefficient in that category. DCT \u00b6 p209 Vector Quantization \u00b6 Cost of encoding \\(O(nN_c)\\) , where \\(n\\) is dimension of vector, \\(N_c\\) is number of codebooks. need memory \\(n N_c\\) LBG \u00b6 LBG\u6f14\u7b97\u6cd5\u662f\u7531Linde,Buzo,Gray\u4e09\u4eba\u57281980\u5e74\u63d0\u51fa\u7684\u3002\u5176\u7b97\u6cd5\u8207K-means\u96f7\u540c\uff0c\u6839\u64da\u7576\u524d\u5283\u5206\u4e4b\u7fa4\u96c6\u8a08\u7b97\u8aa4\u5dee\u91cf\uff0c\u4e0d\u65b7\u8abf\u6574\u6620\u5c04\u5340\u9593(Mapping Region)\u53ca\u91cf\u5316\u5411\u91cf\u4e4b\u91cf\u5316\u9ede: \u7d66\u5b9a\u8a13\u7df4\u6a23\u672c\u4ee5\u53ca\u8aa4\u5dee\u95be\u503c \u8a02\u5b9a\u521d\u59cb\u78bc\u5411\u91cf \u5c07\u758a\u4ee3\u8a08\u6578\u5668\u6b78\u96f6 \u8a08\u7b97\u7e3d\u8aa4\u5dee\u503c\uff0c\u82e5\u4e0d\u70ba\u7b2c\u4e00\u6b21\uff0c\u5247\u6aa2\u67e5\u8207\u524d\u4e00\u6b21\u8aa4\u5dee\u503c\u5dee\u8ddd\u662f\u5426\u5c0f\u65bc\u95be\u503c\u3002 \u6839\u64da\u6bcf\u4e00\u500b\u8a13\u7df4\u6a23\u672c\u8207\u78bc\u5411\u91cf\u7684\u8ddd\u96e2d\uff0c\u627e\u5176\u6700\u5c0f\u503c\uff0c\u5b9a\u7fa9\u70ba\u6620\u5c04\u51fd\u6578Q \u66f4\u65b0\u78bc\u5411\u91cf\uff1a\u5c07\u5c0d\u61c9\u5230\u540c\u4e00\u500b\u78bc\u5411\u91cf\u7684\u5168\u6578\u8a13\u7df4\u6a23\u672c\u505a\u5e73\u5747\u4ee5\u66f4\u65b0\u78bc\u5411\u91cf\u3002 i\u70ba\u758a\u4ee3\u8a08\u6578\u5668\uff0cC\u70ba\u8a72\u7fa4\u96c6\u4e4b\u4ee3\u8868\uff0cx\u70ba\u8cc7\u6599\u9ede\uff0cQ(x)\u70bax\u91cf\u5316\u5f8c\u4e4b\u7fa4\u96c6\u4ee3\u8868C \u758a\u4ee3\u8a08\u6578\u5668\u52a0\u4e00 \u6703\u5230\u6b65\u9a5f\u56db\uff0c\u76f4\u81f3\u8aa4\u5dee\u503c\u5c0f\u65bc\u95a5\u503c LBG\u6f14\u7b97\u6cd5\u5341\u5206\u4f9d\u8cf4\u8d77\u59cb\u7de8\u78bc\u7c3f\uff0c\u7522\u751f\u8d77\u59cb\u7de8\u78bc\u7c3f\u7684\u65b9\u6cd5\u6709\u4ee5\u4e0b\u5e7e\u7a2e\uff1a Tree-Structured Codebooks \u00b6 \\(m\\) -ways tree Cost now decrease to \\(O(n \\cdot m\\log_m{N_c})\\) Memory increase to \\(n m \\frac{N_c - 1}{m - 1}\\) Because there are \\(N_n=(m + m^{2} + m^{3} +\\cdots)\\) nodes, and that \\(N_n = m\\frac{m^{\\log_m N_c} -1}{m-1}=m\\frac{N_c - 1}{m-1}\\) cons perform worse then full-search, since taking branch. Product Code \u00b6 use codebook represent vector direction with size \\(N_1\\) , and codebooks represent vector length with size \\(N_2\\) . this case, we can represent \\(N_1 N_2\\) vectors with only size \\(N_1 + N_2\\) therefore, in same time complexity and bit rate, perform better the full-search M/RVQ \u00b6 \u5e73\u5747/\u9918\u503c VQ \u5c0d\u6bcf\u500b blocks (e.g. \\(n=4\\times 4=16\\) ) \u6e1b\u53bb\u5e73\u5747 \u50b3\u9001\u5e73\u5747 (with DPCM or something) do VQ and send I/RVQ \u00b6 \u5167\u63d2/\u9918\u503c VQ do subsampling to original image (assume \\(N\\times N\\) ) and would get sub-image ( \\(N/\u2113 \\times N/\u2113\\) , normally \\(\u2113=8\\) ). do up-sampling by interpolation, and get the residual image. split residual image to blocks and do VQ. pros perform better (less blocking artifacts) than M/RVQ. G/SVQ \u00b6 Gain/Shape VQ find and send the vector that match the most (has greatest dot product value) CVQ \u00b6 Classification VQ split the image to blocks classify blocks to categories for every categories, there are some specific codebooks normally use many small codebooks, but can reach similar performance with the normal VQ using large codebooks. FSVQ \u00b6 Finite State VQ Given codebooks per state \\(C(s_i)\\) transition function \\(s_i = f(s_{i-1}, Y_{i-1})\\) given \\(s_0\\) , and find \\(Y_0\\) by \\(C(s_0)\\) get \\(s_1 = f(s_0, Y_0)\\) , and find \\(Y_1\\) until all done cons one connection drop can cause serious consequence.","title":"DataCompression"},{"location":"Data%20Compression/DataCompression/#source+coding","text":"Shannon's Source Coding, \u4fe1\u6e90\u7de8\u78bc\u3001\u7b26\u865f\u6e90\u7de8\u78bc information mapping (bits, characters, ....)","title":"Source Coding"},{"location":"Data%20Compression/DataCompression/#entropy","text":"","title":"Entropy"},{"location":"Data%20Compression/DataCompression/#self-information","text":"by three assumptions \\(I(p) \\ge 0\\) \\(I(p_1 \\cdot p_2)=I(p_1) + I(p_2)\\) \\(I(p)\\) is continuous to \\(p\\) thus, gives \\(I(p) = -\\log(p)\\) proof at p.25","title":"Self-Information"},{"location":"Data%20Compression/DataCompression/#entropy_1","text":"Given, \\[ \\begin{gather} S=\\{s_i \\,|\\, p(s_i) = p_i\\} \\end{gather} \\] then, \\[ \\begin{gather} H_r(S) = \\sum\\limits p_i I(s_i)=\\sum\\limits -p_i\\log_r(p_i) \\end{gather} \\] note that \\(r\\) is the base of log","title":"Entropy"},{"location":"Data%20Compression/DataCompression/#gibbs+inequality","text":"\\[ \\begin{gather} \\sum\\limits - p_i\\log(p_i) \\le \\sum\\limits-p_i\\log(q_i) \\end{gather} \\] the lower bound of cross entropy of \\(q_i\\) and \\(p_i\\) are \\(H(p_i)\\) .","title":"Gibbs' Inequality"},{"location":"Data%20Compression/DataCompression/#unique+decodable++instantaneous+code","text":"p32","title":"Unique Decodable &amp; Instantaneous Code"},{"location":"Data%20Compression/DataCompression/#unique+decodable","text":"not unique decodable example \\[ \\begin{gather} (s_1, s_2, s_3, s_4) = (0, 01, 11, 00) \\end{gather} \\] then for the message \\(0011\\) could be decoded as \\[ 0011 = \\left\\{ \\begin{align} &s_4s_3 \\\\ &s_1s_1s_3 \\end{align}\\right. \\]","title":"Unique Decodable"},{"location":"Data%20Compression/DataCompression/#instantaneous+code","text":"instantaneous iff \u6c92\u6709\u4e00\u500b\u7b26\u865f\u662f\u53e6\u4e00\u500b\u7b26\u865f\u7684\u5b57\u9996 instantaneous \u21d2 unique the lengths of codes have to follow Kraft Inequality","title":"Instantaneous Code"},{"location":"Data%20Compression/DataCompression/#kraft+inequality","text":"If instantaneous, then the code have to end at leaves. thus, considering binary case only first we cannot build a binary tree for many short path(short coding length). Therefore, in order to build an binary tree, we must follow \\[ \\begin{gather} \\sum\\limits \\frac{1}{2^{L_i}} \\le 1 \\end{gather} \\] intuitive thinking \u2013 Considering taking a branch of binary tree have prob. 0.5 and 0.5, then the summation of prob. of all leaves is \\(1\\) . intuitively, the lowest bound of average lengths of codes is its entropy. (proof at p36.) \\[ \\begin{gather} H_r(S) \\le L_{avg} \\end{gather} \\]","title":"Kraft Inequality"},{"location":"Data%20Compression/DataCompression/#shannon-fano+coding","text":"Since we know the lower bound of coding length is its entropy, then we can have the coding length (known as Shannon-Fano Length ) as \\[ \\begin{gather} -\\log(p_i)\\le \u2113_i < -\\log(p_i) + 1 \\\\\\\\ \\iff \u2113_i = \\bigg\\lceil -\\log(p_i) \\bigg\\rceil \\end{gather} \\] drawback example Given \\(S=\\{s_1, s_2\\}\\) , and \\(k\\) is large \\[ \\begin{gather} p_1 = 2^{-k}, \\qquad p_2=1-p_1 \\\\\\\\ \\implies l_1 = k, \\qquad l_2=1 \\end{gather} \\] in Huffman coding, both \\(l_1, l_2 = 1\\) , but since \\(p_1\\) is small when \\(k\\) is large, this drawback isn't very critical.","title":"Shannon-Fano Coding"},{"location":"Data%20Compression/DataCompression/#extension+code","text":"quick concept \u2013 if we have an source code \\(S\\) , then we can define an new source code \\(T = S^{n}\\) (then the # of symbol in \\(T\\) would be \\(|S|^{n}\\) ) The \\(H(T) = nH(S)\\) , and denote \\(L_n\\) (average length of \\(n\\) -order S.F. code), then \\[ \\begin{gather} H(T) \\le L_n < H(T) + 1 \\\\\\\\ H(S) \\le \\frac{L_n}{n} < H(S) + \\frac{1}{n} \\end{gather} \\] thus when \\(n\\to \\infty\\) , then \\(\\frac{L_n}{n} \\to H(S)\\) aka Shannon's noiseless coding theorem.","title":"Extension Code"},{"location":"Data%20Compression/DataCompression/#adaptive+huffman","text":"start with one EOF and one ESC (always one in Huffman tree). EOF \u2013 send when end of file. ESC \u2013 send when new symbol is added to tree (send following with ascii of that symbol so that decoder can know what to decode).","title":"Adaptive Huffman"},{"location":"Data%20Compression/DataCompression/#jbig++jbig2","text":"code for binary data directly ( \\(S \\in \\{0, 1\\}\\) ) JBIG \u2013 high order adaptive arithmetic (for \\(P(s_t \\,|\\, s_{(t-n):(t-1)})\\) . since the target is binary, the high order coding table would be \\(2^{n}\\) , much less than high order Huffman. JBIG2 \u2013 Define decoding protocol only. That is the encoder side can be any algorithm, even lossy.","title":"JBIG &amp; JBIG2"},{"location":"Data%20Compression/DataCompression/#lz","text":"","title":"LZ"},{"location":"Data%20Compression/DataCompression/#lz77","text":"sliding window and look-ahead buffer find phrase in window, and encode text in look-ahead buffer send (displacement, length, next_char) in no match case, send (0, 0, next_char) pros fast decode cons slow encode \\(O(n)\\) , ( \\(n\\) is windows size) \\(O(m)\\) , ( \\(m\\) is look-ahead buffer size) worst case when no match prefer larger window size but cost too much time efficiency \u2193 compressed phrases size \u2191 loss memory (after dictionary is full of phrases, some of them have to be removed)","title":"LZ77"},{"location":"Data%20Compression/DataCompression/#lzss","text":"improving version of LZ77 send \\(1\\) bit indicating whether is no match instead of send (0, 0 next_char) Circular queue with WINDOW_SIZE = \\(2^{n}\\) Binary Search Tree for storing phrases (actually storing pointer to particular position of window). Node stores fixed-size length e.g. ( \"LZSS is better than LZ77\" ) with fixed length \\(5\\) , (search by comparing char-wise order) graph TD root(LZSS^) l(^is^b) ll(^bett) lr(is^be) r(ZSS^i) rl(SS^is) rr( ) rll(S^is^) rlr(tter^) root --- l root --- r l --- ll l --- lr r --- rl r --- rr rl --- rll rl --- rlr","title":"LZSS"},{"location":"Data%20Compression/DataCompression/#lz78","text":"create new phrase in dictionary at both encoder size and decoder size whenever no match initially, there is only null string in dictionary. thus whenever create a new phrase with length \\(n\\) , there must exists a phrase with length \\(n-1\\) . therefore, using multiway search tree to store phrases graph TD r('\\0' <br> 0) p1(\"'D' <br> 1\") p2(\"'A' <br> 2\") p3(\"'^' <br> 3\") p4(\"'A' <br> 4\") p5(\"'^' <br> 5\") p6(\"'D' <br> 6\") p7(\"'Y' <br> 7\") p8(\"'^' <br> 8\") p9(\"'O' <br> 9\") r --- p8 r --- p2 r --- p1 p1 --- p3 p1 --- p4 p1 --- p7 p4 --- p5 p4 --- p6 p6 --- p9 - in this case p1 = D , p3 = D^ , p5=DA^ , and so on... example at p119 thus encoder side only have to send phrase code (phrase id), without phrase's length. i.e. (phrase_id, next_char) cons slow decoding (have to maintain dictionary tree)","title":"LZ78"},{"location":"Data%20Compression/DataCompression/#lzw","text":"improving version LZ78 defined every characters as phrases first, then send only (phrase_id, ) examples: PNG, ... algorithm p122 encoder: i := 0 ; Dictionary phrases ; string in_buff ; in_buff . Add ( str [ i ]) while in_buff . Add ( str [++ i ]) if in_buff not in phrases phrases . Add ( in_buff ) OUTPUT << phrases . IndexOf ( in_buff . PopLast ()) in_buff := string ( in_buff [- 1 ]) decoder i := 0 Dictionary phrases ; int in_buff ; string last_phrase // init INPUT >> in_buff last_phrase := phrases [ in_buff ] OUTPUT << last_phrase while true INPUT >> in_buff OUTPUT << phrases [ in_buff ] phrases . Add ( last_phrase + phrases [ in_buff ][ 0 ]) last_phrase = phrase [ in_buff ]","title":"LZW"},{"location":"Data%20Compression/DataCompression/#lossy","text":"RMSE (Root Mean Square Error) SNR (Signal-to-Noise Ratio) \\[ \\begin{gather} \\text{SNR} = \\frac{E\\big[S^{2}\\big]}{E\\big[(X-\\mu)^{2}\\big]} = \\frac{E\\big[S^{2}\\big]}{\\sigma_r^{2}} \\\\\\\\ \\text{SNR}_{dB} = 10\\log_{10}{\\text{SNR}} \\end{gather} \\] in 2d media case \\[ \\begin{gather} \\text{SNR} = \\frac{Q^{2}}{\\sigma_r^{2}} \\end{gather} \\] in which \\(Q = 255\\) in 8-bits case.","title":"Lossy"},{"location":"Data%20Compression/DataCompression/#dm","text":"Delta Modulation Adaptive DM (ADM) adaptive for the magnitude of \\(\\Delta\\)","title":"DM"},{"location":"Data%20Compression/DataCompression/#dpcm","text":"Differential Pulse Code Modulation","title":"DPCM"},{"location":"Data%20Compression/DataCompression/#predictor+optimization","text":"objective \\[ \\begin{align} \\hat x_{m}^* &= \\arg\\min_{\\hat x_m} \\,\\sigma_e^{2} \\\\\\\\ &= \\arg\\min_{\\hat x_m} E\\bigg[(x_m - {\\hat x_m})^{2}\\bigg] \\end{align} \\] in which, \\[ \\begin{gather} \\hat x_m = \\sum\\limits_{i \\in [0, m)}{\\alpha_i}x_i \\end{gather} \\] then solve by differentiation, we have \\[ \\begin{gather} E\\big[(x_m-\\hat x_m)x_i\\big] = 0 \\end{gather} \\] thus \\[ \\begin{gather} E[x_mx_i] = E[\\hat x_m x_i] \\\\\\\\ R_{mi} = E[\\hat x_m x_i] \\end{gather} \\] and further when \\(i=m\\) , \\[ \\begin{gather} E\\left[x_m^{2}\\right]=E\\left[\\hat x_m x_m\\right] \\\\\\\\ \\implies \\sigma_e^{2} = E\\bigg[(x_m -\\hat x_m)^{2}\\bigg]= \\\\\\\\ E\\big[(x_m-\\hat x_m)\\hat x_m\\big]=E\\big[x_m^{2}\\big] - E\\big[\\hat x_m^{2}\\big] \\\\\\\\ \\implies \\sigma^{2} < E\\big[x_m^{2}\\big] \\end{gather} \\] p141","title":"Predictor Optimization"},{"location":"Data%20Compression/DataCompression/#quantizer+optimization","text":"objective ( \\(N\\) is number of order of quantizer) \\[ \\begin{gather} D=\\sum\\limits_{i \\in [0, N)}{\\int_{d_i}^{d_{i+1}}p(e)\\,(e-r_i)^{2}de} \\end{gather} \\] p143","title":"Quantizer Optimization"},{"location":"Data%20Compression/DataCompression/#adaptive+dpcm+adpcm","text":"\u6620\u5c04\u91cf\u5316\u5668 \\(e=x_m - \\hat x_m\\) \\(x_m \\in [0, 2^{n})\\) , thus normally \\(e \\in(-2^{n}, 2^{n})\\) however with known \\(\\hat x_m\\) , then \\(e \\in [-\\hat x_m,\\, 2^{n} - \\hat x_m)\\) \u66ff\u63db\u91cf\u5316\u5668 use multi quantizers, and encode with the best quantizer, and send which of quantizers.","title":"Adaptive DPCM (ADPCM)"},{"location":"Data%20Compression/DataCompression/#lossless+dpcm","text":"without quantizers, send the \\(e\\) directly (or further using other lossless algorithm). thus can be used as preprocessor for others like Huffman , Arith . (since quite popular). more efficient then using adaptive like AHuff , AArith , but perform closely.","title":"Lossless DPCM"},{"location":"Data%20Compression/DataCompression/#non-redundant+sample+coding","text":"aka adaptive sampling coding","title":"Non-Redundant Sample Coding"},{"location":"Data%20Compression/DataCompression/#polynomial+predictor","text":"\\[ \\begin{gather} x_t = x_{t-1} + \\Delta x_{t-1}+\\Delta^{2}x_{t-1}+\\cdots \\end{gather} \\] in which \\[ \\begin{gather} \\Delta^{2}x_{t-1} = \\Delta x_{t-1} - \\Delta x_{t-2} \\end{gather} \\]","title":"Polynomial Predictor"},{"location":"Data%20Compression/DataCompression/#polynomial+interpolator","text":"1\u6b21\u5167\u63d2\u6cd5 = \u6247\u5f62\u6f14\u7b97\u6cd5 = SAPA2","title":"Polynomial Interpolator"},{"location":"Data%20Compression/DataCompression/#aztec","text":"p173 rules use horizontal line if \\(\\ge 3\\) successive samples satisfy \\(x_{max} - x_{min} < \\lambda\\) otherwise, use slope. Further, if next sample have the same signed of slope and \\(|x_m - x_{m-1}| > \\lambda\\) , then keep redundant.","title":"AZTEC"},{"location":"Data%20Compression/DataCompression/#corner","text":"CORCER > SAPA2 > AZTEC algorithm 2-order diff, \\(x''(i)=x(i+1)+x(i-1)-2x(i)\\) \\(\\forall i\\) , if \\(x(i) > \\lambda_1\\) and \\(x''(i)\\) is local maximum, then make \\(x(i)\\) then now have redundant samples \\(x(m_1), x(m_2), x(m_3), \\dots\\) for all redundant, find if \\(x\\left(\\frac{m_1+m_2}{2}\\right)- \\frac{x(m_1)+x(m_2)}{2} > \\lambda_2\\) (that is, if the \\(x\\) very concave (\u51f9)) if not, add \\(x(\\frac{m_1 + m_2}{2})\\) , as \\(x(m_{1\\_2})\\) , and do \\(\\big\\{x(m_1)\\) , \\(x(m_{1\\_2})\\big\\}\\) and \\(\\big\\{x(m_{1\\_2}), x(m_2)\\big\\}\\) as step 4. if step4. so, continue","title":"CORNER"},{"location":"Data%20Compression/DataCompression/#btc","text":"-Block Truncation Coding","title":"BTC"},{"location":"Data%20Compression/DataCompression/#moment-preserving+quantizer","text":"target \u2013 find quantizer that make 1st and 2nd moment unchanged, and since \\[ \\begin{gather} \\sigma^{2} = E\\big[X^{2}\\big] - E\\big[X\\big]^{2} \\end{gather} \\] the variance \\(\\sigma^{2}\\) would also unchanged. solution \u2013 Given \\(X_{th}\\) as the threshold (normally, mean of all pixels), and \\(q\\) is # of \\(b\\) cases \\[ \\hat x_i = \\begin{dcases*} a & if $x_i < X_{th}$ \\\\\\\\ b & otherwise \\end{dcases*} \\] \\[ \\begin{align} \\\\ E\\big[X\\big]&=\\frac{(m-q)\\cdot b+q\\cdot a}{m} \\\\\\\\ E\\big[X^{2}\\big]&=\\frac{(m-q)\\cdot b^{2} + q\\cdot a^{2}}{m} \\end{align} \\] then, \\[ \\begin{align} a &= E\\big[X\\big] - \\sigma \\sqrt{\\frac{q}{m-q}} \\\\\\\\ a &= E\\big[X\\big] + \\sigma \\sqrt{\\frac{m-q}{q}} \\end{align} \\] solution2 (Absolute Moment BTC, AMBTC) since the square root is hard to compute, thus we maintain the absolute moment instead of 2nd moment. \\[ \\begin{gather} \\alpha=E\\bigg[|X_i - \\mu|\\bigg] \\end{gather} \\] then, we can have, \\[ \\begin{align} a &= E[X] - \\frac{m\\alpha}{2(m-q)} \\\\\\\\ b &= E[X] + \\frac{m\\alpha}{2q} \\end{align} \\]","title":"Moment-Preserving Quantizer"},{"location":"Data%20Compression/DataCompression/#transform+coding","text":"Rotation Matrix \\[ M(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin \\theta \\\\ \\sin\\theta & \\cos \\theta \\end{bmatrix} \\] ### Zonal Sampling - \u5340\u57df\u53d6\u6a23 1. \u4fdd\u7559\u4f4e\u983b\uff0c\u9ad8\u983b\u901a\u5e38\u5c0f\uff0c\u4e0d\u7559 2. \u4f4e\u983b\u7528\u6bd4\u8f03\u591a bits 3. \u56fa\u5b9a bits per block \u4f46\u8b93 variance \u5927(\u5c0d\u5176\u4ed6 blocks \u7684\u540c\u4f4d\u7f6e)\u7684\u4fc2\u6578\u6709\u6bd4\u8f03\u591a bits - cons - \u53ef\u80fd\u6709\u5f88\u5927\u96e3\u4ee5\u5ffd\u7565\u7684\u4fc2\u6578","title":"Transform Coding"},{"location":"Data%20Compression/DataCompression/#threshold+sampling","text":"\u81e8\u754c\u53d6\u6a23 \u8a2d threshold \uff0c\u4ee5\u4e0b\u70ba 0\uff0c\u4ee5\u4e0a\u9001\u4f4d\u7f6e\u8207\u503c","title":"Threshold Sampling"},{"location":"Data%20Compression/DataCompression/#jpeg+encode","text":"get \\(F^{*}(u, v)\\) scan with z order First coefficient(DC) encode with DPCM and Huffman. encode remains coefficients(AC) with the following law omit \\(0\\) lookup category \\(k\\) at p220 according to category \\(k\\) and # of \\(0\\) before this coefficient, lookup table in p224. append \\(k\\) bits which indicates the index of AC coefficient in that category.","title":"JPEG encode"},{"location":"Data%20Compression/DataCompression/#dct","text":"p209","title":"DCT"},{"location":"Data%20Compression/DataCompression/#vector+quantization","text":"Cost of encoding \\(O(nN_c)\\) , where \\(n\\) is dimension of vector, \\(N_c\\) is number of codebooks. need memory \\(n N_c\\)","title":"Vector Quantization"},{"location":"Data%20Compression/DataCompression/#lbg","text":"LBG\u6f14\u7b97\u6cd5\u662f\u7531Linde,Buzo,Gray\u4e09\u4eba\u57281980\u5e74\u63d0\u51fa\u7684\u3002\u5176\u7b97\u6cd5\u8207K-means\u96f7\u540c\uff0c\u6839\u64da\u7576\u524d\u5283\u5206\u4e4b\u7fa4\u96c6\u8a08\u7b97\u8aa4\u5dee\u91cf\uff0c\u4e0d\u65b7\u8abf\u6574\u6620\u5c04\u5340\u9593(Mapping Region)\u53ca\u91cf\u5316\u5411\u91cf\u4e4b\u91cf\u5316\u9ede: \u7d66\u5b9a\u8a13\u7df4\u6a23\u672c\u4ee5\u53ca\u8aa4\u5dee\u95be\u503c \u8a02\u5b9a\u521d\u59cb\u78bc\u5411\u91cf \u5c07\u758a\u4ee3\u8a08\u6578\u5668\u6b78\u96f6 \u8a08\u7b97\u7e3d\u8aa4\u5dee\u503c\uff0c\u82e5\u4e0d\u70ba\u7b2c\u4e00\u6b21\uff0c\u5247\u6aa2\u67e5\u8207\u524d\u4e00\u6b21\u8aa4\u5dee\u503c\u5dee\u8ddd\u662f\u5426\u5c0f\u65bc\u95be\u503c\u3002 \u6839\u64da\u6bcf\u4e00\u500b\u8a13\u7df4\u6a23\u672c\u8207\u78bc\u5411\u91cf\u7684\u8ddd\u96e2d\uff0c\u627e\u5176\u6700\u5c0f\u503c\uff0c\u5b9a\u7fa9\u70ba\u6620\u5c04\u51fd\u6578Q \u66f4\u65b0\u78bc\u5411\u91cf\uff1a\u5c07\u5c0d\u61c9\u5230\u540c\u4e00\u500b\u78bc\u5411\u91cf\u7684\u5168\u6578\u8a13\u7df4\u6a23\u672c\u505a\u5e73\u5747\u4ee5\u66f4\u65b0\u78bc\u5411\u91cf\u3002 i\u70ba\u758a\u4ee3\u8a08\u6578\u5668\uff0cC\u70ba\u8a72\u7fa4\u96c6\u4e4b\u4ee3\u8868\uff0cx\u70ba\u8cc7\u6599\u9ede\uff0cQ(x)\u70bax\u91cf\u5316\u5f8c\u4e4b\u7fa4\u96c6\u4ee3\u8868C \u758a\u4ee3\u8a08\u6578\u5668\u52a0\u4e00 \u6703\u5230\u6b65\u9a5f\u56db\uff0c\u76f4\u81f3\u8aa4\u5dee\u503c\u5c0f\u65bc\u95a5\u503c LBG\u6f14\u7b97\u6cd5\u5341\u5206\u4f9d\u8cf4\u8d77\u59cb\u7de8\u78bc\u7c3f\uff0c\u7522\u751f\u8d77\u59cb\u7de8\u78bc\u7c3f\u7684\u65b9\u6cd5\u6709\u4ee5\u4e0b\u5e7e\u7a2e\uff1a","title":"LBG"},{"location":"Data%20Compression/DataCompression/#tree-structured+codebooks","text":"\\(m\\) -ways tree Cost now decrease to \\(O(n \\cdot m\\log_m{N_c})\\) Memory increase to \\(n m \\frac{N_c - 1}{m - 1}\\) Because there are \\(N_n=(m + m^{2} + m^{3} +\\cdots)\\) nodes, and that \\(N_n = m\\frac{m^{\\log_m N_c} -1}{m-1}=m\\frac{N_c - 1}{m-1}\\) cons perform worse then full-search, since taking branch.","title":"Tree-Structured Codebooks"},{"location":"Data%20Compression/DataCompression/#product+code","text":"use codebook represent vector direction with size \\(N_1\\) , and codebooks represent vector length with size \\(N_2\\) . this case, we can represent \\(N_1 N_2\\) vectors with only size \\(N_1 + N_2\\) therefore, in same time complexity and bit rate, perform better the full-search","title":"Product Code"},{"location":"Data%20Compression/DataCompression/#mrvq","text":"\u5e73\u5747/\u9918\u503c VQ \u5c0d\u6bcf\u500b blocks (e.g. \\(n=4\\times 4=16\\) ) \u6e1b\u53bb\u5e73\u5747 \u50b3\u9001\u5e73\u5747 (with DPCM or something) do VQ and send","title":"M/RVQ"},{"location":"Data%20Compression/DataCompression/#irvq","text":"\u5167\u63d2/\u9918\u503c VQ do subsampling to original image (assume \\(N\\times N\\) ) and would get sub-image ( \\(N/\u2113 \\times N/\u2113\\) , normally \\(\u2113=8\\) ). do up-sampling by interpolation, and get the residual image. split residual image to blocks and do VQ. pros perform better (less blocking artifacts) than M/RVQ.","title":"I/RVQ"},{"location":"Data%20Compression/DataCompression/#gsvq","text":"Gain/Shape VQ find and send the vector that match the most (has greatest dot product value)","title":"G/SVQ"},{"location":"Data%20Compression/DataCompression/#cvq","text":"Classification VQ split the image to blocks classify blocks to categories for every categories, there are some specific codebooks normally use many small codebooks, but can reach similar performance with the normal VQ using large codebooks.","title":"CVQ"},{"location":"Data%20Compression/DataCompression/#fsvq","text":"Finite State VQ Given codebooks per state \\(C(s_i)\\) transition function \\(s_i = f(s_{i-1}, Y_{i-1})\\) given \\(s_0\\) , and find \\(Y_0\\) by \\(C(s_0)\\) get \\(s_1 = f(s_0, Y_0)\\) , and find \\(Y_1\\) until all done cons one connection drop can cause serious consequence.","title":"FSVQ"},{"location":"Electromagnetics/Antenna/","text":"Antenna \u00b6 Antenna Characteristics \u00b6 Radiation Intensity \u00b6 \\[ \\begin{gather} U(\\theta, \\phi) \\equiv r^{2} P_{avg} \\end{gather} \\] independent to \\(r\\) represent the power density at particular direction. \\[ \\begin{gather} P_{rad} = \\int_S r^{2}\\vec P_{avg}\\cdot d\\vec S = \\int_\\Omega U \\, d\\Omega \\\\\\\\ \\implies P_{rad} = 4\\pi U_{avg} \\end{gather} \\] Directive Gain \u00b6 \\[ \\begin{gather} G_d(\\theta, \\phi) = \\frac{U(\\theta, \\phi)}{U_{avg}} \\end{gather} \\] normalized version of \\(U\\) . Directivity \u00b6 \\[ \\begin{gather} D = \\max\\big\\{ G_d{(\\theta, \\phi)} \\big\\} \\end{gather} \\] sharpness criteria for antenna Power Gain & Efficiency \u00b6 in average efficiency \\[ \\begin{gather} \\eta_r = \\frac{P_{rad}}{P_{in}} \\end{gather} \\] power gain \\[ \\begin{gather} G_{p-avg}=\\eta_r = \\frac{P_{rad}}{P_{in}} \\end{gather} \\] at particular direction power gain \\[ \\begin{gather} G_{p}(\\theta, \\phi) = \\frac{4\\pi U(\\theta, \\phi)}{P_{in}} \\end{gather} \\] Effective Aperture \u00b6 isotropy \\[ \\begin{gather} A_{iso} = \\frac{\\lambda^{2}}{4\\pi} \\end{gather} \\] effective \\[ \\begin{gather} A_e = A_{iso} D = \\frac{\\lambda^{2}}{4\\pi}D \\end{gather} \\] Beam Solid Angle \u00b6 \\[ \\begin{gather} \\Omega_A = \\frac{P_{rad}}{U_{max}} \\end{gather} \\] fff \u00b6 Hertain dipole \u00b6 \\[ \\begin{align} H_{\\phi s } &= \\frac{j\\beta I_0 \\,d\u2113}{4\\pi r} \\sin \\theta \\,e^{-j\\beta r} \\\\\\\\ E_{\\theta s} &= \\eta H_{\\phi s } \\end{align} \\] Half wave \u00b6 \\[ \\begin{align} H_{\\phi s} &= \\frac{jI_0 \\, e^{-j\\beta r}}{2\\pi}\\frac{\\cos\\big[\\cos\\theta\\big]}{r\\sin\\theta} \\\\\\\\ E_{\\theta s} &= \\eta H_{\\phi s } \\end{align} \\] Radar \u00b6 \\[ \\begin{align} \ud835\udd13_i &= \\frac{P_{rad}}{4\\pi r^{2}}G_d \\\\\\\\ \ud835\udd13_s &= \\lim_{r\\to \\infty}\\frac{\ud835\udd13_i\\,\\sigma}{4\\pi r^{2}} \\end{align} \\]","title":"Antenna"},{"location":"Electromagnetics/Antenna/#antenna","text":"","title":"Antenna"},{"location":"Electromagnetics/Antenna/#antenna+characteristics","text":"","title":"Antenna Characteristics"},{"location":"Electromagnetics/Antenna/#radiation+intensity","text":"\\[ \\begin{gather} U(\\theta, \\phi) \\equiv r^{2} P_{avg} \\end{gather} \\] independent to \\(r\\) represent the power density at particular direction. \\[ \\begin{gather} P_{rad} = \\int_S r^{2}\\vec P_{avg}\\cdot d\\vec S = \\int_\\Omega U \\, d\\Omega \\\\\\\\ \\implies P_{rad} = 4\\pi U_{avg} \\end{gather} \\]","title":"Radiation Intensity"},{"location":"Electromagnetics/Antenna/#directive+gain","text":"\\[ \\begin{gather} G_d(\\theta, \\phi) = \\frac{U(\\theta, \\phi)}{U_{avg}} \\end{gather} \\] normalized version of \\(U\\) .","title":"Directive Gain"},{"location":"Electromagnetics/Antenna/#directivity","text":"\\[ \\begin{gather} D = \\max\\big\\{ G_d{(\\theta, \\phi)} \\big\\} \\end{gather} \\] sharpness criteria for antenna","title":"Directivity"},{"location":"Electromagnetics/Antenna/#power+gain++efficiency","text":"in average efficiency \\[ \\begin{gather} \\eta_r = \\frac{P_{rad}}{P_{in}} \\end{gather} \\] power gain \\[ \\begin{gather} G_{p-avg}=\\eta_r = \\frac{P_{rad}}{P_{in}} \\end{gather} \\] at particular direction power gain \\[ \\begin{gather} G_{p}(\\theta, \\phi) = \\frac{4\\pi U(\\theta, \\phi)}{P_{in}} \\end{gather} \\]","title":"Power Gain &amp; Efficiency"},{"location":"Electromagnetics/Antenna/#effective+aperture","text":"isotropy \\[ \\begin{gather} A_{iso} = \\frac{\\lambda^{2}}{4\\pi} \\end{gather} \\] effective \\[ \\begin{gather} A_e = A_{iso} D = \\frac{\\lambda^{2}}{4\\pi}D \\end{gather} \\]","title":"Effective Aperture"},{"location":"Electromagnetics/Antenna/#beam+solid+angle","text":"\\[ \\begin{gather} \\Omega_A = \\frac{P_{rad}}{U_{max}} \\end{gather} \\]","title":"Beam Solid Angle"},{"location":"Electromagnetics/Antenna/#fff","text":"","title":"fff"},{"location":"Electromagnetics/Antenna/#hertain+dipole","text":"\\[ \\begin{align} H_{\\phi s } &= \\frac{j\\beta I_0 \\,d\u2113}{4\\pi r} \\sin \\theta \\,e^{-j\\beta r} \\\\\\\\ E_{\\theta s} &= \\eta H_{\\phi s } \\end{align} \\]","title":"Hertain dipole"},{"location":"Electromagnetics/Antenna/#half+wave","text":"\\[ \\begin{align} H_{\\phi s} &= \\frac{jI_0 \\, e^{-j\\beta r}}{2\\pi}\\frac{\\cos\\big[\\cos\\theta\\big]}{r\\sin\\theta} \\\\\\\\ E_{\\theta s} &= \\eta H_{\\phi s } \\end{align} \\]","title":"Half wave"},{"location":"Electromagnetics/Antenna/#radar","text":"\\[ \\begin{align} \ud835\udd13_i &= \\frac{P_{rad}}{4\\pi r^{2}}G_d \\\\\\\\ \ud835\udd13_s &= \\lim_{r\\to \\infty}\\frac{\ud835\udd13_i\\,\\sigma}{4\\pi r^{2}} \\end{align} \\]","title":"Radar"},{"location":"Electromagnetics/Boundary%20Value%20Problem/","text":"Boundary Value Problem \u00b6 2D Fourier Series 2D Fourier Series 2D Fourier Series \u00b6 \\(p\\) is a half of period \\[ \\begin{gather} f(x,y) = \\sum{c_{mn}e^{jmx}e^{jny}} \\\\\\\\ c_{mn} = \\frac{1}{2p_x}\\frac{1}{2p_y}\\int_{Py}\\int_{Px}{f(x, y)e^{-jmx}e^{-jny}\\ dxdy} \\end{gather} \\] 2D Fourier Sine Series \u00b6 \\(p\\) is a half of period \\[ \\begin{gather} f(x,y) = \\sum_{n, m \\in \u2115}{b_{m, n}\\sin{\\frac{n\\pi}{p_x} x}\\,\\sin{\\frac{n\\pi}{p_y} y}} \\\\\\\\ b_{m, n} = \\frac{2}{p_x}\\frac{2}{p_y}\\int_{P_y}\\int_{P_x}{f(x,y)\\sin{\\frac{n\\pi}{p_x}x}\\,\\sin{\\frac{n\\pi}{p_y}y}\\,dxdy} \\end{gather} \\] similar to 1D Fourier Sine Series Possion's Equation \u00b6 \\[ \\begin{gather} \\nabla^{2}V=-\\frac{\\rho_v}{\\varepsilon_0} \\end{gather} \\] we can solve Possion's equation by superposition which similar to how we solve non-homogeneous differential equation. let \\(V = V_L + V_P\\) , in which \\(V_L\\) indicates the homogeneous differential equation case ( \\(\\rho_v = 0\\) ) i.e., Laplace's equation. \\[ \\begin{gather} \\nabla^{2} V_L =0 \\end{gather} \\] as for the \\(V_P\\) indicates the homogeneous boundary conditions, i.e., \\(\\forall \\,\\text{boundary }V_b = 0\\) \\[ \\begin{gather} \\nabla^{2}V_P=-\\frac{\\rho_v}{\\varepsilon_0} \\end{gather} \\] assume that \\[ \\begin{gather} V_p = \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty}{A_{mn}\\sin{\\frac{m\\pi x}{b}}\\sin{\\frac{n\\pi y}{a}}} \\\\\\\\ \\rho_v = \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty}{C_{mn}\\sin{\\frac{m\\pi x}{p_x}}\\sin{\\frac{n\\pi y}{p_y}}} \\end{gather} \\] notice that usually \\(C_{mn}\\) is known, but the \\(A_{mn}\\) is unknown for \\(C_{mn}\\) \\[ \\begin{gather} C_{mn} = \\frac{2}{p_x}\\frac{2}{p_y}\\int_X\\int_Y {\\rho_v\\sin\\frac{m\\pi x}{p_x}\\sin\\frac{n\\pi y}{p_y}dxdy} \\end{gather} \\] \\[ \\begin{align} \\nabla^{2}V_P=\\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty}{A_{mn}\\left[\\left(\\frac{m\\pi}{p_x}\\right)^{2}+\\left(\\frac{n\\pi}{p_y}\\right)^{2}\\right]\\sin{\\frac{m\\pi x}{b}}\\sin{\\frac{n\\pi y}{a}}} \\end{align} \\] \\[ \\begin{gather} \\implies A_{mn} \\end{gather} \\] 3D Laplace's Equation \u00b6 Cartesian \u00b6 \\[ \\begin{gather} V(x, y, z) = X(x) Y(y) Z(z) \\\\\\\\ \\end{gather} \\] \\[ \\begin{gather} \\nabla^{2}V = 0 \\\\\\\\ \\frac{X''}{X}+\\frac{Y''}{Y}+\\frac{Z''}{Z} =-k_x^{2}-k_y^{2}-k_z^{2} = 0 \\end{gather} \\] \\[ \\implies \\left\\{ \\begin{gather} X''+k_x^{2}X &= 0 \\\\\\\\ Y''+k_y^{2}Y &= 0 \\\\\\\\ Z''+k_z^{2}Z &= 0 \\\\\\\\ k_{x}^{2}+k_y^{2}+k_z^{2} &= 0 \\end{gather}\\right. \\] Cylindrical \u00b6 \\[ \\begin{gather} V(\\rho, \\phi, z) = P(\\rho)\\Phi(\\phi)Z(z) \\\\\\\\ \\nabla^{2}V=\\frac{1}{\\rho}\\frac{\\partial}{\\partial \\rho}\\left(\\rho V_\\rho\\right)+\\frac{1}{\\rho^{2}}V_{\\phi\\phi}+V_{zz} = 0 \\\\\\\\ \\frac{\\nabla^{2}V}{P\\Phi Z}=\\frac{1}{\\rho}\\left(\\rho \\frac{P''}{P}+\\frac{P'}{P}\\right)+\\frac{1}{\\rho^{2}}\\frac{\\Phi''}{\\Phi}+\\frac{Z''}{Z} =0 \\\\\\\\ \\implies \\frac{1}{\\rho}\\left(\\rho \\frac{P''}{P}+\\frac{P'}{P}\\right)+\\frac{1}{\\rho^{2}}\\frac{\\Phi''}{\\Phi}=-\\frac{Z''}{Z} \\equiv \\mathbf{-\\lambda^{2}} \\\\\\\\ \\rho^{2} \\frac{P''}{P}+\\rho\\frac{P'}{P}+\\rho^{2}\\lambda^{2}=-\\frac{\\Phi''}{\\Phi} \\equiv \\mu^{2} \\\\\\\\ \\end{gather} \\] $$ \\implies \\left{ \\begin{gather} Z''-\\lambda^{2}Z=0 \\\\ \\Phi'' + \\mu^{2}\\Phi = 0 \\\\ \\rho^{2}P'' + \\rho P' + \\left(\\rho {2}\\lambda - \\mu^{2}\\right)P = 0 & #Eq3 \\end{gather}\\right. $$ in which Eq. 3 is a Bessel Function notice that if \\(\\lambda = 0\\) (e.g. the situation that \\(V\\) is independent of \\(z\\) ), Eq. 3 would be an Euler-Cauchy Equation, rather than Bessel. for \\(\\lambda > 0\\) , let \\(x=\\rho\\lambda\\) and \\(y=P\\) note that \\[ \\begin{gather} dx=d(\\rho\\lambda)=\\lambda d\\rho \\\\\\\\ y'=\\frac{dy}{dx}=\\frac{dP}{dx}=\\frac{1}{\\lambda }\\frac{dP}{d\\rho} \\end{gather} \\] Eq.3 become \\[ \\begin{gather} x^{2}y''+xy+(x^{2}-\\mu^{2})y=0 \\end{gather} \\]","title":"Boundary Value Problem"},{"location":"Electromagnetics/Boundary%20Value%20Problem/#boundary+value+problem","text":"2D Fourier Series 2D Fourier Series","title":"Boundary Value Problem"},{"location":"Electromagnetics/Boundary%20Value%20Problem/#2d+fourier+series","text":"\\(p\\) is a half of period \\[ \\begin{gather} f(x,y) = \\sum{c_{mn}e^{jmx}e^{jny}} \\\\\\\\ c_{mn} = \\frac{1}{2p_x}\\frac{1}{2p_y}\\int_{Py}\\int_{Px}{f(x, y)e^{-jmx}e^{-jny}\\ dxdy} \\end{gather} \\]","title":"2D Fourier Series"},{"location":"Electromagnetics/Boundary%20Value%20Problem/#2d+fourier+sine+series","text":"\\(p\\) is a half of period \\[ \\begin{gather} f(x,y) = \\sum_{n, m \\in \u2115}{b_{m, n}\\sin{\\frac{n\\pi}{p_x} x}\\,\\sin{\\frac{n\\pi}{p_y} y}} \\\\\\\\ b_{m, n} = \\frac{2}{p_x}\\frac{2}{p_y}\\int_{P_y}\\int_{P_x}{f(x,y)\\sin{\\frac{n\\pi}{p_x}x}\\,\\sin{\\frac{n\\pi}{p_y}y}\\,dxdy} \\end{gather} \\] similar to 1D Fourier Sine Series","title":"2D Fourier Sine Series"},{"location":"Electromagnetics/Boundary%20Value%20Problem/#possions+equation","text":"\\[ \\begin{gather} \\nabla^{2}V=-\\frac{\\rho_v}{\\varepsilon_0} \\end{gather} \\] we can solve Possion's equation by superposition which similar to how we solve non-homogeneous differential equation. let \\(V = V_L + V_P\\) , in which \\(V_L\\) indicates the homogeneous differential equation case ( \\(\\rho_v = 0\\) ) i.e., Laplace's equation. \\[ \\begin{gather} \\nabla^{2} V_L =0 \\end{gather} \\] as for the \\(V_P\\) indicates the homogeneous boundary conditions, i.e., \\(\\forall \\,\\text{boundary }V_b = 0\\) \\[ \\begin{gather} \\nabla^{2}V_P=-\\frac{\\rho_v}{\\varepsilon_0} \\end{gather} \\] assume that \\[ \\begin{gather} V_p = \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty}{A_{mn}\\sin{\\frac{m\\pi x}{b}}\\sin{\\frac{n\\pi y}{a}}} \\\\\\\\ \\rho_v = \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty}{C_{mn}\\sin{\\frac{m\\pi x}{p_x}}\\sin{\\frac{n\\pi y}{p_y}}} \\end{gather} \\] notice that usually \\(C_{mn}\\) is known, but the \\(A_{mn}\\) is unknown for \\(C_{mn}\\) \\[ \\begin{gather} C_{mn} = \\frac{2}{p_x}\\frac{2}{p_y}\\int_X\\int_Y {\\rho_v\\sin\\frac{m\\pi x}{p_x}\\sin\\frac{n\\pi y}{p_y}dxdy} \\end{gather} \\] \\[ \\begin{align} \\nabla^{2}V_P=\\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty}{A_{mn}\\left[\\left(\\frac{m\\pi}{p_x}\\right)^{2}+\\left(\\frac{n\\pi}{p_y}\\right)^{2}\\right]\\sin{\\frac{m\\pi x}{b}}\\sin{\\frac{n\\pi y}{a}}} \\end{align} \\] \\[ \\begin{gather} \\implies A_{mn} \\end{gather} \\]","title":"Possion's Equation"},{"location":"Electromagnetics/Boundary%20Value%20Problem/#3d+laplaces+equation","text":"","title":"3D Laplace's Equation"},{"location":"Electromagnetics/Boundary%20Value%20Problem/#cartesian","text":"\\[ \\begin{gather} V(x, y, z) = X(x) Y(y) Z(z) \\\\\\\\ \\end{gather} \\] \\[ \\begin{gather} \\nabla^{2}V = 0 \\\\\\\\ \\frac{X''}{X}+\\frac{Y''}{Y}+\\frac{Z''}{Z} =-k_x^{2}-k_y^{2}-k_z^{2} = 0 \\end{gather} \\] \\[ \\implies \\left\\{ \\begin{gather} X''+k_x^{2}X &= 0 \\\\\\\\ Y''+k_y^{2}Y &= 0 \\\\\\\\ Z''+k_z^{2}Z &= 0 \\\\\\\\ k_{x}^{2}+k_y^{2}+k_z^{2} &= 0 \\end{gather}\\right. \\]","title":"Cartesian"},{"location":"Electromagnetics/Boundary%20Value%20Problem/#cylindrical","text":"\\[ \\begin{gather} V(\\rho, \\phi, z) = P(\\rho)\\Phi(\\phi)Z(z) \\\\\\\\ \\nabla^{2}V=\\frac{1}{\\rho}\\frac{\\partial}{\\partial \\rho}\\left(\\rho V_\\rho\\right)+\\frac{1}{\\rho^{2}}V_{\\phi\\phi}+V_{zz} = 0 \\\\\\\\ \\frac{\\nabla^{2}V}{P\\Phi Z}=\\frac{1}{\\rho}\\left(\\rho \\frac{P''}{P}+\\frac{P'}{P}\\right)+\\frac{1}{\\rho^{2}}\\frac{\\Phi''}{\\Phi}+\\frac{Z''}{Z} =0 \\\\\\\\ \\implies \\frac{1}{\\rho}\\left(\\rho \\frac{P''}{P}+\\frac{P'}{P}\\right)+\\frac{1}{\\rho^{2}}\\frac{\\Phi''}{\\Phi}=-\\frac{Z''}{Z} \\equiv \\mathbf{-\\lambda^{2}} \\\\\\\\ \\rho^{2} \\frac{P''}{P}+\\rho\\frac{P'}{P}+\\rho^{2}\\lambda^{2}=-\\frac{\\Phi''}{\\Phi} \\equiv \\mu^{2} \\\\\\\\ \\end{gather} \\] $$ \\implies \\left{ \\begin{gather} Z''-\\lambda^{2}Z=0 \\\\ \\Phi'' + \\mu^{2}\\Phi = 0 \\\\ \\rho^{2}P'' + \\rho P' + \\left(\\rho {2}\\lambda - \\mu^{2}\\right)P = 0 & #Eq3 \\end{gather}\\right. $$ in which Eq. 3 is a Bessel Function notice that if \\(\\lambda = 0\\) (e.g. the situation that \\(V\\) is independent of \\(z\\) ), Eq. 3 would be an Euler-Cauchy Equation, rather than Bessel. for \\(\\lambda > 0\\) , let \\(x=\\rho\\lambda\\) and \\(y=P\\) note that \\[ \\begin{gather} dx=d(\\rho\\lambda)=\\lambda d\\rho \\\\\\\\ y'=\\frac{dy}{dx}=\\frac{dP}{dx}=\\frac{1}{\\lambda }\\frac{dP}{d\\rho} \\end{gather} \\] Eq.3 become \\[ \\begin{gather} x^{2}y''+xy+(x^{2}-\\mu^{2})y=0 \\end{gather} \\]","title":"Cylindrical"},{"location":"Electromagnetics/EM_Wave/","text":"EM Wave Equations \u00b6 in free space by Maxwell's equations, we have \\[ \\begin{align} \\nabla \\times \\vec E &= - \\mu_0\\frac{\\partial \\vec H}{\\partial t}\\tag 1 \\\\\\\\ \\nabla \\times \\vec H &= \\varepsilon_0 \\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\end{align} \\] ^equ-1 \\[ \\begin{align} \\nabla^{2}\\vec E&=\\nabla\\left(\\nabla \\cdot \\vec E\\right)-\\nabla \\times \\nabla \\times \\vec E \\\\\\\\ &=0-\\left(-\\mu_0\\frac{\\partial }{\\partial t}\\left(\\nabla \\times \\vec H\\right)\\right) \\\\\\\\ &=\\mu_0\\varepsilon_0\\frac{\\partial^{2} \\vec E}{\\partial t^{2}} \\end{align} \\] and similar for \\(\\nabla ^{2}\\vec H\\) . We can now have the wave equations \\[ \\left\\{ \\begin{align} \\nabla ^{2}\\vec E&=\\mu_0\\varepsilon_0\\frac{\\partial^{2} \\vec E}{\\partial t^{2}} \\\\\\\\ \\nabla^{2}\\vec H &= \\mu_0\\varepsilon_0\\frac{\\partial^{2} \\vec H}{\\partial t^{2}} \\end{align}\\right. \\] compare to standard form of wave equation \\[ \\begin{gather} \\frac{\\partial ^{2}\\vec U}{\\partial t^{2}}=c^{2}\\,\\nabla ^{2} \\vec U \\end{gather} \\] we have \\[ \\begin{gather} c=\\frac{1}{\\sqrt{\\mu_0\\varepsilon_0}} \\end{gather} \\] General Solutions (Frequency Domain) \u00b6 \\[ \\begin{gather} \\nabla ^{2}\\vec {E_s} - \\gamma^{2}\\vec {E_s} = 0 \\\\\\\\ \\nabla ^{2}\\vec {H_s} - \\gamma^{2}\\vec {H_s} = 0 \\end{gather} \\] in which \\(\\gamma\\) is propagation constant. \\[ \\begin{gather} \\gamma = \\sqrt{j\\omega\\mu(\\sigma + j\\omega \\varepsilon)} \\end{gather} \\] Thinking Faraday's Law \\[ \\begin{gather} \\nabla \\times \\vec{E_s} = j\\omega\\mu \\vec{H_s} \\end{gather} \\] Ampere's Law \\[ \\begin{gather} \\nabla \\times \\vec{H_s} = \\sigma \\vec {E_s} + j\\omega \\varepsilon\\vec{E_s} \\end{gather} \\] General Solutions (Transmission Line) \u00b6 Take the wave propagates through \\(\\hat {a_z}\\) direction, then \\[ \\left\\{ \\begin{align} \\vec E= E_x^{+}(z-ut) \\, \\hat{a_x} + E^{-}_x(z+ut)\\,\\hat{a_x} \\\\\\\\ \\vec H= H_y^{+}(z-ut) \\, \\hat{a_y} + H^{-}_y(z+ut)\\,\\hat{a_y} \\end{align}\\right. \\] from \\(\\text{equation (a)}\\) we have \\[ \\begin{align} \\mu_0\\frac{\\partial \\vec H}{\\partial t} &= - \\nabla \\times \\vec E = -\\frac{\\partial \\vec E}{\\partial z} \\\\\\\\ &= -\\bigg({E_x^{+}}'(z-ut) \\, \\hat{a_x} + {E^{-}_x}'(z+ut)\\,\\hat{a_x}\\bigg) \\\\\\\\ \\implies \\vec H &= -\\frac{1}{\\mu_0}\\int\\left({E_x^{+}}'(z-ut) \\, \\hat{a_x} + {E^{-}_x}'(z+ut)\\,\\hat{a_x}\\right)\\,dt \\\\\\\\ &= \\frac{1}{u\\mu_0}\\bigg({E_x^{+}}(z-ut) \\, \\hat{a_x} - {E^{-}_x}(z+ut)\\,\\hat{a_x}\\bigg) \\\\\\\\ &= \\frac{1}{\\sqrt{\\mu_0/\\varepsilon_0}}\\bigg({E_x^{+}}(z-ut) \\, \\hat{a_x} - {E^{-}_x}(z+ut)\\,\\hat{a_x}\\bigg) \\end{align} \\] Thus we can obtain the wave impedance in free space \\(\\eta_0\\) \\[ \\begin{gather} \\eta_0 = \\sqrt\\frac{\\mu_0}{\\varepsilon_0} = \\frac{E_x^{+}}{H_x^{+}}=-\\frac{E_x^{-}}{H_x^{-}} \\end{gather} \\] Power \u00b6 Poynting Vector \\[ \\begin{gather} \\vec P = \\vec E \\times \\vec H \\qquad \\text{W/m}^{2} \\end{gather} \\] Instantaneous power across \\(S\\) \\[ \\begin{gather} P(t)=\\int_S \\vec P \\cdot d\\vec S \\end{gather} \\] complex Poynting vector not a phasor, but come up by phasors trickily. notice that complex Poynting vector is not function of \\(t\\) . Since it is generated from phasors, it represents the concept of average. \\[ \\begin{gather} \\vec P_c=\\frac{1}{2}\\vec E_s\\times\\vec H_s^{*} =\\frac{1}{2}\\vec E_s^{*}\\times\\vec H_s \\\\\\\\ \\vec P_{ave}=\\text{Re}\\left[\\vec P_c\\right]\\qquad \\text{W/m}^{2} \\end{gather} \\] Loss Tangent \u00b6 for lossless medium, \\(\\sigma = 0\\) . propagation constant \\(\\gamma\\) \\[ \\begin{gather} \\gamma = \\sqrt{j\\omega \\mu(\\sigma + j\\omega\\varepsilon)} \\end{gather} \\] Loss Tangent \\[ \\begin{gather} \\tan\\theta = \\frac{J_{cs}}{J_{ds}} = \\frac{|\\sigma E_s|}{|j\\omega\\varepsilon E_s|} = \\frac{\\sigma}{\\omega \\varepsilon} \\end{gather} \\] Loss Angle \\[ \\begin{gather} \\theta = \\tan^{-1}{\\frac{\\sigma}{\\omega\\varepsilon}} \\end{gather} \\] Skin Depth \\[ \\begin{gather} \\delta = \\frac{1}{\\alpha} \\end{gather} \\] Wave Polarization \u00b6 \\[ \\begin{gather} \\vec E = E_x \\,\\hat{a_x} + E_y \\,\\hat{a_y} \\\\\\\\ = E_{ox}\\cos{(\\omega t - \\beta z + \\phi_x)} \\, \\hat{a_x} + E_{oy}\\cos{(\\omega t - \\beta z + \\phi_y)} \\, \\hat{a_y} \\\\\\\\ \\Delta \\phi = \\phi_y - \\phi_x \\end{gather} \\] Linear Polarization \u00b6 \\[ \\begin{align} \\vec E &= E_x \\,\\hat{a_x} + E_y \\,\\hat{a_y} \\\\\\\\ &= E_1 \\,\\hat{a_1} + E_2 \\,\\hat{a_2} \\, \\end{align} \\] \\(\\Delta \\phi = n\\pi, \\quad n\\in Z^{+}\\) \\[ \\begin{gather} \\frac{E_{y}}{E_{x}}= \\frac{E_{oy}\\cos{(\\omega t - \\beta z + \\phi_x + \\Delta \\phi)}}{E_{ox}\\cos{(\\omega t - \\beta z + \\phi_x)}} = \\frac{E_{oy}}{E_{ox}} \\end{gather} \\] no necessarily perpendicular Circular Polarization \u00b6 \\(E_{ox} = E_{oy}\\) \\(\\Delta \\phi = (\\frac{1}{2} + n)\\pi\\) \\[ \\begin{gather} \\frac{E_{y}}{E_{x}}= \\frac{E_{oy}\\cos{(\\omega t - \\beta z + \\phi_x + \\Delta \\phi)}}{E_{ox}\\cos{(\\omega t - \\beta z + \\phi_x)}} = \\pm\\frac{E_{oy}}{E_{ox}} \\,\\tan{(\\omega t - \\beta z + \\phi_x)} \\end{gather} \\] thus, \\[ \\begin{gather} \\phi = \\tan^{-1}{\\frac{E_y}{E_x}}=\\pm(\\omega t - \\beta z + \\phi_x) \\implies \\text{circular} \\end{gather} \\] necessarily perpendicular Elliptical Polarization \u00b6 if \\(\\Delta \\phi > 0\\) , \\(E_y\\) leads \\(E_x\\) , (left-handed) if \\(\\Delta \\phi < 0\\) , \\(E_y\\) lags \\(E_x\\) , (right-handed) Reflection and Transmission \u00b6 Snell's Law \u00b6 lossless \u00b6 \\[ \\begin{gather} k_i \\sin{\\theta_i} = k_r \\sin{\\theta_r}=k_t\\sin{\\theta_t} \\end{gather} \\] lossy \u00b6 \\[ \\begin{gather} \\gamma_i \\sin{\\theta_i} = \\gamma_r \\sin{\\theta_r}=\\gamma_t\\sin{\\theta_t} \\end{gather} \\] True Angle of Refraction \u00b6 Since \\(\\gamma\\) is complex, \\(\\sin \\theta_t\\) and \\(\\sin \\theta_r\\) is basically complex as well. Thus \\[ \\begin{align} \\vec \\gamma_t \\cdot \\vec r &= \\gamma_t \\sin{\\theta_t}\\,x + \\gamma_t \\cos{\\theta_t}\\,z \\\\\\\\ &= \\Re\\big[ \\gamma_t \\sin{\\theta_t}\\big]x + \\Re\\big[ \\gamma_t \\cos{\\theta_t}\\big]z + j\\bigg[ \\Im\\big[ \\gamma_t \\sin{\\theta_t}\\big]x + \\Im\\big[ \\gamma_t \\cos{\\theta_t}\\big]z \\bigg] \\end{align} \\] True angle of refraction \\(\\psi_\\beta\\) \\[ \\begin{gather} \\psi_\\beta = \\tan^{-1}{\\frac{\\Im\\big[\\gamma_t\\sin\\theta_t\\big]}{\\Im\\big[\\gamma_t\\cos\\theta_t\\big]}} \\end{gather} \\] Polarization \u00b6 Parallel Polarization \u00b6 \\(E\\) is parallel to surface formed by ( \\(k_i, k_r, k_t\\) ) at \\(z=0\\) , solve, \\[ \\left\\{ \\begin{align} &E_{i0}\\cos{\\theta_i} + E_{r0}\\cos{\\theta_r} = E_{t0}\\cos{\\theta_t} \\\\\\\\ &H_{i0} - H_{r0} = H_{t0} \\end{align}\\right. \\] then we have \\[ \\begin{gather} \\Gamma_{\\parallel}=\\frac{E_{r0}}{E_{i0}}=\\frac{\\eta_2\\cos{\\theta_t}-\\eta_1\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_t}+\\eta_1\\cos{\\theta_i}} \\\\\\\\ \\tau_{\\parallel}=\\frac{E_{t0}}{E_{i0}}=\\frac{2\\eta_2\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_t}+\\eta_1\\cos{\\theta_i}} \\end{gather} \\] Perpendicular Polarization \u00b6 at \\(z=0\\) , solve, \\[ \\left\\{ \\begin{align} &-H_{i0}\\cos{\\theta_i} + H_{r0}\\cos{\\theta_r} = -H_{t0}\\cos{\\theta_t} \\\\\\\\ &E_{i0} + E_{r0} = E_{t0} \\end{align}\\right. \\] then we have \\[ \\begin{gather} \\Gamma_{\\bot}=\\frac{E_{r0}}{E_{i0}}=\\frac{\\eta_2\\cos{\\theta_i}-\\eta_1\\cos{\\theta_t}}{\\eta_2\\cos{\\theta_i}+\\eta_1\\cos{\\theta_t}} \\\\\\\\ \\tau_{\\bot}=\\frac{E_{t0}}{E_{i0}}=\\frac{2\\eta_2\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_i}+\\eta_1\\cos{\\theta_t}} \\end{gather} \\] Brewster Angle \u00b6 Denote as \\(\\theta_{B\\parallel}\\) a special \\(\\theta_i\\) that make no reflection. For nonmagnetic medium ( \\(\\mu_r = 1\\) ), only exists at parallel polarization . \\[ \\begin{gather} \\Gamma_{\\parallel} = \\frac{\\eta_2\\cos{\\theta_t}- \\eta_1\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_t}+ \\eta_1\\cos{\\theta_i}} = 0 \\\\\\\\ \\implies \\eta_2 \\cos{\\theta_t} = \\eta_1\\cos{\\theta_{B\\parallel}} \\end{gather} \\] \\[ \\begin{gather} \\\\ \\eta_2^{2} \\cos^{2}{\\theta_t} = \\eta^{2}_1 \\cos^{2}{\\theta_{B\\parallel}} \\\\\\\\ \\frac{\\mu_2}{\\varepsilon_2}\\left(1-\\frac{\\mu_1\\varepsilon_1}{\\mu_2\\varepsilon_2}\\sin^{2}{\\theta_{B\\parallel}}\\right) = \\frac{\\mu_1}{\\varepsilon_1}\\left(1-\\sin^{2}{\\theta_{B\\parallel}}\\right) \\end{gather} \\] for non-magnetic medium \\(\\mu_1 = \\mu_2 = \\mu_0\\) \\[ \\begin{align} \\\\ \\implies \\sin^{2}{\\theta_{B\\parallel}} &= \\frac{\\displaystyle{ \\frac{\\mu_2}{\\varepsilon_2}-\\frac{\\mu_1}{\\varepsilon_1}}} {\\displaystyle { \\frac{\\mu_1\\varepsilon_1}{\\mu_2\\varepsilon_2}\\frac{\\mu_2}{\\varepsilon_2} -\\frac{\\mu_1}{\\varepsilon_1} }} \\\\\\\\ &= \\frac{\\displaystyle{ \\frac{\\mu_0}{\\varepsilon_2}-\\frac{\\mu_0}{\\varepsilon_1}}} {\\displaystyle { \\frac{\\varepsilon_1}{\\varepsilon_2}\\frac{\\mu_0}{\\varepsilon_2} -\\frac{\\mu_0}{\\varepsilon_1} }} \\\\\\\\ &= \\frac{\\displaystyle{ \\varepsilon_1- \\varepsilon_2}} {\\displaystyle { \\frac{\\varepsilon_1^{2}}{\\varepsilon_2}-\\varepsilon_2 }} \\\\\\\\ &= \\frac{\\displaystyle{ \\varepsilon_1- \\varepsilon_2}} {\\displaystyle { \\frac{1}{\\varepsilon_2}(\\varepsilon_1^{2}-\\varepsilon_2^{2}) }} \\\\\\\\ &= \\boxed{ \\frac{\\varepsilon_2} {\\displaystyle { \\varepsilon_1 + \\varepsilon_2 }}} \\end{align} \\] then, \\[ \\begin{gather} \\tan{\\theta_{B\\parallel}} = \\sqrt\\frac{\\varepsilon_2}{\\varepsilon_1} \\end{gather} \\] Normal Incidence \u00b6 Power \u00b6 incident power \\[ \\begin{gather} P_i = \\frac{1}{2}\\Re\\bigg[E_{i0}H_{i0}^{*}\\bigg] =\\frac{1}{2}\\frac{|{E_{i0}}|^{2}}{\\Re\\big[\\eta_1^{*}\\big]} \\end{gather} \\] reflected power \\[ \\begin{align} P_r &= -\\frac{1}{2}\\Re\\bigg[E_{r0}H_{r0}^{*}\\bigg] \\\\\\\\ &=-\\frac{1}{2}\\frac{|\\Gamma {E_{i0}}|^{2}}{\\Re\\big[\\eta_1^{*}\\big]} \\\\\\\\ &= - |\\Gamma|^{2}P_i \\end{align} \\] \\[ \\begin{gather} E_{ts} = E_{t0}e^{-\\gamma_2 z}=E_{i0}(1+\\Gamma)e^{-\\gamma_2 z} \\\\\\\\ H_{ts} = H_{t0}e^{-\\gamma_2 z} = \\frac{E_{t0}}{\\eta_2}e^{-\\gamma_2 z} = \\frac{E_{i0}(1+\\Gamma)}{\\eta_2}e^{-\\gamma_2 z} \\\\\\\\ = \\frac{H_{i0}\\,\\eta_1(1+\\Gamma)}{\\eta_2}e^{-\\gamma_2 z} = H_{i0}(1-\\Gamma)e^{-\\gamma_2 z} \\end{gather} \\] then, \\[ \\begin{align} P_t &= \\frac{1}{2}\\Re\\bigg[E_{t0}H_{t0}^{*}\\bigg] \\\\\\\\ &= P_i\\,(1+\\Gamma)(1-\\Gamma^{*}) \\\\\\\\ &\\overset{\\alpha=0}{=}P_i(1-|\\Gamma|^{2}) \\end{align} \\] Note compare to \\(\\text{equation (6)}\\) \\[ \\begin{align} P_i &\\leftrightarrow P^{+} \\\\\\\\ P_r &\\leftrightarrow P^{-} \\\\\\\\ P_t &\\leftrightarrow P \\\\\\\\ P_i+P_r=P_t &\\leftrightarrow P^{+}+P^{-}=P \\end{align} \\]","title":"EM Wave"},{"location":"Electromagnetics/EM_Wave/#em+wave+equations","text":"in free space by Maxwell's equations, we have \\[ \\begin{align} \\nabla \\times \\vec E &= - \\mu_0\\frac{\\partial \\vec H}{\\partial t}\\tag 1 \\\\\\\\ \\nabla \\times \\vec H &= \\varepsilon_0 \\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\end{align} \\] ^equ-1 \\[ \\begin{align} \\nabla^{2}\\vec E&=\\nabla\\left(\\nabla \\cdot \\vec E\\right)-\\nabla \\times \\nabla \\times \\vec E \\\\\\\\ &=0-\\left(-\\mu_0\\frac{\\partial }{\\partial t}\\left(\\nabla \\times \\vec H\\right)\\right) \\\\\\\\ &=\\mu_0\\varepsilon_0\\frac{\\partial^{2} \\vec E}{\\partial t^{2}} \\end{align} \\] and similar for \\(\\nabla ^{2}\\vec H\\) . We can now have the wave equations \\[ \\left\\{ \\begin{align} \\nabla ^{2}\\vec E&=\\mu_0\\varepsilon_0\\frac{\\partial^{2} \\vec E}{\\partial t^{2}} \\\\\\\\ \\nabla^{2}\\vec H &= \\mu_0\\varepsilon_0\\frac{\\partial^{2} \\vec H}{\\partial t^{2}} \\end{align}\\right. \\] compare to standard form of wave equation \\[ \\begin{gather} \\frac{\\partial ^{2}\\vec U}{\\partial t^{2}}=c^{2}\\,\\nabla ^{2} \\vec U \\end{gather} \\] we have \\[ \\begin{gather} c=\\frac{1}{\\sqrt{\\mu_0\\varepsilon_0}} \\end{gather} \\]","title":"EM Wave Equations"},{"location":"Electromagnetics/EM_Wave/#general+solutions+frequency+domain","text":"\\[ \\begin{gather} \\nabla ^{2}\\vec {E_s} - \\gamma^{2}\\vec {E_s} = 0 \\\\\\\\ \\nabla ^{2}\\vec {H_s} - \\gamma^{2}\\vec {H_s} = 0 \\end{gather} \\] in which \\(\\gamma\\) is propagation constant. \\[ \\begin{gather} \\gamma = \\sqrt{j\\omega\\mu(\\sigma + j\\omega \\varepsilon)} \\end{gather} \\] Thinking Faraday's Law \\[ \\begin{gather} \\nabla \\times \\vec{E_s} = j\\omega\\mu \\vec{H_s} \\end{gather} \\] Ampere's Law \\[ \\begin{gather} \\nabla \\times \\vec{H_s} = \\sigma \\vec {E_s} + j\\omega \\varepsilon\\vec{E_s} \\end{gather} \\]","title":"General Solutions (Frequency Domain)"},{"location":"Electromagnetics/EM_Wave/#general+solutions+transmission+line","text":"Take the wave propagates through \\(\\hat {a_z}\\) direction, then \\[ \\left\\{ \\begin{align} \\vec E= E_x^{+}(z-ut) \\, \\hat{a_x} + E^{-}_x(z+ut)\\,\\hat{a_x} \\\\\\\\ \\vec H= H_y^{+}(z-ut) \\, \\hat{a_y} + H^{-}_y(z+ut)\\,\\hat{a_y} \\end{align}\\right. \\] from \\(\\text{equation (a)}\\) we have \\[ \\begin{align} \\mu_0\\frac{\\partial \\vec H}{\\partial t} &= - \\nabla \\times \\vec E = -\\frac{\\partial \\vec E}{\\partial z} \\\\\\\\ &= -\\bigg({E_x^{+}}'(z-ut) \\, \\hat{a_x} + {E^{-}_x}'(z+ut)\\,\\hat{a_x}\\bigg) \\\\\\\\ \\implies \\vec H &= -\\frac{1}{\\mu_0}\\int\\left({E_x^{+}}'(z-ut) \\, \\hat{a_x} + {E^{-}_x}'(z+ut)\\,\\hat{a_x}\\right)\\,dt \\\\\\\\ &= \\frac{1}{u\\mu_0}\\bigg({E_x^{+}}(z-ut) \\, \\hat{a_x} - {E^{-}_x}(z+ut)\\,\\hat{a_x}\\bigg) \\\\\\\\ &= \\frac{1}{\\sqrt{\\mu_0/\\varepsilon_0}}\\bigg({E_x^{+}}(z-ut) \\, \\hat{a_x} - {E^{-}_x}(z+ut)\\,\\hat{a_x}\\bigg) \\end{align} \\] Thus we can obtain the wave impedance in free space \\(\\eta_0\\) \\[ \\begin{gather} \\eta_0 = \\sqrt\\frac{\\mu_0}{\\varepsilon_0} = \\frac{E_x^{+}}{H_x^{+}}=-\\frac{E_x^{-}}{H_x^{-}} \\end{gather} \\]","title":"General Solutions (Transmission Line)"},{"location":"Electromagnetics/EM_Wave/#power","text":"Poynting Vector \\[ \\begin{gather} \\vec P = \\vec E \\times \\vec H \\qquad \\text{W/m}^{2} \\end{gather} \\] Instantaneous power across \\(S\\) \\[ \\begin{gather} P(t)=\\int_S \\vec P \\cdot d\\vec S \\end{gather} \\] complex Poynting vector not a phasor, but come up by phasors trickily. notice that complex Poynting vector is not function of \\(t\\) . Since it is generated from phasors, it represents the concept of average. \\[ \\begin{gather} \\vec P_c=\\frac{1}{2}\\vec E_s\\times\\vec H_s^{*} =\\frac{1}{2}\\vec E_s^{*}\\times\\vec H_s \\\\\\\\ \\vec P_{ave}=\\text{Re}\\left[\\vec P_c\\right]\\qquad \\text{W/m}^{2} \\end{gather} \\]","title":"Power"},{"location":"Electromagnetics/EM_Wave/#loss+tangent","text":"for lossless medium, \\(\\sigma = 0\\) . propagation constant \\(\\gamma\\) \\[ \\begin{gather} \\gamma = \\sqrt{j\\omega \\mu(\\sigma + j\\omega\\varepsilon)} \\end{gather} \\] Loss Tangent \\[ \\begin{gather} \\tan\\theta = \\frac{J_{cs}}{J_{ds}} = \\frac{|\\sigma E_s|}{|j\\omega\\varepsilon E_s|} = \\frac{\\sigma}{\\omega \\varepsilon} \\end{gather} \\] Loss Angle \\[ \\begin{gather} \\theta = \\tan^{-1}{\\frac{\\sigma}{\\omega\\varepsilon}} \\end{gather} \\] Skin Depth \\[ \\begin{gather} \\delta = \\frac{1}{\\alpha} \\end{gather} \\]","title":"Loss Tangent"},{"location":"Electromagnetics/EM_Wave/#wave+polarization","text":"\\[ \\begin{gather} \\vec E = E_x \\,\\hat{a_x} + E_y \\,\\hat{a_y} \\\\\\\\ = E_{ox}\\cos{(\\omega t - \\beta z + \\phi_x)} \\, \\hat{a_x} + E_{oy}\\cos{(\\omega t - \\beta z + \\phi_y)} \\, \\hat{a_y} \\\\\\\\ \\Delta \\phi = \\phi_y - \\phi_x \\end{gather} \\]","title":"Wave Polarization"},{"location":"Electromagnetics/EM_Wave/#linear+polarization","text":"\\[ \\begin{align} \\vec E &= E_x \\,\\hat{a_x} + E_y \\,\\hat{a_y} \\\\\\\\ &= E_1 \\,\\hat{a_1} + E_2 \\,\\hat{a_2} \\, \\end{align} \\] \\(\\Delta \\phi = n\\pi, \\quad n\\in Z^{+}\\) \\[ \\begin{gather} \\frac{E_{y}}{E_{x}}= \\frac{E_{oy}\\cos{(\\omega t - \\beta z + \\phi_x + \\Delta \\phi)}}{E_{ox}\\cos{(\\omega t - \\beta z + \\phi_x)}} = \\frac{E_{oy}}{E_{ox}} \\end{gather} \\] no necessarily perpendicular","title":"Linear Polarization"},{"location":"Electromagnetics/EM_Wave/#circular+polarization","text":"\\(E_{ox} = E_{oy}\\) \\(\\Delta \\phi = (\\frac{1}{2} + n)\\pi\\) \\[ \\begin{gather} \\frac{E_{y}}{E_{x}}= \\frac{E_{oy}\\cos{(\\omega t - \\beta z + \\phi_x + \\Delta \\phi)}}{E_{ox}\\cos{(\\omega t - \\beta z + \\phi_x)}} = \\pm\\frac{E_{oy}}{E_{ox}} \\,\\tan{(\\omega t - \\beta z + \\phi_x)} \\end{gather} \\] thus, \\[ \\begin{gather} \\phi = \\tan^{-1}{\\frac{E_y}{E_x}}=\\pm(\\omega t - \\beta z + \\phi_x) \\implies \\text{circular} \\end{gather} \\] necessarily perpendicular","title":"Circular Polarization"},{"location":"Electromagnetics/EM_Wave/#elliptical+polarization","text":"if \\(\\Delta \\phi > 0\\) , \\(E_y\\) leads \\(E_x\\) , (left-handed) if \\(\\Delta \\phi < 0\\) , \\(E_y\\) lags \\(E_x\\) , (right-handed)","title":"Elliptical Polarization"},{"location":"Electromagnetics/EM_Wave/#reflection+and+transmission","text":"","title":"Reflection and Transmission"},{"location":"Electromagnetics/EM_Wave/#snells+law","text":"","title":"Snell's Law"},{"location":"Electromagnetics/EM_Wave/#lossless","text":"\\[ \\begin{gather} k_i \\sin{\\theta_i} = k_r \\sin{\\theta_r}=k_t\\sin{\\theta_t} \\end{gather} \\]","title":"lossless"},{"location":"Electromagnetics/EM_Wave/#lossy","text":"\\[ \\begin{gather} \\gamma_i \\sin{\\theta_i} = \\gamma_r \\sin{\\theta_r}=\\gamma_t\\sin{\\theta_t} \\end{gather} \\]","title":"lossy"},{"location":"Electromagnetics/EM_Wave/#true+angle+of+refraction","text":"Since \\(\\gamma\\) is complex, \\(\\sin \\theta_t\\) and \\(\\sin \\theta_r\\) is basically complex as well. Thus \\[ \\begin{align} \\vec \\gamma_t \\cdot \\vec r &= \\gamma_t \\sin{\\theta_t}\\,x + \\gamma_t \\cos{\\theta_t}\\,z \\\\\\\\ &= \\Re\\big[ \\gamma_t \\sin{\\theta_t}\\big]x + \\Re\\big[ \\gamma_t \\cos{\\theta_t}\\big]z + j\\bigg[ \\Im\\big[ \\gamma_t \\sin{\\theta_t}\\big]x + \\Im\\big[ \\gamma_t \\cos{\\theta_t}\\big]z \\bigg] \\end{align} \\] True angle of refraction \\(\\psi_\\beta\\) \\[ \\begin{gather} \\psi_\\beta = \\tan^{-1}{\\frac{\\Im\\big[\\gamma_t\\sin\\theta_t\\big]}{\\Im\\big[\\gamma_t\\cos\\theta_t\\big]}} \\end{gather} \\]","title":"True Angle of Refraction"},{"location":"Electromagnetics/EM_Wave/#polarization","text":"","title":"Polarization"},{"location":"Electromagnetics/EM_Wave/#parallel+polarization","text":"\\(E\\) is parallel to surface formed by ( \\(k_i, k_r, k_t\\) ) at \\(z=0\\) , solve, \\[ \\left\\{ \\begin{align} &E_{i0}\\cos{\\theta_i} + E_{r0}\\cos{\\theta_r} = E_{t0}\\cos{\\theta_t} \\\\\\\\ &H_{i0} - H_{r0} = H_{t0} \\end{align}\\right. \\] then we have \\[ \\begin{gather} \\Gamma_{\\parallel}=\\frac{E_{r0}}{E_{i0}}=\\frac{\\eta_2\\cos{\\theta_t}-\\eta_1\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_t}+\\eta_1\\cos{\\theta_i}} \\\\\\\\ \\tau_{\\parallel}=\\frac{E_{t0}}{E_{i0}}=\\frac{2\\eta_2\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_t}+\\eta_1\\cos{\\theta_i}} \\end{gather} \\]","title":"Parallel Polarization"},{"location":"Electromagnetics/EM_Wave/#perpendicular+polarization","text":"at \\(z=0\\) , solve, \\[ \\left\\{ \\begin{align} &-H_{i0}\\cos{\\theta_i} + H_{r0}\\cos{\\theta_r} = -H_{t0}\\cos{\\theta_t} \\\\\\\\ &E_{i0} + E_{r0} = E_{t0} \\end{align}\\right. \\] then we have \\[ \\begin{gather} \\Gamma_{\\bot}=\\frac{E_{r0}}{E_{i0}}=\\frac{\\eta_2\\cos{\\theta_i}-\\eta_1\\cos{\\theta_t}}{\\eta_2\\cos{\\theta_i}+\\eta_1\\cos{\\theta_t}} \\\\\\\\ \\tau_{\\bot}=\\frac{E_{t0}}{E_{i0}}=\\frac{2\\eta_2\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_i}+\\eta_1\\cos{\\theta_t}} \\end{gather} \\]","title":"Perpendicular Polarization"},{"location":"Electromagnetics/EM_Wave/#brewster+angle","text":"Denote as \\(\\theta_{B\\parallel}\\) a special \\(\\theta_i\\) that make no reflection. For nonmagnetic medium ( \\(\\mu_r = 1\\) ), only exists at parallel polarization . \\[ \\begin{gather} \\Gamma_{\\parallel} = \\frac{\\eta_2\\cos{\\theta_t}- \\eta_1\\cos{\\theta_i}}{\\eta_2\\cos{\\theta_t}+ \\eta_1\\cos{\\theta_i}} = 0 \\\\\\\\ \\implies \\eta_2 \\cos{\\theta_t} = \\eta_1\\cos{\\theta_{B\\parallel}} \\end{gather} \\] \\[ \\begin{gather} \\\\ \\eta_2^{2} \\cos^{2}{\\theta_t} = \\eta^{2}_1 \\cos^{2}{\\theta_{B\\parallel}} \\\\\\\\ \\frac{\\mu_2}{\\varepsilon_2}\\left(1-\\frac{\\mu_1\\varepsilon_1}{\\mu_2\\varepsilon_2}\\sin^{2}{\\theta_{B\\parallel}}\\right) = \\frac{\\mu_1}{\\varepsilon_1}\\left(1-\\sin^{2}{\\theta_{B\\parallel}}\\right) \\end{gather} \\] for non-magnetic medium \\(\\mu_1 = \\mu_2 = \\mu_0\\) \\[ \\begin{align} \\\\ \\implies \\sin^{2}{\\theta_{B\\parallel}} &= \\frac{\\displaystyle{ \\frac{\\mu_2}{\\varepsilon_2}-\\frac{\\mu_1}{\\varepsilon_1}}} {\\displaystyle { \\frac{\\mu_1\\varepsilon_1}{\\mu_2\\varepsilon_2}\\frac{\\mu_2}{\\varepsilon_2} -\\frac{\\mu_1}{\\varepsilon_1} }} \\\\\\\\ &= \\frac{\\displaystyle{ \\frac{\\mu_0}{\\varepsilon_2}-\\frac{\\mu_0}{\\varepsilon_1}}} {\\displaystyle { \\frac{\\varepsilon_1}{\\varepsilon_2}\\frac{\\mu_0}{\\varepsilon_2} -\\frac{\\mu_0}{\\varepsilon_1} }} \\\\\\\\ &= \\frac{\\displaystyle{ \\varepsilon_1- \\varepsilon_2}} {\\displaystyle { \\frac{\\varepsilon_1^{2}}{\\varepsilon_2}-\\varepsilon_2 }} \\\\\\\\ &= \\frac{\\displaystyle{ \\varepsilon_1- \\varepsilon_2}} {\\displaystyle { \\frac{1}{\\varepsilon_2}(\\varepsilon_1^{2}-\\varepsilon_2^{2}) }} \\\\\\\\ &= \\boxed{ \\frac{\\varepsilon_2} {\\displaystyle { \\varepsilon_1 + \\varepsilon_2 }}} \\end{align} \\] then, \\[ \\begin{gather} \\tan{\\theta_{B\\parallel}} = \\sqrt\\frac{\\varepsilon_2}{\\varepsilon_1} \\end{gather} \\]","title":"Brewster Angle"},{"location":"Electromagnetics/EM_Wave/#normal+incidence","text":"","title":"Normal Incidence"},{"location":"Electromagnetics/EM_Wave/#power_1","text":"incident power \\[ \\begin{gather} P_i = \\frac{1}{2}\\Re\\bigg[E_{i0}H_{i0}^{*}\\bigg] =\\frac{1}{2}\\frac{|{E_{i0}}|^{2}}{\\Re\\big[\\eta_1^{*}\\big]} \\end{gather} \\] reflected power \\[ \\begin{align} P_r &= -\\frac{1}{2}\\Re\\bigg[E_{r0}H_{r0}^{*}\\bigg] \\\\\\\\ &=-\\frac{1}{2}\\frac{|\\Gamma {E_{i0}}|^{2}}{\\Re\\big[\\eta_1^{*}\\big]} \\\\\\\\ &= - |\\Gamma|^{2}P_i \\end{align} \\] \\[ \\begin{gather} E_{ts} = E_{t0}e^{-\\gamma_2 z}=E_{i0}(1+\\Gamma)e^{-\\gamma_2 z} \\\\\\\\ H_{ts} = H_{t0}e^{-\\gamma_2 z} = \\frac{E_{t0}}{\\eta_2}e^{-\\gamma_2 z} = \\frac{E_{i0}(1+\\Gamma)}{\\eta_2}e^{-\\gamma_2 z} \\\\\\\\ = \\frac{H_{i0}\\,\\eta_1(1+\\Gamma)}{\\eta_2}e^{-\\gamma_2 z} = H_{i0}(1-\\Gamma)e^{-\\gamma_2 z} \\end{gather} \\] then, \\[ \\begin{align} P_t &= \\frac{1}{2}\\Re\\bigg[E_{t0}H_{t0}^{*}\\bigg] \\\\\\\\ &= P_i\\,(1+\\Gamma)(1-\\Gamma^{*}) \\\\\\\\ &\\overset{\\alpha=0}{=}P_i(1-|\\Gamma|^{2}) \\end{align} \\] Note compare to \\(\\text{equation (6)}\\) \\[ \\begin{align} P_i &\\leftrightarrow P^{+} \\\\\\\\ P_r &\\leftrightarrow P^{-} \\\\\\\\ P_t &\\leftrightarrow P \\\\\\\\ P_i+P_r=P_t &\\leftrightarrow P^{+}+P^{-}=P \\end{align} \\]","title":"Power"},{"location":"Electromagnetics/Electrostatics/","text":"Gauss's Law \u00b6 \\[ \\begin{gather} \\Phi = Q_{enc} \\\\\\\\ \\Phi = \\oint_S{\\vec D \\ \\cdot\\ d\\vec S} = \\int_v{ \\nabla \\cdot \\vec D\\ dv} = \\int_v{\\rho_v \\ dv} \\\\\\\\ \\nabla \\cdot \\vec D = \\rho_v \\end{gather} \\] Electric Field \u00b6 Electric dipole \u00b6 Dipole moment: \\(\\vec p\\) \\[ \\begin{gather} \\vec p = Q\\vec d \\\\\\\\ V = \\frac{Q}{4\\pi \\varepsilon}\\cdot \\frac{d \\cos\\theta}{r^{2}} = \\frac{\\vec p \\cdot \\hat {a_r}}{4\\pi \\varepsilon r^{2}}= \\frac{\\vec p \\cdot \\vec {r}}{4\\pi \\varepsilon r^{3}} \\\\\\\\ \\end{gather} \\] note that \\(\\hat{a_r}\\) is unit vector. \\[ \\begin{align} \\vec E &= -\\nabla V \\\\\\\\ &= - \\left[\\frac{\\partial V}{\\partial r} \\hat r \\ +\\frac{1}{r}\\frac{\\partial V}{\\partial \\theta}\\hat \\theta \\ \\right] \\\\\\\\ &= \\frac{p}{4\\pi \\varepsilon r^3}(2\\cos \\theta \\ \\hat r\\ + \\sin\\theta\\ \\hat \\theta\\ ) \\end{align} \\] Polarization \u00b6 Polarization vector: \\(\\vec P\\) \\[ \\begin{gather} \\vec P \\equiv \\chi_e \\ \\varepsilon_0\\ \\vec E \\end{gather} \\] surface bound charge density: \\(\\rho_{ps}\\) the direction \\(\\hat {a_n}\\) is outgoing the volume which contain charge \\[ \\begin{gather} \\rho_{ps} = \\vec P \\cdot \\vec{a_n} \\end{gather} \\] volume bound charge density: \\(\\rho_{pv}\\) \\[ \\begin{gather} \\rho_{pv} = -\\nabla \\cdot \\vec P \\end{gather} \\] Electric flux density field \\(D\\) \\[ \\begin{align} \\rho_v &= \\rho_{\\text{total}}-\\rho_{pv} \\\\\\\\ \\implies \\nabla \\cdot \\vec D &= \\nabla \\cdot \\left(\\varepsilon_0\\vec E \\right) - (- \\nabla \\cdot \\vec P) \\\\\\\\ \\implies \\vec D &= \\varepsilon_0\\ \\vec E + \\vec P \\\\\\\\ &= \\varepsilon_0 (1 + \\chi_e) \\vec E \\\\\\\\ &= \\varepsilon_r\\ \\varepsilon_0\\ \\vec E \\\\\\\\ &= \\varepsilon\\ \\vec E \\end{align} \\] Electric Potential \u00b6 \\[ \\begin{align} V_{AB} &= -\\int_A^{B}{\\vec E \\ \\cdot\\ d\\vec l}=V_B - V_A \\\\\\\\ &= \\int_A^{B}{dV}=\\int_A^{B}{ \\vec\\nabla V \\cdot d \\vec l} \\end{align} \\] implies, \\[ \\begin{gather} \\vec E = - \\nabla V \\\\\\\\ \\end{gather} \\] by Stokes' Theorem \\[ \\begin{gather} \\oint_L{\\vec E \\cdot d \\vec l = \\int_S (\\nabla \\times \\vec E)\\cdot d \\vec S} = 0 \\\\\\\\ \\implies \\nabla \\times \\vec E =0 \\end{gather} \\] Electric Energy \u00b6 \\(W_E\\) : Energy stored in the capacitor discrete charges \\[ \\begin{gather} W_E = \\frac{1}{2}\\sum{Q_k V_k} \\end{gather} \\] continuous charges \\[ \\begin{align} W_E &= \\int_v{\\rho_v V(\\vec r) dv} \\\\\\\\ &=\\frac{1}{2}\\int_v{(\\nabla \\cdot \\vec D)V \\ dv} = \\frac{1}{2}\\left(\\int_v{\\nabla \\cdot (\\vec D \\,V)\\ dv}-\\int_v{\\vec D \\cdot (\\nabla V)\\ dv} \\right) \\\\\\\\ &= \\frac{1}{2}\\left(\\oint_S{V\\vec D\\ \\cdot d\\vec S} + \\int_v{\\vec D \\cdot \\vec E\\ dv}\\right) \\end{align} \\] consider \\(S = U\\) (Universe), then \\[ \\begin{gather} \\oint_S{V\\vec D\\ \\cdot d\\vec S} \\to 0 \\\\\\\\ \\implies W_E = \\frac{1}{2}\\int_U{\\vec D\\cdot \\vec E \\ dv} \\end{gather} \\] Conduction Current \u00b6 Definition: Charge flows due to \\(\\vec E\\) force on electron \\(F_e\\) \\[ \\begin{gather} \\vec F_e = -e\\vec E \\end{gather} \\] friction \\(F_r\\) \\[ \\begin{gather} \\vec F_r = \\frac{\\Delta p}{\\Delta t} =- \\frac{m\\vec u}{\\tau} \\end{gather} \\] \\[ \\begin{gather} \\vec F_e + \\vec F_r = 0 \\\\\\\\ \\implies \\frac{m\\vec u}{\\tau} = -e \\vec E \\end{gather} \\] mean free time \\(\\tau\\) average drift velocity \\(\\vec u\\) \\[ \\begin{gather} \\vec u = - \\frac{e\\tau}{m}\\vec E = -\\mu_e \\vec E \\end{gather} \\] Current Density \u00b6 current view: \\[ \\begin{gather} \\vec J = \\frac{\\partial I}{\\partial \\vec S} \\end{gather} \\] convection current view: \\[ \\begin{gather} \\vec J = \\rho_v \\vec u=(-ne)\\left(-\\frac{e\\tau}{m}\\right)\\vec E \\end{gather} \\] Ohm's law view: \\[ \\begin{gather} \\because I = YV \\\\\\\\ \\vec J = \\sigma \\vec E \\end{gather} \\] conductivity \\(\\sigma\\) Continuity Equation \u00b6 \\[ \\begin{gather} I_{net} = -\\frac{d}{dt}Q = -\\frac{d}{dt}\\int_v{\\rho_v\\, dv} \\\\\\\\ I_{net} = \\oint_S{\\vec J \\cdot \\vec S} = \\int_v{\\nabla \\cdot \\vec J\\, dv} \\\\\\\\ \\implies \\nabla \\cdot \\vec J = - \\frac{d}{dt}\\rho_v \\end{gather} \\] note collapse: true Recall that the physical meaning of divergence, \\[ \\begin{gather} \\nabla \\cdot \\vec A 0 & \\text{(source)} \\\\\\\\ \\nabla \\cdot \\vec A < 0 & \\text{(sink)} \\end{gather} \\] It is reasonable to write \\(\\mathbf{-}\\rho_v'\\) for a charge source. Also intuitive for steady current have \\[ \\begin{gather} \\nabla \\cdot \\vec J = 0 \\end{gather} \\] Relaxation Time \u00b6 relaxation time \\(\\tau\\) view from solving differential equation \\[ \\begin{gather} \\nabla \\cdot \\vec J = -\\frac{d}{dt}\\rho_v = \\nabla \\cdot (\\sigma \\vec E) = \\frac{\\sigma}{\\varepsilon}\\nabla \\cdot \\vec D = \\frac{\\sigma}{\\varepsilon}\\rho_v \\\\\\\\ -\\frac{\\sigma}{\\varepsilon} dt = \\frac{d\\rho_v}{\\rho_v} \\\\\\\\ \\implies \\rho_v = \\rho_{o}\\exp\\left(-\\frac{\\sigma}{\\varepsilon}t\\right) = \\rho_o e^{-t/\\tau} \\\\\\\\ \\implies \\tau = \\frac{\\varepsilon}{\\sigma} \\end{gather} \\] view from having \\(\\tau = RC\\) \\[ \\begin{gather} \\tau = RC = \\frac{V}{I}\\frac{Q}{V}=\\frac{\\int{\\varepsilon\\vec E \\cdot d\\vec S}}{\\int{\\sigma\\vec E\\cdot d\\vec S}}=\\frac{\\varepsilon}{\\sigma} \\end{gather} \\] Boundary Condition \u00b6 for there is no surface charge density \\[ \\begin{gather} \\nabla \\cdot (\\rho_S \\,\\vec{a_n})=0 \\\\\\\\ \\implies \\nabla \\cdot \\vec D=0 \\\\\\\\ \\vec J=\\sigma\\vec E \\\\\\\\ \\implies \\vec\\nabla \\cdot \\vec J=0 \\end{gather} \\] implies \\[ \\begin{gather} D_{1n} = D_{2n} \\\\\\\\ J_{1n}=J_{2n} \\end{gather} \\] tangent direction for the closed-loop in conservative field, we have \\[ \\begin{gather} \\because \\oint_c{\\vec E \\cdot d\\vec l}=0 \\end{gather} \\] then \\[ \\begin{gather} \\vec E_1 = \\vec E_2 \\\\\\\\ E_{1n} = E_{2n} =0 \\\\\\\\ \\implies E_{1t} = E_{2t} \\end{gather} \\] for \\(\\vec J = \\sigma \\vec E\\) \\[ \\begin{gather} \\frac{J_{1t}}{\\sigma_1}= \\frac{J_{2t}}{\\sigma_2} \\end{gather} \\]","title":"Electrostatics"},{"location":"Electromagnetics/Electrostatics/#gausss+law","text":"\\[ \\begin{gather} \\Phi = Q_{enc} \\\\\\\\ \\Phi = \\oint_S{\\vec D \\ \\cdot\\ d\\vec S} = \\int_v{ \\nabla \\cdot \\vec D\\ dv} = \\int_v{\\rho_v \\ dv} \\\\\\\\ \\nabla \\cdot \\vec D = \\rho_v \\end{gather} \\]","title":"Gauss's Law"},{"location":"Electromagnetics/Electrostatics/#electric+field","text":"","title":"Electric Field"},{"location":"Electromagnetics/Electrostatics/#electric+dipole","text":"Dipole moment: \\(\\vec p\\) \\[ \\begin{gather} \\vec p = Q\\vec d \\\\\\\\ V = \\frac{Q}{4\\pi \\varepsilon}\\cdot \\frac{d \\cos\\theta}{r^{2}} = \\frac{\\vec p \\cdot \\hat {a_r}}{4\\pi \\varepsilon r^{2}}= \\frac{\\vec p \\cdot \\vec {r}}{4\\pi \\varepsilon r^{3}} \\\\\\\\ \\end{gather} \\] note that \\(\\hat{a_r}\\) is unit vector. \\[ \\begin{align} \\vec E &= -\\nabla V \\\\\\\\ &= - \\left[\\frac{\\partial V}{\\partial r} \\hat r \\ +\\frac{1}{r}\\frac{\\partial V}{\\partial \\theta}\\hat \\theta \\ \\right] \\\\\\\\ &= \\frac{p}{4\\pi \\varepsilon r^3}(2\\cos \\theta \\ \\hat r\\ + \\sin\\theta\\ \\hat \\theta\\ ) \\end{align} \\]","title":"Electric dipole"},{"location":"Electromagnetics/Electrostatics/#polarization","text":"Polarization vector: \\(\\vec P\\) \\[ \\begin{gather} \\vec P \\equiv \\chi_e \\ \\varepsilon_0\\ \\vec E \\end{gather} \\] surface bound charge density: \\(\\rho_{ps}\\) the direction \\(\\hat {a_n}\\) is outgoing the volume which contain charge \\[ \\begin{gather} \\rho_{ps} = \\vec P \\cdot \\vec{a_n} \\end{gather} \\] volume bound charge density: \\(\\rho_{pv}\\) \\[ \\begin{gather} \\rho_{pv} = -\\nabla \\cdot \\vec P \\end{gather} \\] Electric flux density field \\(D\\) \\[ \\begin{align} \\rho_v &= \\rho_{\\text{total}}-\\rho_{pv} \\\\\\\\ \\implies \\nabla \\cdot \\vec D &= \\nabla \\cdot \\left(\\varepsilon_0\\vec E \\right) - (- \\nabla \\cdot \\vec P) \\\\\\\\ \\implies \\vec D &= \\varepsilon_0\\ \\vec E + \\vec P \\\\\\\\ &= \\varepsilon_0 (1 + \\chi_e) \\vec E \\\\\\\\ &= \\varepsilon_r\\ \\varepsilon_0\\ \\vec E \\\\\\\\ &= \\varepsilon\\ \\vec E \\end{align} \\]","title":"Polarization"},{"location":"Electromagnetics/Electrostatics/#electric+potential","text":"\\[ \\begin{align} V_{AB} &= -\\int_A^{B}{\\vec E \\ \\cdot\\ d\\vec l}=V_B - V_A \\\\\\\\ &= \\int_A^{B}{dV}=\\int_A^{B}{ \\vec\\nabla V \\cdot d \\vec l} \\end{align} \\] implies, \\[ \\begin{gather} \\vec E = - \\nabla V \\\\\\\\ \\end{gather} \\] by Stokes' Theorem \\[ \\begin{gather} \\oint_L{\\vec E \\cdot d \\vec l = \\int_S (\\nabla \\times \\vec E)\\cdot d \\vec S} = 0 \\\\\\\\ \\implies \\nabla \\times \\vec E =0 \\end{gather} \\]","title":"Electric Potential"},{"location":"Electromagnetics/Electrostatics/#electric+energy","text":"\\(W_E\\) : Energy stored in the capacitor discrete charges \\[ \\begin{gather} W_E = \\frac{1}{2}\\sum{Q_k V_k} \\end{gather} \\] continuous charges \\[ \\begin{align} W_E &= \\int_v{\\rho_v V(\\vec r) dv} \\\\\\\\ &=\\frac{1}{2}\\int_v{(\\nabla \\cdot \\vec D)V \\ dv} = \\frac{1}{2}\\left(\\int_v{\\nabla \\cdot (\\vec D \\,V)\\ dv}-\\int_v{\\vec D \\cdot (\\nabla V)\\ dv} \\right) \\\\\\\\ &= \\frac{1}{2}\\left(\\oint_S{V\\vec D\\ \\cdot d\\vec S} + \\int_v{\\vec D \\cdot \\vec E\\ dv}\\right) \\end{align} \\] consider \\(S = U\\) (Universe), then \\[ \\begin{gather} \\oint_S{V\\vec D\\ \\cdot d\\vec S} \\to 0 \\\\\\\\ \\implies W_E = \\frac{1}{2}\\int_U{\\vec D\\cdot \\vec E \\ dv} \\end{gather} \\]","title":"Electric Energy"},{"location":"Electromagnetics/Electrostatics/#conduction+current","text":"Definition: Charge flows due to \\(\\vec E\\) force on electron \\(F_e\\) \\[ \\begin{gather} \\vec F_e = -e\\vec E \\end{gather} \\] friction \\(F_r\\) \\[ \\begin{gather} \\vec F_r = \\frac{\\Delta p}{\\Delta t} =- \\frac{m\\vec u}{\\tau} \\end{gather} \\] \\[ \\begin{gather} \\vec F_e + \\vec F_r = 0 \\\\\\\\ \\implies \\frac{m\\vec u}{\\tau} = -e \\vec E \\end{gather} \\] mean free time \\(\\tau\\) average drift velocity \\(\\vec u\\) \\[ \\begin{gather} \\vec u = - \\frac{e\\tau}{m}\\vec E = -\\mu_e \\vec E \\end{gather} \\]","title":"Conduction Current"},{"location":"Electromagnetics/Electrostatics/#current+density","text":"current view: \\[ \\begin{gather} \\vec J = \\frac{\\partial I}{\\partial \\vec S} \\end{gather} \\] convection current view: \\[ \\begin{gather} \\vec J = \\rho_v \\vec u=(-ne)\\left(-\\frac{e\\tau}{m}\\right)\\vec E \\end{gather} \\] Ohm's law view: \\[ \\begin{gather} \\because I = YV \\\\\\\\ \\vec J = \\sigma \\vec E \\end{gather} \\] conductivity \\(\\sigma\\)","title":"Current Density"},{"location":"Electromagnetics/Electrostatics/#continuity+equation","text":"\\[ \\begin{gather} I_{net} = -\\frac{d}{dt}Q = -\\frac{d}{dt}\\int_v{\\rho_v\\, dv} \\\\\\\\ I_{net} = \\oint_S{\\vec J \\cdot \\vec S} = \\int_v{\\nabla \\cdot \\vec J\\, dv} \\\\\\\\ \\implies \\nabla \\cdot \\vec J = - \\frac{d}{dt}\\rho_v \\end{gather} \\] note collapse: true Recall that the physical meaning of divergence, \\[ \\begin{gather} \\nabla \\cdot \\vec A 0 & \\text{(source)} \\\\\\\\ \\nabla \\cdot \\vec A < 0 & \\text{(sink)} \\end{gather} \\] It is reasonable to write \\(\\mathbf{-}\\rho_v'\\) for a charge source. Also intuitive for steady current have \\[ \\begin{gather} \\nabla \\cdot \\vec J = 0 \\end{gather} \\]","title":"Continuity Equation"},{"location":"Electromagnetics/Electrostatics/#relaxation+time","text":"relaxation time \\(\\tau\\) view from solving differential equation \\[ \\begin{gather} \\nabla \\cdot \\vec J = -\\frac{d}{dt}\\rho_v = \\nabla \\cdot (\\sigma \\vec E) = \\frac{\\sigma}{\\varepsilon}\\nabla \\cdot \\vec D = \\frac{\\sigma}{\\varepsilon}\\rho_v \\\\\\\\ -\\frac{\\sigma}{\\varepsilon} dt = \\frac{d\\rho_v}{\\rho_v} \\\\\\\\ \\implies \\rho_v = \\rho_{o}\\exp\\left(-\\frac{\\sigma}{\\varepsilon}t\\right) = \\rho_o e^{-t/\\tau} \\\\\\\\ \\implies \\tau = \\frac{\\varepsilon}{\\sigma} \\end{gather} \\] view from having \\(\\tau = RC\\) \\[ \\begin{gather} \\tau = RC = \\frac{V}{I}\\frac{Q}{V}=\\frac{\\int{\\varepsilon\\vec E \\cdot d\\vec S}}{\\int{\\sigma\\vec E\\cdot d\\vec S}}=\\frac{\\varepsilon}{\\sigma} \\end{gather} \\]","title":"Relaxation Time"},{"location":"Electromagnetics/Electrostatics/#boundary+condition","text":"for there is no surface charge density \\[ \\begin{gather} \\nabla \\cdot (\\rho_S \\,\\vec{a_n})=0 \\\\\\\\ \\implies \\nabla \\cdot \\vec D=0 \\\\\\\\ \\vec J=\\sigma\\vec E \\\\\\\\ \\implies \\vec\\nabla \\cdot \\vec J=0 \\end{gather} \\] implies \\[ \\begin{gather} D_{1n} = D_{2n} \\\\\\\\ J_{1n}=J_{2n} \\end{gather} \\] tangent direction for the closed-loop in conservative field, we have \\[ \\begin{gather} \\because \\oint_c{\\vec E \\cdot d\\vec l}=0 \\end{gather} \\] then \\[ \\begin{gather} \\vec E_1 = \\vec E_2 \\\\\\\\ E_{1n} = E_{2n} =0 \\\\\\\\ \\implies E_{1t} = E_{2t} \\end{gather} \\] for \\(\\vec J = \\sigma \\vec E\\) \\[ \\begin{gather} \\frac{J_{1t}}{\\sigma_1}= \\frac{J_{2t}}{\\sigma_2} \\end{gather} \\]","title":"Boundary Condition"},{"location":"Electromagnetics/Magnetostatics/","text":"Biot-Savart's Law \u00b6 Sources \\[ \\begin{gather} \\vec udQ = \\frac{d\\vec \u2113}{dt}Q = Id\\vec \u2113 \\end{gather} \\] Magnetic flux density: \\[ \\begin{gather} d\\vec B = \\frac{\\mu_o}{4\\pi}\\frac{Id\\vec \u2113 \\times \\hat{a_r}}{R^{2}} \\end{gather} \\] Magnetic field density: \\[ \\begin{gather} d\\vec H = \\frac{d\\vec B}{\\mu_o}= \\frac{1}{4\\pi}\\frac{Id\\vec \u2113 \\times \\hat{a_r}}{R^{2}} \\end{gather} \\] Magnetic force \\[ \\begin{align} d\\vec F &= Id\\vec \u2113 \\times d\\vec B \\\\\\\\ &= Id\\vec \u2113 \\times \\left(\\frac{\\mu_o}{4\\pi}\\frac{Id\\vec \u2113 \\times \\hat{a_r}}{R^{2}}\\right) \\end{align} \\] Comparison \\[ \\begin{gather} d\\vec F= q \\vec E \\end{gather} \\] Ampere's Law \u00b6 original Ampere's law \ud83d\udc4e \\[ \\begin{gather} \\oint_C{\\vec H \\cdot d\\vec \u2113}=\\int_S{\\vec J\\cdot d\\vec S}=I_{enc} \\\\\\\\ \\nabla \\times \\vec H=\\vec J \\end{gather} \\] Ampere-Maxwell equation \ud83d\udc4d \\[ \\begin{gather} \\oint_C{\\vec H \\cdot d \\vec \u2113} = \\int_S{\\left(\\vec J_f+\\frac{\\partial \\vec D}{\\partial t}\\right)\\cdot d \\vec S} \\\\\\\\ \\nabla \\times \\vec H =\\vec J_f + \\frac{\\partial \\vec D}{\\partial t} \\end{gather} \\] displacement current for the identity \\[ \\begin{gather} \\nabla \\cdot (\\nabla \\times \\vec H) =0 \\\\\\\\ \\nabla \\cdot (\\nabla \\times \\vec H) = \\nabla \\cdot \\vec J \\neq 0 \\quad\\text{ when}\\quad\\nabla \\cdot \\vec J=-\\frac{\\partial \\rho_v}{\\partial t} \\neq 0 \\end{gather} \\] thus, let's write \\[ \\begin{gather} \\nabla \\times \\vec H = \\vec J +\\vec J_d \\end{gather} \\] then \\[ \\begin{gather} \\nabla \\cdot (\\nabla \\times \\vec H) = \\nabla \\cdot \\vec J + \\nabla \\cdot \\vec J_d= 0 \\\\\\\\ \\implies \\nabla \\cdot \\vec J_d = \\frac{\\partial \\rho_v}{\\partial t} = \\frac{\\partial }{\\partial t}\\left(\\nabla \\cdot \\vec D\\right) \\\\\\\\ \\implies\\vec J_d= \\frac{\\partial \\vec D}{\\partial t} \\end{gather} \\] displacement current \\(I_d\\) <br \\(\\vec J_d\\) is called displacement current density, and the displacement current \\(I_d\\) id defined by <br \\[ \\begin{gather} I_d = \\int_S{\\vec J_d \\cdot d\\vec S} \\end{gather} \\] for \\(I_d \\neq 0\\) whenever there is an accumulation of charges, or the imginary current inside capacitor. <br e.g. semi-infinite conductive wire. Gauss' Law \u00b6 \\[ \\begin{gather} \\oint_S{\\vec B \\cdot d \\vec S} = \\int_V{\\nabla \\cdot \\vec B}=0 \\end{gather} \\] For it exists not isolated magnetic poles, Guass' law here in magnetism is pretty intuitive. Faraday's law \u00b6 static electric field \\(E_e\\) transformer-induced electric field \\(E_T\\) motion-induced electric field \\(E_m\\) induced electric field \\(E_{ind} \\equiv E_T +E_m\\) \\[ \\begin{align} V_{emf}&=\\oint_L (E_e+E_T+E_m) \\\\\\\\ &=0+\\left(-\\int_S{\\frac{\\partial \\vec B}{\\partial t}\\cdot d\\vec S}\\right)+\\oint_L{\\left(\\vec u \\times \\vec B\\right)\\cdot d\\vec \u2113} \\\\\\\\ &=-\\frac{d}{dt}\\int{\\vec B \\cdot d\\vec S} \\end{align} \\] differential form \\[ \\begin{gather} \\nabla \\times \\vec E=-\\frac{\\partial \\vec B}{\\partial t}+\\nabla \\times \\left(\\vec u \\times \\vec B\\right) \\\\\\\\ \\iff \\nabla \\times \\left(\\vec E- \\vec u \\times \\vec B\\right)=-\\frac{\\partial \\vec B}{\\partial t} \\end{gather} \\] Magnetic Potentials \u00b6 Magnetic Scalar Potential \u00b6 magnetic scalar potential \\(V_m\\) (in \\(A\\) ) Ignoring displacement current \\(\\vec J_d\\) , if \\(\\vec J = 0\\) , then \\[ \\begin{gather} \\nabla \\times \\vec H = 0 \\\\\\\\ \\vec H = - \\nabla V_m \\end{gather} \\] comparison \\[ \\begin{gather} \\frac{1}{\\varepsilon}\\vec D = - \\nabla V \\end{gather} \\] Magnetic Vector Potential \u00b6 magnetic vector potential \\(\\vec A\\) (Wb/m) since \\[ \\begin{gather} \\nabla \\cdot \\vec B = 0 \\end{gather} \\] and recall the identity \\[ \\begin{gather} \\nabla \\cdot \\nabla \\times \\vec A = 0 \\end{gather} \\] then we can write \\[ \\begin{gather} \\vec B = \\nabla \\times \\vec A \\end{gather} \\] by Stokes' theorem \\[ \\begin{gather} \\psi = \\int_S{\\vec B \\cdot d \\vec S}=\\int_S{(\\nabla \\times \\vec A)\\cdot d\\vec S}=\\oint_L{\\vec A\\cdot d \\vec \u2113} \\end{gather} \\] and just remember that \\[ \\begin{gather} \\vec A = \\int_Q{\\frac{\\mu}{4\\pi}\\frac{\\vec udq}{R}} \\end{gather} \\] line current \\[ \\begin{gather} \\vec udq = I d\\vec \u2113 \\end{gather} \\] surface current \\[ \\begin{gather} \\vec udq = \\vec K dS \\end{gather} \\] volume current \\[ \\begin{gather} \\vec udq = \\vec J dv \\end{gather} \\] Comparison \\[ \\begin{gather} V=\\int_Q{\\frac{1}{4\\pi \\varepsilon}\\frac{dq}{R}} \\end{gather} \\] Time-Varying Potentials \u00b6 Easy but tedious derivation \\(\\nabla \\times \\vec E\\) \\[ \\begin{gather} \\nabla \\times \\vec E = -\\frac{\\partial \\left(\\nabla \\times \\vec A\\right)}{\\partial t}=-\\nabla \\times \\frac{\\partial \\vec A}{\\partial t} \\\\\\\\ \\implies \\nabla \\times \\left(\\vec E + \\frac{\\partial \\vec A }{\\partial t}\\right)=0 \\iff \\nabla \\times \\left(-\\nabla V\\right)=0 \\\\\\\\ \\implies \\vec E = -\\frac{\\partial \\vec A}{\\partial t}-\\nabla V \\end{gather} \\] \\(\\nabla \\cdot \\vec D\\) \\[ \\begin{gather} \\nabla \\cdot \\vec E= \\frac{\\rho_v}{\\varepsilon} \\\\\\\\ \\implies \\nabla^{2}V +\\frac{\\partial }{\\partial t}\\left(\\nabla \\cdot \\vec A\\right) =-\\frac{\\rho_v}{\\varepsilon} \\end{gather} \\] \\(\\nabla \\times \\vec H\\) \\[ \\begin{gather} \\nabla \\times \\vec H= \\vec J + \\varepsilon\\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\nabla \\times \\left(\\frac{\\nabla \\times A}{\\mu}\\right)= \\vec J + \\varepsilon\\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\nabla^{2}\\vec A=\\nabla \\left(\\nabla \\cdot \\vec A\\right)-\\nabla \\times \\nabla \\times \\vec A \\\\\\\\ \\nabla^{2}\\vec A = \\nabla \\left(\\nabla \\cdot \\vec A\\right)-\\mu \\vec J - \\mu\\varepsilon \\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\nabla^{2}\\vec A - \\nabla \\left(\\nabla \\cdot \\vec A\\right)=-\\mu \\vec J + \\mu\\varepsilon\\nabla \\left(\\frac{\\partial V}{\\partial t}\\right) +\\mu\\varepsilon \\frac{\\partial ^{2}\\vec A}{\\partial t^{2}} \\end{gather} \\] coupled equations for \\(V\\) and \\(\\vec A\\) , (normally given \\(\\rho_v\\) and \\(\\vec J\\) ) \\[ \\left\\{ \\begin{align} &\\nabla^{2}V +\\frac{\\partial }{\\partial t}\\left(\\nabla \\cdot \\vec A\\right) =-\\frac{\\rho_v}{\\varepsilon} \\\\\\\\ &\\nabla^{2}\\vec A - \\nabla \\left(\\nabla \\cdot \\vec A\\right)=-\\mu \\vec J + \\mu\\varepsilon\\nabla \\left(\\frac{\\partial V}{\\partial t}\\right) +\\mu\\varepsilon \\frac{\\partial ^{2}\\vec A}{\\partial t^{2}} \\end{align}\\right. \\] decoupling: Lorenz condition eliminate \\(\\mu\\varepsilon\\nabla (\\frac{\\partial }{\\partial t}V)\\) term, by let \\[ \\begin{gather} \\nabla \\cdot \\vec A=-\\mu\\varepsilon \\frac{\\partial V}{\\partial t} \\end{gather} \\] Coulomb condition let \\[ \\begin{gather} \\nabla \\cdot \\vec A = 0 \\end{gather} \\] Magnetic Moment \u00b6 magnetic dipole \\[ \\begin{gather} \\vec m = I_b\\, \\vec S \\end{gather} \\] magnetization vector \\[ \\begin{gather} \\vec M = \\lim_{\\Delta v \\to 0}{\\frac{\\sum \\vec m_k}{\\Delta v}} \\\\\\\\ \\implies d\\vec m = \\vec M dv \\end{gather} \\] Magnetization Current \u00b6 bound volume current density \\[ \\begin{gather} \\vec J_b = \\nabla \\times \\vec M \\end{gather} \\] Comparison In a space \\(\\vec M = 0\\) (with free current only) \\[ \\begin{gather} \\nabla \\times \\vec H = \\vec J_f \\end{gather} \\] bound surface current density \\[ \\begin{gather} \\vec K_b = \\vec M \\times \\hat{a_n} \\end{gather} \\] the equation can be intuitively come up with by Permeability \u00b6 In material \\(\\vec M \\neq 0\\) \\[ \\begin{gather} \\nabla \\times \\frac{\\vec B}{\\mu_0} &= &\\vec J_f &+ \\vec J_b &= \\vec J \\\\\\\\ &=&\\nabla \\times \\vec H &+ \\nabla \\times \\vec M & \\end{gather} \\] \\[ \\begin{gather} \\\\ \\implies \\vec B = \\mu_0(\\vec H + \\vec M) \\end{gather} \\] For linear Model \\[ \\begin{gather} \\vec M = \\chi_m\\vec H \\end{gather} \\] \\[ \\begin{align} \\\\ \\implies \\vec B &= \\mu_0(1+\\chi_m)\\vec H \\\\\\\\ &= \\mu_0\\mu_r\\,\\vec H \\\\\\\\ &= \\mu\\vec H \\end{align} \\] Comparison \\[ \\begin{gather} \\vec D = \\varepsilon_0 \\vec E + \\vec P \\end{gather} \\] Boundary Condition \u00b6 by Guass' law \\[ \\begin{gather} \\nabla \\cdot \\vec B = 0 \\\\\\\\ \\implies \\vec B_{1n}= \\vec B_{2n} \\end{gather} \\] by Amp.'s law \\[ \\begin{gather} \\vec H_{1t\\bot S} - \\vec H_{2t\\bot S}=\\vec K \\end{gather} \\] Inductance \u00b6 Definition in electric circuit \\[ \\begin{gather} v=L\\frac{di}{dt} \\end{gather} \\] Magnetic Flux Linkage \u00b6 magnetic flux linkage \\(\\lambda\\) \\[ \\begin{gather} d\\lambda = Nd \\psi = \\frac{I_{int}}{I_{ext}}d\\psi \\end{gather} \\] Self Inductance \u00b6 \\[ \\begin{gather} L= \\frac{\\lambda }{I}=\\frac{\\displaystyle {N\\int_S \\vec B \\cdot d \\vec S}}{I} \\end{gather} \\] Mutual Inductance \u00b6 \\[ \\begin{gather} M_{12}=\\frac{\\lambda_{12}}{I_2}=\\frac{\\displaystyle{N_1\\int_{S_1}{\\vec B_2\\cdot d\\vec S}}}{I_2} \\\\\\\\ M_{21}=\\frac{\\lambda_{21}}{I_1}=\\frac{\\displaystyle{N_2\\int_{S_2}{\\vec B_1\\cdot d\\vec S}}}{I_1} \\\\\\\\ M_{12} = M_{21} = M \\end{gather} \\] Magnetic Energy \u00b6 Magnetic energy in circuit theory \\[ \\begin{align} W_m &= \\int{Pdt}=\\int{VI\\,dt} \\\\\\\\ &= \\int_0^{t}{L\\frac{dI}{dt}I\\,dt} \\\\\\\\ &= \\int_0^{I}{LI\\,dI}=\\frac{1}{2}LI^{2} \\end{align} \\] Comparison \\[ \\begin{gather} W_e=\\frac{1}{2}CV^{2} \\end{gather} \\] magnetostatic energy density (in general) \\[ \\begin{gather} w_m=\\frac{dW_m}{dv}=\\frac{1}{2}\\vec B \\cdot \\vec H \\end{gather} \\] Magnetic Circuits \u00b6 Electric Magnetic \\(I\\) \\(\\psi\\) emf \\(V\\) mmf \\(\u2131\\) resistance \\(R\\) reluctance \\(\u211b\\) magnetomotive force (mmf) \\[ \\begin{gather} \u2131=\\oint{\\vec H \\cdot d\\vec \u2113}=NI \\end{gather} \\] Ohm's law \\[ \\begin{gather} \u211b=\\frac{\u2131}{\\psi}=\\frac{H\u2113}{BS}=\\frac{\u2113}{\\mu S} \\end{gather} \\]","title":"Magnetostatics"},{"location":"Electromagnetics/Magnetostatics/#biot-savarts+law","text":"Sources \\[ \\begin{gather} \\vec udQ = \\frac{d\\vec \u2113}{dt}Q = Id\\vec \u2113 \\end{gather} \\] Magnetic flux density: \\[ \\begin{gather} d\\vec B = \\frac{\\mu_o}{4\\pi}\\frac{Id\\vec \u2113 \\times \\hat{a_r}}{R^{2}} \\end{gather} \\] Magnetic field density: \\[ \\begin{gather} d\\vec H = \\frac{d\\vec B}{\\mu_o}= \\frac{1}{4\\pi}\\frac{Id\\vec \u2113 \\times \\hat{a_r}}{R^{2}} \\end{gather} \\] Magnetic force \\[ \\begin{align} d\\vec F &= Id\\vec \u2113 \\times d\\vec B \\\\\\\\ &= Id\\vec \u2113 \\times \\left(\\frac{\\mu_o}{4\\pi}\\frac{Id\\vec \u2113 \\times \\hat{a_r}}{R^{2}}\\right) \\end{align} \\] Comparison \\[ \\begin{gather} d\\vec F= q \\vec E \\end{gather} \\]","title":"Biot-Savart's Law"},{"location":"Electromagnetics/Magnetostatics/#amperes+law","text":"original Ampere's law \ud83d\udc4e \\[ \\begin{gather} \\oint_C{\\vec H \\cdot d\\vec \u2113}=\\int_S{\\vec J\\cdot d\\vec S}=I_{enc} \\\\\\\\ \\nabla \\times \\vec H=\\vec J \\end{gather} \\] Ampere-Maxwell equation \ud83d\udc4d \\[ \\begin{gather} \\oint_C{\\vec H \\cdot d \\vec \u2113} = \\int_S{\\left(\\vec J_f+\\frac{\\partial \\vec D}{\\partial t}\\right)\\cdot d \\vec S} \\\\\\\\ \\nabla \\times \\vec H =\\vec J_f + \\frac{\\partial \\vec D}{\\partial t} \\end{gather} \\] displacement current for the identity \\[ \\begin{gather} \\nabla \\cdot (\\nabla \\times \\vec H) =0 \\\\\\\\ \\nabla \\cdot (\\nabla \\times \\vec H) = \\nabla \\cdot \\vec J \\neq 0 \\quad\\text{ when}\\quad\\nabla \\cdot \\vec J=-\\frac{\\partial \\rho_v}{\\partial t} \\neq 0 \\end{gather} \\] thus, let's write \\[ \\begin{gather} \\nabla \\times \\vec H = \\vec J +\\vec J_d \\end{gather} \\] then \\[ \\begin{gather} \\nabla \\cdot (\\nabla \\times \\vec H) = \\nabla \\cdot \\vec J + \\nabla \\cdot \\vec J_d= 0 \\\\\\\\ \\implies \\nabla \\cdot \\vec J_d = \\frac{\\partial \\rho_v}{\\partial t} = \\frac{\\partial }{\\partial t}\\left(\\nabla \\cdot \\vec D\\right) \\\\\\\\ \\implies\\vec J_d= \\frac{\\partial \\vec D}{\\partial t} \\end{gather} \\] displacement current \\(I_d\\) <br \\(\\vec J_d\\) is called displacement current density, and the displacement current \\(I_d\\) id defined by <br \\[ \\begin{gather} I_d = \\int_S{\\vec J_d \\cdot d\\vec S} \\end{gather} \\] for \\(I_d \\neq 0\\) whenever there is an accumulation of charges, or the imginary current inside capacitor. <br e.g. semi-infinite conductive wire.","title":"Ampere's Law"},{"location":"Electromagnetics/Magnetostatics/#gauss+law","text":"\\[ \\begin{gather} \\oint_S{\\vec B \\cdot d \\vec S} = \\int_V{\\nabla \\cdot \\vec B}=0 \\end{gather} \\] For it exists not isolated magnetic poles, Guass' law here in magnetism is pretty intuitive.","title":"Gauss' Law"},{"location":"Electromagnetics/Magnetostatics/#faradays+law","text":"static electric field \\(E_e\\) transformer-induced electric field \\(E_T\\) motion-induced electric field \\(E_m\\) induced electric field \\(E_{ind} \\equiv E_T +E_m\\) \\[ \\begin{align} V_{emf}&=\\oint_L (E_e+E_T+E_m) \\\\\\\\ &=0+\\left(-\\int_S{\\frac{\\partial \\vec B}{\\partial t}\\cdot d\\vec S}\\right)+\\oint_L{\\left(\\vec u \\times \\vec B\\right)\\cdot d\\vec \u2113} \\\\\\\\ &=-\\frac{d}{dt}\\int{\\vec B \\cdot d\\vec S} \\end{align} \\] differential form \\[ \\begin{gather} \\nabla \\times \\vec E=-\\frac{\\partial \\vec B}{\\partial t}+\\nabla \\times \\left(\\vec u \\times \\vec B\\right) \\\\\\\\ \\iff \\nabla \\times \\left(\\vec E- \\vec u \\times \\vec B\\right)=-\\frac{\\partial \\vec B}{\\partial t} \\end{gather} \\]","title":"Faraday's law"},{"location":"Electromagnetics/Magnetostatics/#magnetic+potentials","text":"","title":"Magnetic Potentials"},{"location":"Electromagnetics/Magnetostatics/#magnetic+scalar+potential","text":"magnetic scalar potential \\(V_m\\) (in \\(A\\) ) Ignoring displacement current \\(\\vec J_d\\) , if \\(\\vec J = 0\\) , then \\[ \\begin{gather} \\nabla \\times \\vec H = 0 \\\\\\\\ \\vec H = - \\nabla V_m \\end{gather} \\] comparison \\[ \\begin{gather} \\frac{1}{\\varepsilon}\\vec D = - \\nabla V \\end{gather} \\]","title":"Magnetic Scalar Potential"},{"location":"Electromagnetics/Magnetostatics/#magnetic+vector+potential","text":"magnetic vector potential \\(\\vec A\\) (Wb/m) since \\[ \\begin{gather} \\nabla \\cdot \\vec B = 0 \\end{gather} \\] and recall the identity \\[ \\begin{gather} \\nabla \\cdot \\nabla \\times \\vec A = 0 \\end{gather} \\] then we can write \\[ \\begin{gather} \\vec B = \\nabla \\times \\vec A \\end{gather} \\] by Stokes' theorem \\[ \\begin{gather} \\psi = \\int_S{\\vec B \\cdot d \\vec S}=\\int_S{(\\nabla \\times \\vec A)\\cdot d\\vec S}=\\oint_L{\\vec A\\cdot d \\vec \u2113} \\end{gather} \\] and just remember that \\[ \\begin{gather} \\vec A = \\int_Q{\\frac{\\mu}{4\\pi}\\frac{\\vec udq}{R}} \\end{gather} \\] line current \\[ \\begin{gather} \\vec udq = I d\\vec \u2113 \\end{gather} \\] surface current \\[ \\begin{gather} \\vec udq = \\vec K dS \\end{gather} \\] volume current \\[ \\begin{gather} \\vec udq = \\vec J dv \\end{gather} \\] Comparison \\[ \\begin{gather} V=\\int_Q{\\frac{1}{4\\pi \\varepsilon}\\frac{dq}{R}} \\end{gather} \\]","title":"Magnetic Vector Potential"},{"location":"Electromagnetics/Magnetostatics/#time-varying+potentials","text":"Easy but tedious derivation \\(\\nabla \\times \\vec E\\) \\[ \\begin{gather} \\nabla \\times \\vec E = -\\frac{\\partial \\left(\\nabla \\times \\vec A\\right)}{\\partial t}=-\\nabla \\times \\frac{\\partial \\vec A}{\\partial t} \\\\\\\\ \\implies \\nabla \\times \\left(\\vec E + \\frac{\\partial \\vec A }{\\partial t}\\right)=0 \\iff \\nabla \\times \\left(-\\nabla V\\right)=0 \\\\\\\\ \\implies \\vec E = -\\frac{\\partial \\vec A}{\\partial t}-\\nabla V \\end{gather} \\] \\(\\nabla \\cdot \\vec D\\) \\[ \\begin{gather} \\nabla \\cdot \\vec E= \\frac{\\rho_v}{\\varepsilon} \\\\\\\\ \\implies \\nabla^{2}V +\\frac{\\partial }{\\partial t}\\left(\\nabla \\cdot \\vec A\\right) =-\\frac{\\rho_v}{\\varepsilon} \\end{gather} \\] \\(\\nabla \\times \\vec H\\) \\[ \\begin{gather} \\nabla \\times \\vec H= \\vec J + \\varepsilon\\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\nabla \\times \\left(\\frac{\\nabla \\times A}{\\mu}\\right)= \\vec J + \\varepsilon\\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\nabla^{2}\\vec A=\\nabla \\left(\\nabla \\cdot \\vec A\\right)-\\nabla \\times \\nabla \\times \\vec A \\\\\\\\ \\nabla^{2}\\vec A = \\nabla \\left(\\nabla \\cdot \\vec A\\right)-\\mu \\vec J - \\mu\\varepsilon \\frac{\\partial \\vec E}{\\partial t} \\\\\\\\ \\nabla^{2}\\vec A - \\nabla \\left(\\nabla \\cdot \\vec A\\right)=-\\mu \\vec J + \\mu\\varepsilon\\nabla \\left(\\frac{\\partial V}{\\partial t}\\right) +\\mu\\varepsilon \\frac{\\partial ^{2}\\vec A}{\\partial t^{2}} \\end{gather} \\] coupled equations for \\(V\\) and \\(\\vec A\\) , (normally given \\(\\rho_v\\) and \\(\\vec J\\) ) \\[ \\left\\{ \\begin{align} &\\nabla^{2}V +\\frac{\\partial }{\\partial t}\\left(\\nabla \\cdot \\vec A\\right) =-\\frac{\\rho_v}{\\varepsilon} \\\\\\\\ &\\nabla^{2}\\vec A - \\nabla \\left(\\nabla \\cdot \\vec A\\right)=-\\mu \\vec J + \\mu\\varepsilon\\nabla \\left(\\frac{\\partial V}{\\partial t}\\right) +\\mu\\varepsilon \\frac{\\partial ^{2}\\vec A}{\\partial t^{2}} \\end{align}\\right. \\] decoupling: Lorenz condition eliminate \\(\\mu\\varepsilon\\nabla (\\frac{\\partial }{\\partial t}V)\\) term, by let \\[ \\begin{gather} \\nabla \\cdot \\vec A=-\\mu\\varepsilon \\frac{\\partial V}{\\partial t} \\end{gather} \\] Coulomb condition let \\[ \\begin{gather} \\nabla \\cdot \\vec A = 0 \\end{gather} \\]","title":"Time-Varying Potentials"},{"location":"Electromagnetics/Magnetostatics/#magnetic+moment","text":"magnetic dipole \\[ \\begin{gather} \\vec m = I_b\\, \\vec S \\end{gather} \\] magnetization vector \\[ \\begin{gather} \\vec M = \\lim_{\\Delta v \\to 0}{\\frac{\\sum \\vec m_k}{\\Delta v}} \\\\\\\\ \\implies d\\vec m = \\vec M dv \\end{gather} \\]","title":"Magnetic Moment"},{"location":"Electromagnetics/Magnetostatics/#magnetization+current","text":"bound volume current density \\[ \\begin{gather} \\vec J_b = \\nabla \\times \\vec M \\end{gather} \\] Comparison In a space \\(\\vec M = 0\\) (with free current only) \\[ \\begin{gather} \\nabla \\times \\vec H = \\vec J_f \\end{gather} \\] bound surface current density \\[ \\begin{gather} \\vec K_b = \\vec M \\times \\hat{a_n} \\end{gather} \\] the equation can be intuitively come up with by","title":"Magnetization Current"},{"location":"Electromagnetics/Magnetostatics/#permeability","text":"In material \\(\\vec M \\neq 0\\) \\[ \\begin{gather} \\nabla \\times \\frac{\\vec B}{\\mu_0} &= &\\vec J_f &+ \\vec J_b &= \\vec J \\\\\\\\ &=&\\nabla \\times \\vec H &+ \\nabla \\times \\vec M & \\end{gather} \\] \\[ \\begin{gather} \\\\ \\implies \\vec B = \\mu_0(\\vec H + \\vec M) \\end{gather} \\] For linear Model \\[ \\begin{gather} \\vec M = \\chi_m\\vec H \\end{gather} \\] \\[ \\begin{align} \\\\ \\implies \\vec B &= \\mu_0(1+\\chi_m)\\vec H \\\\\\\\ &= \\mu_0\\mu_r\\,\\vec H \\\\\\\\ &= \\mu\\vec H \\end{align} \\] Comparison \\[ \\begin{gather} \\vec D = \\varepsilon_0 \\vec E + \\vec P \\end{gather} \\]","title":"Permeability"},{"location":"Electromagnetics/Magnetostatics/#boundary+condition","text":"by Guass' law \\[ \\begin{gather} \\nabla \\cdot \\vec B = 0 \\\\\\\\ \\implies \\vec B_{1n}= \\vec B_{2n} \\end{gather} \\] by Amp.'s law \\[ \\begin{gather} \\vec H_{1t\\bot S} - \\vec H_{2t\\bot S}=\\vec K \\end{gather} \\]","title":"Boundary Condition"},{"location":"Electromagnetics/Magnetostatics/#inductance","text":"Definition in electric circuit \\[ \\begin{gather} v=L\\frac{di}{dt} \\end{gather} \\]","title":"Inductance"},{"location":"Electromagnetics/Magnetostatics/#magnetic+flux+linkage","text":"magnetic flux linkage \\(\\lambda\\) \\[ \\begin{gather} d\\lambda = Nd \\psi = \\frac{I_{int}}{I_{ext}}d\\psi \\end{gather} \\]","title":"Magnetic Flux Linkage"},{"location":"Electromagnetics/Magnetostatics/#self+inductance","text":"\\[ \\begin{gather} L= \\frac{\\lambda }{I}=\\frac{\\displaystyle {N\\int_S \\vec B \\cdot d \\vec S}}{I} \\end{gather} \\]","title":"Self Inductance"},{"location":"Electromagnetics/Magnetostatics/#mutual+inductance","text":"\\[ \\begin{gather} M_{12}=\\frac{\\lambda_{12}}{I_2}=\\frac{\\displaystyle{N_1\\int_{S_1}{\\vec B_2\\cdot d\\vec S}}}{I_2} \\\\\\\\ M_{21}=\\frac{\\lambda_{21}}{I_1}=\\frac{\\displaystyle{N_2\\int_{S_2}{\\vec B_1\\cdot d\\vec S}}}{I_1} \\\\\\\\ M_{12} = M_{21} = M \\end{gather} \\]","title":"Mutual Inductance"},{"location":"Electromagnetics/Magnetostatics/#magnetic+energy","text":"Magnetic energy in circuit theory \\[ \\begin{align} W_m &= \\int{Pdt}=\\int{VI\\,dt} \\\\\\\\ &= \\int_0^{t}{L\\frac{dI}{dt}I\\,dt} \\\\\\\\ &= \\int_0^{I}{LI\\,dI}=\\frac{1}{2}LI^{2} \\end{align} \\] Comparison \\[ \\begin{gather} W_e=\\frac{1}{2}CV^{2} \\end{gather} \\] magnetostatic energy density (in general) \\[ \\begin{gather} w_m=\\frac{dW_m}{dv}=\\frac{1}{2}\\vec B \\cdot \\vec H \\end{gather} \\]","title":"Magnetic Energy"},{"location":"Electromagnetics/Magnetostatics/#magnetic+circuits","text":"Electric Magnetic \\(I\\) \\(\\psi\\) emf \\(V\\) mmf \\(\u2131\\) resistance \\(R\\) reluctance \\(\u211b\\) magnetomotive force (mmf) \\[ \\begin{gather} \u2131=\\oint{\\vec H \\cdot d\\vec \u2113}=NI \\end{gather} \\] Ohm's law \\[ \\begin{gather} \u211b=\\frac{\u2131}{\\psi}=\\frac{H\u2113}{BS}=\\frac{\u2113}{\\mu S} \\end{gather} \\]","title":"Magnetic Circuits"},{"location":"Electromagnetics/Method%20of%20Images/","text":"Apollonian Circles \u00b6 https://en.wikipedia.org/wiki/Apollonian_circles If you look at the blue circles, you would probably find that It's pretty similar to the figures on the handout. Let's take a look on the definition of the first family of Apollonian circles \\[ \\begin{gather} \\left\\{X \\big| \\frac{d(X, C)}{d(X, D)}=r\\right\\} \\end{gather} \\] It's not difficult to find it is actually the same. It is the most important thing in Apollonian circles that it have a principle Every circles in the first family (blue circles) can be written as the linear combination of two different circles in the first family. This Lemma is not hard to proof. By the definition above we have \\[ \\begin{gather} \\frac{d(X, C)}{d(X, D)}=\\frac{\\sqrt{(x-C_x)^{2}+(y-C_y)^{2}}}{\\sqrt{(x-D_x)^{2}+(y-D_y)^{2}}} = r \\\\\\\\ \\frac{(x-C_x)^{2}+(y-C_y)^{2}}{(x-D_x)^{2}+(y-D_y)^{2}} = r^{2} \\\\\\\\ \\bigg((x-C_x)^{2}+(y-C_y)^{2}\\bigg) -r^{2}\\bigg((x-D_x)^{2}+(y-D_y)^{2}\\bigg)=0 \\end{gather} \\] for every circles in Apollonian circles family, we have the form of above formula. Namely, linear combination of two source point (in Electromagnetics usually line charges or point charges) can represent all of the circles. note that these source points can also be view as circles with \\(r=0\\) . Line Charge Problem \u00b6 Type 1 \u00b6 Given \\(\\rho_2/\\rho_1 = k\\) and \\(b\\) we can write the circle equation of the potential \\(V_o\\) as \\[ \\begin{gather} \\frac{d(X, -b)}{d(X, b)}=\\frac{\\rho_2}{\\rho_1} = k \\\\\\\\ \\implies ((x+b)^{2}+y^{2}) - k^{2}\\left((x-b)^{2}+y^{2}\\right) =0 \\\\\\\\ (1-k^{2})x^{2}+(1-k^{2})y^{2}+2b(1+k^{2})x+(1-k^{2})b^{2}=0 \\\\\\\\ x^{2}+2b\\frac{1+k^{2}}{1-k^{2}}x+y^{2} + b^{2}=0 \\\\\\\\ \\left(x+b\\frac{1+k^{2}}{1-k^{2}}\\right)^{2}+y^{2}=\\left(b\\frac{1+k^{2}}{1-k^{2}}\\right)^{2}-b^{2} \\\\\\\\ \\end{gather} \\] \\[ \\implies \\left\\{ \\begin{align} c &= -b\\frac{1+k^{2}}{1-k^{2}} \\\\\\\\ a^{2} &= \\left(b\\frac{1+k^{2}}{1-k^{2}}\\right)^{2}-b^{2} = c^{2}-b^{2} \\end{align}\\right. \\] Type 2 \u00b6 Given \\(a_1, a_2, D\\) simply assume two circles formula as \\[ \\begin{gather} (x+c_1)^{2} +y^{2} - a_1^{2} = 0 \\\\\\\\ (x-c_2)^{2} +y^{2} - a_2^{2} = 0 \\end{gather} \\] Lemma of Apollonian circles tell as that every circle in the first family can be written as the linear combination of these two circle. \\(y\\) axis, bisection(\u4e2d\u5782\u7dda) of \\(q_1, q_2\\) in this problem is also included(circle that have \\(\\infty\\) radius). \\[ \\begin{gather} \\bigg((x+c_1)^{2} +y^{2} - a_1^{2}\\bigg) +m \\bigg( (x-c_2)^{2} +y^{2} - a_2^{2}\\bigg)= 0 \\end{gather} \\] and for the \\(y\\) axis \\(x=0\\) , it's reasonable to guess \\(m=-1\\) \\[ \\begin{gather} \\bigg((x+c_1)^{2} +y^{2} - a_1^{2}\\bigg) - \\bigg( (x-c_2)^{2} +y^{2} - a_2^{2}\\bigg)= 0 \\\\\\\\ 2(c_1+c_2)x +c_1^{2} -c_2^{2} -a_1^{2}+a_2^{2}=0 \\\\\\\\ \\implies c_1^{2} -c_2^{2} -a_1^{2}+a_2^{2}=0 \\\\\\\\ \\implies (c_1+c_2)(c_1-c_2) = a_1^{2}-a_2^{2} \\\\\\\\ \\implies c_1-c_2 = \\frac{a_1^{2}-a_2^{2}}{D} \\end{gather} \\] It's easy to solve \\(c_1, c_2\\) now. Sphere Problem \u00b6 for sphere problem, we commonly can get the relationship \\[ \\begin{gather} \\frac{r_1}{r_2} = \\frac{q_1}{q_2} \\end{gather} \\] \\(q_1/q_2\\) is definitely a constant. Thus it is an Apollonian circles problem again. In this type of problem, usually given \\(a, d_1, q_1\\) as the Lemma mentioned above we know that there exists a bisection(\u4e2d\u5782\u7dda) of \\(q_1\\) and \\(q_2\\) . The bisection could be written as the linear combination of two circle equations, surface of sphere and the point \\(q_1\\) \\[ \\begin{gather} x^{2}+z^{2} - a^{2} = 0 \\\\\\\\ x^{2} + (z-d_1)^{2} = 0 \\\\\\\\ \\implies \\bigg(x^{2}+z^{2} - a^{2}\\bigg) +m \\bigg( x^{2} + (z-d_1)^{2}\\bigg) = 0 \\end{gather} \\] guess \\(m=-1\\) \\[ \\begin{gather} \\bigg(x^{2}+z^{2} - a^{2}\\bigg) - \\bigg( x^{2} + (z-d_1)^{2}\\bigg) = 0 \\\\\\\\ 2d_1z-a^{2}-d_1^{2} = 0 \\\\\\\\ z = \\frac{a^{2}+d_1^{2}}{2d_1} \\end{gather} \\] now we can fine \\(b\\) \\[ \\begin{gather} b = d_1 - \\frac{a^{2}+d_1^{2}}{2d_1} = \\frac{d_1^{2}-a^{2}}{2d_1} \\\\\\\\ \\implies d_2= d_1-2b = d_1-\\frac{d_1^{2}-a^{2}}{d_1} = \\frac{a^{2}}{d_1} \\end{gather} \\] and for the value of \\(r_1/r_2\\) which is a constant . Thus, simply take the point \\(A(0, 0, a)\\) on the surface of sphere. \\[ \\begin{gather} k = \\frac{r_1}{r_2} = \\frac{d(A, q_1)}{d(A, q_2)} = \\frac{d_1-a}{a-d_2} = \\frac{d_1-a}{a-\\frac{a^{2}}{d_1}} = \\frac{d_1(1-\\frac{a}{d_1})}{a(1-\\frac{a}{d_1})} = \\frac{d_1}{a} \\end{gather} \\]","title":"Method of Images"},{"location":"Electromagnetics/Method%20of%20Images/#apollonian+circles","text":"https://en.wikipedia.org/wiki/Apollonian_circles If you look at the blue circles, you would probably find that It's pretty similar to the figures on the handout. Let's take a look on the definition of the first family of Apollonian circles \\[ \\begin{gather} \\left\\{X \\big| \\frac{d(X, C)}{d(X, D)}=r\\right\\} \\end{gather} \\] It's not difficult to find it is actually the same. It is the most important thing in Apollonian circles that it have a principle Every circles in the first family (blue circles) can be written as the linear combination of two different circles in the first family. This Lemma is not hard to proof. By the definition above we have \\[ \\begin{gather} \\frac{d(X, C)}{d(X, D)}=\\frac{\\sqrt{(x-C_x)^{2}+(y-C_y)^{2}}}{\\sqrt{(x-D_x)^{2}+(y-D_y)^{2}}} = r \\\\\\\\ \\frac{(x-C_x)^{2}+(y-C_y)^{2}}{(x-D_x)^{2}+(y-D_y)^{2}} = r^{2} \\\\\\\\ \\bigg((x-C_x)^{2}+(y-C_y)^{2}\\bigg) -r^{2}\\bigg((x-D_x)^{2}+(y-D_y)^{2}\\bigg)=0 \\end{gather} \\] for every circles in Apollonian circles family, we have the form of above formula. Namely, linear combination of two source point (in Electromagnetics usually line charges or point charges) can represent all of the circles. note that these source points can also be view as circles with \\(r=0\\) .","title":"Apollonian Circles"},{"location":"Electromagnetics/Method%20of%20Images/#line+charge+problem","text":"","title":"Line Charge Problem"},{"location":"Electromagnetics/Method%20of%20Images/#type+1","text":"Given \\(\\rho_2/\\rho_1 = k\\) and \\(b\\) we can write the circle equation of the potential \\(V_o\\) as \\[ \\begin{gather} \\frac{d(X, -b)}{d(X, b)}=\\frac{\\rho_2}{\\rho_1} = k \\\\\\\\ \\implies ((x+b)^{2}+y^{2}) - k^{2}\\left((x-b)^{2}+y^{2}\\right) =0 \\\\\\\\ (1-k^{2})x^{2}+(1-k^{2})y^{2}+2b(1+k^{2})x+(1-k^{2})b^{2}=0 \\\\\\\\ x^{2}+2b\\frac{1+k^{2}}{1-k^{2}}x+y^{2} + b^{2}=0 \\\\\\\\ \\left(x+b\\frac{1+k^{2}}{1-k^{2}}\\right)^{2}+y^{2}=\\left(b\\frac{1+k^{2}}{1-k^{2}}\\right)^{2}-b^{2} \\\\\\\\ \\end{gather} \\] \\[ \\implies \\left\\{ \\begin{align} c &= -b\\frac{1+k^{2}}{1-k^{2}} \\\\\\\\ a^{2} &= \\left(b\\frac{1+k^{2}}{1-k^{2}}\\right)^{2}-b^{2} = c^{2}-b^{2} \\end{align}\\right. \\]","title":"Type 1"},{"location":"Electromagnetics/Method%20of%20Images/#type+2","text":"Given \\(a_1, a_2, D\\) simply assume two circles formula as \\[ \\begin{gather} (x+c_1)^{2} +y^{2} - a_1^{2} = 0 \\\\\\\\ (x-c_2)^{2} +y^{2} - a_2^{2} = 0 \\end{gather} \\] Lemma of Apollonian circles tell as that every circle in the first family can be written as the linear combination of these two circle. \\(y\\) axis, bisection(\u4e2d\u5782\u7dda) of \\(q_1, q_2\\) in this problem is also included(circle that have \\(\\infty\\) radius). \\[ \\begin{gather} \\bigg((x+c_1)^{2} +y^{2} - a_1^{2}\\bigg) +m \\bigg( (x-c_2)^{2} +y^{2} - a_2^{2}\\bigg)= 0 \\end{gather} \\] and for the \\(y\\) axis \\(x=0\\) , it's reasonable to guess \\(m=-1\\) \\[ \\begin{gather} \\bigg((x+c_1)^{2} +y^{2} - a_1^{2}\\bigg) - \\bigg( (x-c_2)^{2} +y^{2} - a_2^{2}\\bigg)= 0 \\\\\\\\ 2(c_1+c_2)x +c_1^{2} -c_2^{2} -a_1^{2}+a_2^{2}=0 \\\\\\\\ \\implies c_1^{2} -c_2^{2} -a_1^{2}+a_2^{2}=0 \\\\\\\\ \\implies (c_1+c_2)(c_1-c_2) = a_1^{2}-a_2^{2} \\\\\\\\ \\implies c_1-c_2 = \\frac{a_1^{2}-a_2^{2}}{D} \\end{gather} \\] It's easy to solve \\(c_1, c_2\\) now.","title":"Type 2"},{"location":"Electromagnetics/Method%20of%20Images/#sphere+problem","text":"for sphere problem, we commonly can get the relationship \\[ \\begin{gather} \\frac{r_1}{r_2} = \\frac{q_1}{q_2} \\end{gather} \\] \\(q_1/q_2\\) is definitely a constant. Thus it is an Apollonian circles problem again. In this type of problem, usually given \\(a, d_1, q_1\\) as the Lemma mentioned above we know that there exists a bisection(\u4e2d\u5782\u7dda) of \\(q_1\\) and \\(q_2\\) . The bisection could be written as the linear combination of two circle equations, surface of sphere and the point \\(q_1\\) \\[ \\begin{gather} x^{2}+z^{2} - a^{2} = 0 \\\\\\\\ x^{2} + (z-d_1)^{2} = 0 \\\\\\\\ \\implies \\bigg(x^{2}+z^{2} - a^{2}\\bigg) +m \\bigg( x^{2} + (z-d_1)^{2}\\bigg) = 0 \\end{gather} \\] guess \\(m=-1\\) \\[ \\begin{gather} \\bigg(x^{2}+z^{2} - a^{2}\\bigg) - \\bigg( x^{2} + (z-d_1)^{2}\\bigg) = 0 \\\\\\\\ 2d_1z-a^{2}-d_1^{2} = 0 \\\\\\\\ z = \\frac{a^{2}+d_1^{2}}{2d_1} \\end{gather} \\] now we can fine \\(b\\) \\[ \\begin{gather} b = d_1 - \\frac{a^{2}+d_1^{2}}{2d_1} = \\frac{d_1^{2}-a^{2}}{2d_1} \\\\\\\\ \\implies d_2= d_1-2b = d_1-\\frac{d_1^{2}-a^{2}}{d_1} = \\frac{a^{2}}{d_1} \\end{gather} \\] and for the value of \\(r_1/r_2\\) which is a constant . Thus, simply take the point \\(A(0, 0, a)\\) on the surface of sphere. \\[ \\begin{gather} k = \\frac{r_1}{r_2} = \\frac{d(A, q_1)}{d(A, q_2)} = \\frac{d_1-a}{a-d_2} = \\frac{d_1-a}{a-\\frac{a^{2}}{d_1}} = \\frac{d_1(1-\\frac{a}{d_1})}{a(1-\\frac{a}{d_1})} = \\frac{d_1}{a} \\end{gather} \\]","title":"Sphere Problem"},{"location":"Electromagnetics/Transmission%20Line/","text":"Transmission Line \u00b6 Transmission-Line Equations \u00b6 First define - \\(\u212d\\) : inductance per unit length - \\(\ud835\udd0f\\) : inductance per unit length Lossless Transmission Lines \u00b6 \\[ \\left\\{ \\begin{align} \\frac{\\partial V}{\\partial z}&=-\ud835\udd0f\\frac{\\partial I}{\\partial t} \\\\\\\\ \\frac{\\partial I}{\\partial z}&= -\u212d\\frac{\\partial V}{\\partial t} \\end{align}\\right. \\] tip These equation can be easily derived from the definition of capacitance and capacitance in electric circuit theorem \\[ \\begin{gather} v = L\\frac{di}{dt} \\\\\\\\ i= C\\frac{dv}{dt} \\end{gather} \\] Similar to the derivation in EM wave equation, we can obtain the wave equation form \\[ \\left\\{ \\begin{align} \\frac{\\partial^{2} V}{\\partial z^{2}} &= \ud835\udd0f\u212d\\frac{\\partial^{2} V}{\\partial t^{2}} \\\\\\\\ \\frac{\\partial^{2}I}{\\partial z^{2}} &= \ud835\udd0f\u212d\\frac{\\partial^{2} I}{\\partial t^{2}} \\end{align}\\right. \\] Thus we have the wave velocity \\[ \\begin{gather} u = \\frac{1}{\\sqrt{\ud835\udd0f\u212d}} \\end{gather} \\] Comparing to the velocity of EM wave, it tell us the important relation that \\[ \\begin{gather} \\boxed{ \ud835\udd0f\u212d = \\mu\\varepsilon } \\end{gather} \\] Furthermore, considering the phasor form of transmission-line equation, we have \\[ \\begin{align} \\frac{d^{2}V_s}{dz^{2}} = (j\\beta)^{2}\\,V_s \\\\\\\\ \\frac{d^{2}I_s}{dz^{2}} = (j\\beta)^{2}\\,I_s \\end{align} \\] in which \\[ \\begin{gather} (j\\beta)^{2} = (j\\omega)^{2}\ud835\udd0f\u212d \\\\\\\\ \\implies \\boxed{ \\beta = \\omega\\sqrt{\ud835\udd0f\u212d} = \\frac{\\omega}{u} } \\end{gather} \\] General Solutions \u00b6 \\[ \\begin{align} V(z, t) &= V^{+}(z-ut)+V^{-}(z+ut) \\\\\\\\ I(z, t) &= I^{+}(z-ut)+I^{-}(z+ut) \\end{align} \\] Similarly, we have the characteristic impedance \\(Z_0\\) \\[ \\begin{gather} Z_0 = \\sqrt\\frac{\ud835\udd0f}{\u212d}=\\frac{V^{+}}{I^{+}}=-\\frac{V^{-}}{I^{-}} \\end{gather} \\] General Transmission LInes \u00b6 Revise the transmission-line equation in the lossless case into \\[ \\left\\{ \\begin{align} -\\frac{\\partial V}{\\partial z}&=RI+L\\frac{\\partial I}{\\partial t} \\\\\\\\ -\\frac{\\partial I}{\\partial z}&= GV+C\\frac{\\partial V}{\\partial t} \\end{align}\\right. \\qquad\\overset{\ud835\udd09ourier}{\\iff}\\qquad \\left\\{ \\begin{aligned} \\frac{d V_s}{d z}&=-(R+ j\\omega L)I_s \\\\\\\\ \\frac{d \\,I_s}{d z}&= -(G+j\\omega C)V_s \\end{aligned}\\right. \\] Also, in the form of wave equations \\[ \\begin{gather} \\frac{d^{2}V_s}{dz^{2}} = \\gamma^{2}V_s \\\\\\\\ \\frac{d^{2}I_s}{dz^{2}} = \\gamma^{2}I_s \\end{gather} \\] in which \\(\\gamma\\) is also called as propagation constant . \\[ \\begin{gather} \\gamma = \\sqrt{(R+j\\omega L)(G+j\\omega C)} = \\alpha+j\\beta \\end{gather} \\] General Solutions \u00b6 solve the wave equations above \\[ \\begin{align} V_s &= V_0^{+}e^{-\\gamma z}+V_0^{-}e^{\\gamma z} \\\\\\\\ I_s &= I_0^{+}e^{-\\gamma z}+I_0^{-}e^{\\gamma z} \\\\\\\\ &= \\frac{V_0^{+}}{Z_0}e^{-\\gamma z}-\\frac{V_0^{+}}{Z_0}e^{\\gamma z} \\end{align} \\] and the corresponding time domain form \\[ \\begin{align} V(z, t) &= |V_0^{+}|e^{-\\alpha z}\\cos(\\omega t- \\beta z + \\phi^{+}) + |V_0^{-}|e^{\\alpha z}\\cos(\\omega t+ \\beta z + \\phi^{-}) \\\\\\\\ I(z, t) &= |I_0^{+}|e^{-\\alpha z}\\cos(\\omega t- \\beta z + \\varphi^{+}) + |I_0^{-}|e^{\\alpha z}\\cos(\\omega t+ \\beta z + \\varphi^{-}) \\end{align} \\] power attenuation \\[ \\begin{align} \\text{attenuation const} &= \\alpha \\quad\\text{(NP/m)} \\\\\\\\ \\text{power attenuation} &= 20 \\log{e^{\\alpha}}\\quad\\text{(dB/m)} \\end{align} \\] thus, \\[ \\begin{gather} 1\\,\\text{NP} = 20\\log{e^{1}} = 8.69 \\, \\text{dB} \\end{gather} \\] Power \u00b6 time-average power \\[ \\begin{align} P(z) &= \\frac{1}{2}\\text{Re}\\bigg[VI^{*}\\bigg] \\\\\\\\ &= \\frac{1}{2}\\text{Re}\\bigg[V_0^{+}(1+\\Gamma)\\left(\\frac{V_0^{+}}{Z_0}\\right)^{*}(1-\\Gamma^{*})\\bigg] \\\\\\\\ &= \\frac{1}{2}\\text{Re}\\bigg[\\frac{{V_0^{+}}^{2}}{Z_0}(1+\\Gamma)(1-\\Gamma^{*})\\bigg] \\\\\\\\ &=\\frac{1}{2}\\frac{\\left|V_0^{+}\\right|^{2}}{Z_0}(1-|\\Gamma|^{2}) \\tag 6 \\end{align} \\] ^equ-6 \\[ \\implies \\begin{dcases} P^{+}=\\frac{1}{2}\\left|V_0^{+}\\right||I_0^{+}|\\,e^{+ 2\\alpha z} \\\\\\\\ P^{-}=\\frac{1}{2}\\left|V_0^{-}\\right||I_0^{-}|\\,e^{- 2\\alpha z} \\end{dcases} \\] power loss \\[ \\begin{gather} \\text{PL} =P(0)-P(z) \\end{gather} \\] Distortionless \u00b6 conditions \\[ \\begin{gather} \\frac{R}{L} = \\frac{G}{C} \\end{gather} \\] thus, we have the propagation constant \\(\\gamma\\) \\[ \\begin{align} \\gamma &= \\sqrt{(R+j\\omega L)(G+ j\\omega C)} \\\\\\\\ &= \\sqrt{LC} \\sqrt{\\left(\\frac{R}{L} +j\\omega\\right)\\left(\\frac{G}{C} +j\\omega\\right)} \\\\\\\\ &= \\sqrt{LC} \\left(\\frac{R}{L} +j\\omega\\right) \\\\\\\\ &= \\sqrt{RG} + j\\omega\\sqrt{LC} \\end{align} \\] and the characteristic impedance \\(Z_0\\) \\[ \\begin{align} Z_0 &= \\sqrt{\\frac{R+j\\omega L}{G+ j\\omega C}} \\\\\\\\ &= \\sqrt {\\frac{R(1+j\\omega\\frac{L}{R})}{G(1+j\\omega(\\frac{C}{G}))}} \\\\\\\\ &= \\sqrt{\\frac{R}{G}} = \\sqrt{\\frac{L}{C}} \\end{align} \\] Standing Wave Ratio \u00b6 aka \\(\\text{SWR}\\) Reflection \u00b6 voltage reflection coefficient \\[ \\begin{gather} \\Gamma(z) = \\frac{V^{-}(z)}{V^{+}(z)} =\\frac{V_0^{-}e^{+\\gamma z}}{V_0^{+}e^{-\\gamma z}} =\\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma z} \\end{gather} \\] current reflection coefficient \\[ \\begin{gather} \\Gamma_I(z)=\\frac{I^{-}}{I^{+}}=-\\frac{V^{-}}{V^{+}}=-\\Gamma(z) \\end{gather} \\] \\(\\Gamma_L\\) Let's define the impedance from load \\(Z_L=V_L/I_L\\) \\[ \\begin{gather} V_0^{+}e^{-\\gamma \u2113}+V_0^{-}e^{+\\gamma\u2113}=\\frac{Z_L}{Z_0}(V_0^{+}e^{-\\gamma\u2113}-V_0^{-}e^{+\\gamma \u2113}) \\\\\\\\ \\implies (Z_0-Z_L)V_0^{+}e^{-\\gamma \u2113}=-(Z_L+Z_0)V_0^{-}e^{+\\gamma \u2113} \\\\\\\\ \\implies \\boxed{ \\Gamma_L=\\frac{V_0^{-}e^{+\\gamma \u2113 }}{V_0^{+}e^{-\\gamma \u2113}}=\\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma \u2113 }=\\frac{Z_L-Z_0}{Z_L+Z_0} } \\tag{2} \\end{gather} \\] ^equ-2 We can further write the reflection coefficient from load at \\(d\\) as \\[ \\begin{align} \\Gamma(d)= \\Gamma_{L}\\,e^{-2\\gamma d} \\tag 3 \\end{align} \\] ^equ-3 tips from \\(\\text{equation (3-1)}\\) , \\[ \\begin{gather} Z(z) = Z_0\\frac{1+\\Gamma}{1-\\Gamma} \\\\\\\\ \\implies Z(z) - Z(z)\\,\\Gamma = Z_0+Z_0\\,\\Gamma \\\\\\\\ \\implies \\boxed{ \\Gamma(z) = \\frac{Z(z) - Z_0}{Z(z) + Z_0} } \\end{gather} \\] we can also derive \\(\\text{equation (2)}\\) by virtue of this equation \\[ \\begin{gather} \\Gamma_L = \\Gamma(z=\u2113) = \\Gamma(d=0) = \\frac{Z(z=\u2113)-Z_0}{Z(z=\u2113)+Z_0} \\end{gather} \\] Furthermore, at the generator-end (i.e. \\(z=0\\) ), we have \\[ \\begin{gather} \\Gamma(z=0)=\\Gamma(d=\u2113)=\\frac{Z_{in}-Z_0}{Z_{in} + Z_0} \\end{gather} \\] impedance \u00b6 Line impedance at \\(z\\) , \\(Z(z)\\) \\[ \\begin{align} Z(z) &= \\frac{V_s}{I_s} \\\\\\\\ &= Z_0\\frac{V_0^{+}e^{-\\gamma z}+V_0^{-}e^{+\\gamma z}}{V_0^{+}e^{-\\gamma z}-V_0^{-}e^{+\\gamma z}} = Z_0\\frac{1+\\displaystyle \\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma z}}{1-\\displaystyle \\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma z}} \\\\\\\\ &= Z_0\\frac{1+\\Gamma(z)}{1-\\Gamma(z)} \\tag{3-1} \\end{align} \\] ^equ-3-1 impedance at load \\(Z_L\\) \\[ \\begin{align} Z_L &= Z(z=\u2113) = Z_0\\frac{1+\\Gamma(z=\u2113)}{1-\\Gamma(z=\u2113)} \\\\\\\\ &= Z_0\\frac{1+\\Gamma_L}{1-\\Gamma_L} \\end{align} \\] input impedance \\(Z_{in}\\) \\[ \\begin{align} Z_{in} &= Z(z=0) = Z_0\\frac{1+\\Gamma(z=0)}{1-\\Gamma(z=0)} \\\\\\\\ &= Z_0\\,\\frac{1+ \\displaystyle \\frac{V_0^{-}}{V_0^{+}}}{1- \\displaystyle \\frac{V_0^{-}}{V_0^{+}}} \\\\\\\\ &= Z_0\\,\\frac{1+ \\Gamma_L\\,e^{-2\\gamma \u2113}}{1- \\Gamma_L\\,e^{-2\\gamma \u2113}} \\end{align} \\] from \\(\\text{equation (2)}\\) we have \\[ \\begin{align} Z_{in} &= Z_0\\,\\frac{1+ \\Gamma_L\\,e^{-2\\gamma \u2113}}{1- \\Gamma_L\\,e^{-2\\gamma \u2113}} \\\\\\\\ &= Z_0\\,\\frac{1+ \\displaystyle \\frac{Z_L-Z_0}{Z_L+Z_0}\\,e^{-2\\gamma \u2113}}{1- \\displaystyle \\frac{Z_L-Z_0}{Z_L+Z_0}\\,e^{-2\\gamma \u2113}} \\\\\\\\ &= Z_0\\,\\frac{Z_L+Z_0+ (Z_L-Z_0)\\,e^{-2\\gamma \u2113}}{Z_L+Z_0- (Z_L-Z_0)\\,e^{-2\\gamma \u2113}} \\\\\\\\ &= Z_0\\,\\frac{Z_L(1+e^{-2\\gamma \u2113})+Z_0(1-e^{-2\\gamma \u2113})}{Z_L(1-e^{-2\\gamma \u2113})+Z_0(1+e^{-2\\gamma \u2113})} \\\\\\\\ &= Z_0\\,\\frac{Z_L+Z_0\\,\\tanh{\\gamma\u2113}}{Z_L\\,\\tanh{\\gamma\u2113}+Z_0} \\\\\\\\ &= \\boxed{ Z_0\\,\\frac{Z_L+Z_0\\,\\tanh{\\gamma\u2113}}{Z_0+Z_L\\,\\tanh{\\gamma\u2113}} } \\\\\\\\ &\\xlongequal{\\alpha = 0} Z_0\\,\\frac{Z_L+jZ_0\\,\\tan{\\beta\u2113}}{Z_0+jZ_L\\,\\tan{\\beta\u2113}} \\end{align} \\] Complex form \u00b6 As usual, we can represent \\(\\Gamma\\) with complex coordinate. \\[ \\begin{gather} \\Gamma = |\\Gamma |\\,\\angle{\\Gamma} \\end{gather} \\] Consider \\(\\text{equation (3)}\\) , we further have \\[ \\begin{align} \\boxed{ \\Gamma=\\Gamma_L\\,e^{-2\\alpha d}\\,e^{-j2\\beta d} } \\end{align} \\] Besides, we can rewrite \\(V_s, I_s\\) as \\[ \\begin{align} |V_s| &= \\left|V^{+}\\right||1+\\Gamma| \\\\\\\\ |I_s| &= \\left|I^{+}\\right||1-\\Gamma| \\end{align} \\] Standing Wave Ratio for Lossless \u00b6 \\[ \\begin{align} \\text{VSWR} &\\equiv \\frac{V_{max}}{V_{min}} = \\frac{|V_0^{+}||1+\\Gamma|_{max}}{|V_0^{+}||1+\\Gamma|_{min}} \\\\\\\\ &= \\frac{1+ |\\Gamma|}{1- |\\Gamma|} \\xlongequal{\\alpha\\,=\\,0}\\frac{1+ |\\Gamma_L|}{1- |\\Gamma_L|} \\end{align} \\] and similarly for \\(\\text{ISWR}\\) , \\[ \\begin{gather} \\frac{I_{max}}{I_{min}}=\\frac{V_{max}}{V_{min}}=\\text{SWR} \\end{gather} \\] according to \\(\\text{equation (5)}\\) , \\[ \\begin{align} z=\\frac{1+\\Gamma}{1-\\Gamma}=r+jx \\end{align} \\] we can obtain \\(\\text{SWR}\\) by picking the point that \\(\\angle\\Gamma = 0\\) , then \\[ \\begin{align} \\text{SWR} = \\frac{1+|\\Gamma|}{1-|\\Gamma|}= r+j0=r \\end{align} \\] Smith Chart \u00b6 Smith chart help us quickly obtain the normalized impedance \\(Z\\) w.r.t \\(Z_0\\) . \\[ \\begin{align} Z= z\\times Z_0 = (r+jx)\\times Z_0 \\end{align} \\] then, we can have the normalized impedance \\(z\\) \\[ \\begin{align} z=r+jx= \\frac{Z}{Z_0}=\\frac{1+\\Gamma}{1-\\Gamma} \\tag 5 \\end{align} \\] ^equ-5 \\(r\\) -circles \\(x\\) -circles Admittance Chart \u00b6 for the admittance of transmission line, we can easily obtain the relationship of admittance and impedance. \\[ \\begin{gather} y=\\frac{Y}{Y_0}=\\frac{1}{z}=\\frac{1-\\Gamma}{1+\\Gamma} \\\\\\\\ \\implies y=z\\,e^{-j\\pi} \\tag 4 \\end{gather} \\] ^equ-4 the \\(\\text{equation (4)}\\) tell us that we can obtain the normalized admittance \\(y\\) with Smith chart by simply rotate the corresponding \\(z\\) with \\(180\\degree\\) .","title":"Transmission Line"},{"location":"Electromagnetics/Transmission%20Line/#transmission+line","text":"","title":"Transmission Line"},{"location":"Electromagnetics/Transmission%20Line/#transmission-line+equations","text":"First define - \\(\u212d\\) : inductance per unit length - \\(\ud835\udd0f\\) : inductance per unit length","title":"Transmission-Line Equations"},{"location":"Electromagnetics/Transmission%20Line/#lossless+transmission+lines","text":"\\[ \\left\\{ \\begin{align} \\frac{\\partial V}{\\partial z}&=-\ud835\udd0f\\frac{\\partial I}{\\partial t} \\\\\\\\ \\frac{\\partial I}{\\partial z}&= -\u212d\\frac{\\partial V}{\\partial t} \\end{align}\\right. \\] tip These equation can be easily derived from the definition of capacitance and capacitance in electric circuit theorem \\[ \\begin{gather} v = L\\frac{di}{dt} \\\\\\\\ i= C\\frac{dv}{dt} \\end{gather} \\] Similar to the derivation in EM wave equation, we can obtain the wave equation form \\[ \\left\\{ \\begin{align} \\frac{\\partial^{2} V}{\\partial z^{2}} &= \ud835\udd0f\u212d\\frac{\\partial^{2} V}{\\partial t^{2}} \\\\\\\\ \\frac{\\partial^{2}I}{\\partial z^{2}} &= \ud835\udd0f\u212d\\frac{\\partial^{2} I}{\\partial t^{2}} \\end{align}\\right. \\] Thus we have the wave velocity \\[ \\begin{gather} u = \\frac{1}{\\sqrt{\ud835\udd0f\u212d}} \\end{gather} \\] Comparing to the velocity of EM wave, it tell us the important relation that \\[ \\begin{gather} \\boxed{ \ud835\udd0f\u212d = \\mu\\varepsilon } \\end{gather} \\] Furthermore, considering the phasor form of transmission-line equation, we have \\[ \\begin{align} \\frac{d^{2}V_s}{dz^{2}} = (j\\beta)^{2}\\,V_s \\\\\\\\ \\frac{d^{2}I_s}{dz^{2}} = (j\\beta)^{2}\\,I_s \\end{align} \\] in which \\[ \\begin{gather} (j\\beta)^{2} = (j\\omega)^{2}\ud835\udd0f\u212d \\\\\\\\ \\implies \\boxed{ \\beta = \\omega\\sqrt{\ud835\udd0f\u212d} = \\frac{\\omega}{u} } \\end{gather} \\]","title":"Lossless Transmission Lines"},{"location":"Electromagnetics/Transmission%20Line/#general+solutions","text":"\\[ \\begin{align} V(z, t) &= V^{+}(z-ut)+V^{-}(z+ut) \\\\\\\\ I(z, t) &= I^{+}(z-ut)+I^{-}(z+ut) \\end{align} \\] Similarly, we have the characteristic impedance \\(Z_0\\) \\[ \\begin{gather} Z_0 = \\sqrt\\frac{\ud835\udd0f}{\u212d}=\\frac{V^{+}}{I^{+}}=-\\frac{V^{-}}{I^{-}} \\end{gather} \\]","title":"General Solutions"},{"location":"Electromagnetics/Transmission%20Line/#general+transmission+lines","text":"Revise the transmission-line equation in the lossless case into \\[ \\left\\{ \\begin{align} -\\frac{\\partial V}{\\partial z}&=RI+L\\frac{\\partial I}{\\partial t} \\\\\\\\ -\\frac{\\partial I}{\\partial z}&= GV+C\\frac{\\partial V}{\\partial t} \\end{align}\\right. \\qquad\\overset{\ud835\udd09ourier}{\\iff}\\qquad \\left\\{ \\begin{aligned} \\frac{d V_s}{d z}&=-(R+ j\\omega L)I_s \\\\\\\\ \\frac{d \\,I_s}{d z}&= -(G+j\\omega C)V_s \\end{aligned}\\right. \\] Also, in the form of wave equations \\[ \\begin{gather} \\frac{d^{2}V_s}{dz^{2}} = \\gamma^{2}V_s \\\\\\\\ \\frac{d^{2}I_s}{dz^{2}} = \\gamma^{2}I_s \\end{gather} \\] in which \\(\\gamma\\) is also called as propagation constant . \\[ \\begin{gather} \\gamma = \\sqrt{(R+j\\omega L)(G+j\\omega C)} = \\alpha+j\\beta \\end{gather} \\]","title":"General Transmission LInes"},{"location":"Electromagnetics/Transmission%20Line/#general+solutions_1","text":"solve the wave equations above \\[ \\begin{align} V_s &= V_0^{+}e^{-\\gamma z}+V_0^{-}e^{\\gamma z} \\\\\\\\ I_s &= I_0^{+}e^{-\\gamma z}+I_0^{-}e^{\\gamma z} \\\\\\\\ &= \\frac{V_0^{+}}{Z_0}e^{-\\gamma z}-\\frac{V_0^{+}}{Z_0}e^{\\gamma z} \\end{align} \\] and the corresponding time domain form \\[ \\begin{align} V(z, t) &= |V_0^{+}|e^{-\\alpha z}\\cos(\\omega t- \\beta z + \\phi^{+}) + |V_0^{-}|e^{\\alpha z}\\cos(\\omega t+ \\beta z + \\phi^{-}) \\\\\\\\ I(z, t) &= |I_0^{+}|e^{-\\alpha z}\\cos(\\omega t- \\beta z + \\varphi^{+}) + |I_0^{-}|e^{\\alpha z}\\cos(\\omega t+ \\beta z + \\varphi^{-}) \\end{align} \\] power attenuation \\[ \\begin{align} \\text{attenuation const} &= \\alpha \\quad\\text{(NP/m)} \\\\\\\\ \\text{power attenuation} &= 20 \\log{e^{\\alpha}}\\quad\\text{(dB/m)} \\end{align} \\] thus, \\[ \\begin{gather} 1\\,\\text{NP} = 20\\log{e^{1}} = 8.69 \\, \\text{dB} \\end{gather} \\]","title":"General Solutions"},{"location":"Electromagnetics/Transmission%20Line/#power","text":"time-average power \\[ \\begin{align} P(z) &= \\frac{1}{2}\\text{Re}\\bigg[VI^{*}\\bigg] \\\\\\\\ &= \\frac{1}{2}\\text{Re}\\bigg[V_0^{+}(1+\\Gamma)\\left(\\frac{V_0^{+}}{Z_0}\\right)^{*}(1-\\Gamma^{*})\\bigg] \\\\\\\\ &= \\frac{1}{2}\\text{Re}\\bigg[\\frac{{V_0^{+}}^{2}}{Z_0}(1+\\Gamma)(1-\\Gamma^{*})\\bigg] \\\\\\\\ &=\\frac{1}{2}\\frac{\\left|V_0^{+}\\right|^{2}}{Z_0}(1-|\\Gamma|^{2}) \\tag 6 \\end{align} \\] ^equ-6 \\[ \\implies \\begin{dcases} P^{+}=\\frac{1}{2}\\left|V_0^{+}\\right||I_0^{+}|\\,e^{+ 2\\alpha z} \\\\\\\\ P^{-}=\\frac{1}{2}\\left|V_0^{-}\\right||I_0^{-}|\\,e^{- 2\\alpha z} \\end{dcases} \\] power loss \\[ \\begin{gather} \\text{PL} =P(0)-P(z) \\end{gather} \\]","title":"Power"},{"location":"Electromagnetics/Transmission%20Line/#distortionless","text":"conditions \\[ \\begin{gather} \\frac{R}{L} = \\frac{G}{C} \\end{gather} \\] thus, we have the propagation constant \\(\\gamma\\) \\[ \\begin{align} \\gamma &= \\sqrt{(R+j\\omega L)(G+ j\\omega C)} \\\\\\\\ &= \\sqrt{LC} \\sqrt{\\left(\\frac{R}{L} +j\\omega\\right)\\left(\\frac{G}{C} +j\\omega\\right)} \\\\\\\\ &= \\sqrt{LC} \\left(\\frac{R}{L} +j\\omega\\right) \\\\\\\\ &= \\sqrt{RG} + j\\omega\\sqrt{LC} \\end{align} \\] and the characteristic impedance \\(Z_0\\) \\[ \\begin{align} Z_0 &= \\sqrt{\\frac{R+j\\omega L}{G+ j\\omega C}} \\\\\\\\ &= \\sqrt {\\frac{R(1+j\\omega\\frac{L}{R})}{G(1+j\\omega(\\frac{C}{G}))}} \\\\\\\\ &= \\sqrt{\\frac{R}{G}} = \\sqrt{\\frac{L}{C}} \\end{align} \\]","title":"Distortionless"},{"location":"Electromagnetics/Transmission%20Line/#standing+wave+ratio","text":"aka \\(\\text{SWR}\\)","title":"Standing Wave Ratio"},{"location":"Electromagnetics/Transmission%20Line/#reflection","text":"voltage reflection coefficient \\[ \\begin{gather} \\Gamma(z) = \\frac{V^{-}(z)}{V^{+}(z)} =\\frac{V_0^{-}e^{+\\gamma z}}{V_0^{+}e^{-\\gamma z}} =\\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma z} \\end{gather} \\] current reflection coefficient \\[ \\begin{gather} \\Gamma_I(z)=\\frac{I^{-}}{I^{+}}=-\\frac{V^{-}}{V^{+}}=-\\Gamma(z) \\end{gather} \\] \\(\\Gamma_L\\) Let's define the impedance from load \\(Z_L=V_L/I_L\\) \\[ \\begin{gather} V_0^{+}e^{-\\gamma \u2113}+V_0^{-}e^{+\\gamma\u2113}=\\frac{Z_L}{Z_0}(V_0^{+}e^{-\\gamma\u2113}-V_0^{-}e^{+\\gamma \u2113}) \\\\\\\\ \\implies (Z_0-Z_L)V_0^{+}e^{-\\gamma \u2113}=-(Z_L+Z_0)V_0^{-}e^{+\\gamma \u2113} \\\\\\\\ \\implies \\boxed{ \\Gamma_L=\\frac{V_0^{-}e^{+\\gamma \u2113 }}{V_0^{+}e^{-\\gamma \u2113}}=\\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma \u2113 }=\\frac{Z_L-Z_0}{Z_L+Z_0} } \\tag{2} \\end{gather} \\] ^equ-2 We can further write the reflection coefficient from load at \\(d\\) as \\[ \\begin{align} \\Gamma(d)= \\Gamma_{L}\\,e^{-2\\gamma d} \\tag 3 \\end{align} \\] ^equ-3 tips from \\(\\text{equation (3-1)}\\) , \\[ \\begin{gather} Z(z) = Z_0\\frac{1+\\Gamma}{1-\\Gamma} \\\\\\\\ \\implies Z(z) - Z(z)\\,\\Gamma = Z_0+Z_0\\,\\Gamma \\\\\\\\ \\implies \\boxed{ \\Gamma(z) = \\frac{Z(z) - Z_0}{Z(z) + Z_0} } \\end{gather} \\] we can also derive \\(\\text{equation (2)}\\) by virtue of this equation \\[ \\begin{gather} \\Gamma_L = \\Gamma(z=\u2113) = \\Gamma(d=0) = \\frac{Z(z=\u2113)-Z_0}{Z(z=\u2113)+Z_0} \\end{gather} \\] Furthermore, at the generator-end (i.e. \\(z=0\\) ), we have \\[ \\begin{gather} \\Gamma(z=0)=\\Gamma(d=\u2113)=\\frac{Z_{in}-Z_0}{Z_{in} + Z_0} \\end{gather} \\]","title":"Reflection"},{"location":"Electromagnetics/Transmission%20Line/#impedance","text":"Line impedance at \\(z\\) , \\(Z(z)\\) \\[ \\begin{align} Z(z) &= \\frac{V_s}{I_s} \\\\\\\\ &= Z_0\\frac{V_0^{+}e^{-\\gamma z}+V_0^{-}e^{+\\gamma z}}{V_0^{+}e^{-\\gamma z}-V_0^{-}e^{+\\gamma z}} = Z_0\\frac{1+\\displaystyle \\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma z}}{1-\\displaystyle \\frac{V_0^{-}}{V_0^{+}}e^{+2\\gamma z}} \\\\\\\\ &= Z_0\\frac{1+\\Gamma(z)}{1-\\Gamma(z)} \\tag{3-1} \\end{align} \\] ^equ-3-1 impedance at load \\(Z_L\\) \\[ \\begin{align} Z_L &= Z(z=\u2113) = Z_0\\frac{1+\\Gamma(z=\u2113)}{1-\\Gamma(z=\u2113)} \\\\\\\\ &= Z_0\\frac{1+\\Gamma_L}{1-\\Gamma_L} \\end{align} \\] input impedance \\(Z_{in}\\) \\[ \\begin{align} Z_{in} &= Z(z=0) = Z_0\\frac{1+\\Gamma(z=0)}{1-\\Gamma(z=0)} \\\\\\\\ &= Z_0\\,\\frac{1+ \\displaystyle \\frac{V_0^{-}}{V_0^{+}}}{1- \\displaystyle \\frac{V_0^{-}}{V_0^{+}}} \\\\\\\\ &= Z_0\\,\\frac{1+ \\Gamma_L\\,e^{-2\\gamma \u2113}}{1- \\Gamma_L\\,e^{-2\\gamma \u2113}} \\end{align} \\] from \\(\\text{equation (2)}\\) we have \\[ \\begin{align} Z_{in} &= Z_0\\,\\frac{1+ \\Gamma_L\\,e^{-2\\gamma \u2113}}{1- \\Gamma_L\\,e^{-2\\gamma \u2113}} \\\\\\\\ &= Z_0\\,\\frac{1+ \\displaystyle \\frac{Z_L-Z_0}{Z_L+Z_0}\\,e^{-2\\gamma \u2113}}{1- \\displaystyle \\frac{Z_L-Z_0}{Z_L+Z_0}\\,e^{-2\\gamma \u2113}} \\\\\\\\ &= Z_0\\,\\frac{Z_L+Z_0+ (Z_L-Z_0)\\,e^{-2\\gamma \u2113}}{Z_L+Z_0- (Z_L-Z_0)\\,e^{-2\\gamma \u2113}} \\\\\\\\ &= Z_0\\,\\frac{Z_L(1+e^{-2\\gamma \u2113})+Z_0(1-e^{-2\\gamma \u2113})}{Z_L(1-e^{-2\\gamma \u2113})+Z_0(1+e^{-2\\gamma \u2113})} \\\\\\\\ &= Z_0\\,\\frac{Z_L+Z_0\\,\\tanh{\\gamma\u2113}}{Z_L\\,\\tanh{\\gamma\u2113}+Z_0} \\\\\\\\ &= \\boxed{ Z_0\\,\\frac{Z_L+Z_0\\,\\tanh{\\gamma\u2113}}{Z_0+Z_L\\,\\tanh{\\gamma\u2113}} } \\\\\\\\ &\\xlongequal{\\alpha = 0} Z_0\\,\\frac{Z_L+jZ_0\\,\\tan{\\beta\u2113}}{Z_0+jZ_L\\,\\tan{\\beta\u2113}} \\end{align} \\]","title":"impedance"},{"location":"Electromagnetics/Transmission%20Line/#complex+form","text":"As usual, we can represent \\(\\Gamma\\) with complex coordinate. \\[ \\begin{gather} \\Gamma = |\\Gamma |\\,\\angle{\\Gamma} \\end{gather} \\] Consider \\(\\text{equation (3)}\\) , we further have \\[ \\begin{align} \\boxed{ \\Gamma=\\Gamma_L\\,e^{-2\\alpha d}\\,e^{-j2\\beta d} } \\end{align} \\] Besides, we can rewrite \\(V_s, I_s\\) as \\[ \\begin{align} |V_s| &= \\left|V^{+}\\right||1+\\Gamma| \\\\\\\\ |I_s| &= \\left|I^{+}\\right||1-\\Gamma| \\end{align} \\]","title":"Complex form"},{"location":"Electromagnetics/Transmission%20Line/#standing+wave+ratio+for+lossless","text":"\\[ \\begin{align} \\text{VSWR} &\\equiv \\frac{V_{max}}{V_{min}} = \\frac{|V_0^{+}||1+\\Gamma|_{max}}{|V_0^{+}||1+\\Gamma|_{min}} \\\\\\\\ &= \\frac{1+ |\\Gamma|}{1- |\\Gamma|} \\xlongequal{\\alpha\\,=\\,0}\\frac{1+ |\\Gamma_L|}{1- |\\Gamma_L|} \\end{align} \\] and similarly for \\(\\text{ISWR}\\) , \\[ \\begin{gather} \\frac{I_{max}}{I_{min}}=\\frac{V_{max}}{V_{min}}=\\text{SWR} \\end{gather} \\] according to \\(\\text{equation (5)}\\) , \\[ \\begin{align} z=\\frac{1+\\Gamma}{1-\\Gamma}=r+jx \\end{align} \\] we can obtain \\(\\text{SWR}\\) by picking the point that \\(\\angle\\Gamma = 0\\) , then \\[ \\begin{align} \\text{SWR} = \\frac{1+|\\Gamma|}{1-|\\Gamma|}= r+j0=r \\end{align} \\]","title":"Standing Wave Ratio for Lossless"},{"location":"Electromagnetics/Transmission%20Line/#smith+chart","text":"Smith chart help us quickly obtain the normalized impedance \\(Z\\) w.r.t \\(Z_0\\) . \\[ \\begin{align} Z= z\\times Z_0 = (r+jx)\\times Z_0 \\end{align} \\] then, we can have the normalized impedance \\(z\\) \\[ \\begin{align} z=r+jx= \\frac{Z}{Z_0}=\\frac{1+\\Gamma}{1-\\Gamma} \\tag 5 \\end{align} \\] ^equ-5 \\(r\\) -circles \\(x\\) -circles","title":"Smith Chart"},{"location":"Electromagnetics/Transmission%20Line/#admittance+chart","text":"for the admittance of transmission line, we can easily obtain the relationship of admittance and impedance. \\[ \\begin{gather} y=\\frac{Y}{Y_0}=\\frac{1}{z}=\\frac{1-\\Gamma}{1+\\Gamma} \\\\\\\\ \\implies y=z\\,e^{-j\\pi} \\tag 4 \\end{gather} \\] ^equ-4 the \\(\\text{equation (4)}\\) tell us that we can obtain the normalized admittance \\(y\\) with Smith chart by simply rotate the corresponding \\(z\\) with \\(180\\degree\\) .","title":"Admittance Chart"},{"location":"Electromagnetics/Vector%20Calculus/","text":"Gradient \u00b6 Cartesian \u00b6 \\[ \\begin{gather} \\nabla V = \\left<\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z}\\right> V \\end{gather} \\] cylindrical \u00b6 \\[ \\begin{gather} \\nabla V = \\left<\\frac{\\partial }{\\partial \\rho}, \\frac{1}{\\rho}\\frac{\\partial }{\\partial \\phi}, \\frac{\\partial }{\\partial z}\\right> V \\end{gather} \\] spherical \u00b6 \\[ \\begin{gather} \\nabla V = \\left<\\frac{\\partial }{\\partial r}, \\frac{1}{r}\\frac{\\partial }{\\partial \\theta}, \\frac{1}{r\\sin \\theta}\\frac{\\partial }{\\partial \\phi}\\right> V \\end{gather} \\] Divergence \u00b6 Definition \u00b6 hint: Stoke's Theorem \\[ \\begin{gather} \\nabla \\cdot \\vec A \\equiv \\lim_{v\\to 0}\\frac{\\oint_S{\\vec A \\cdot d\\vec S}}{\\int_vdv} \\end{gather} \\] Physical Interpretation \u00b6 flux per unit volume measure of outgoingness of vector (\u767c\u6563\u50be\u5411) \\(\\nabla \\cdot \\vec A > 0\\) (source) \\(\\nabla \\cdot \\vec A < 0\\) (sink) proof: https://en.wikipedia.org/wiki/Del_in_cylindrical_and_spherical_coordinates Cartesian \u00b6 \\[ \\begin{gather} \\nabla \\cdot \\vec D = \\frac{\\partial }{\\partial x}D_x + \\frac{\\partial }{\\partial y}D_y + \\frac{\\partial }{\\partial z}D_z \\end{gather} \\] cylindrical \u00b6 \\[ \\begin{gather} \\nabla \\cdot \\vec D = \\frac{1}{\\rho}\\frac{\\partial }{\\partial \\rho}(\\rho D_\\rho) + \\frac{1}{\\rho}\\frac{\\partial }{\\partial \\phi}D_\\phi + \\frac{\\partial }{\\partial z}D_z \\end{gather} \\] spherical \u00b6 \\[ \\begin{gather} \\nabla \\cdot \\vec D = \\frac{1}{r^{2}}\\frac{\\partial }{\\partial r}(r^{2}D_r) + \\frac{1}{r\\sin\\theta}\\frac{\\partial }{\\partial \\theta}(D_\\theta \\sin\\theta) + \\frac{1}{r\\sin\\theta}\\frac{\\partial }{\\partial \\phi}D_\\phi \\end{gather} \\] Laplacian \u00b6 \\[ \\begin{gather} \\nabla^2 V \\equiv \\nabla \\cdot (\\nabla V) \\\\\\\\ \\nabla^2 \\vec A \\equiv \\nabla (\\nabla \\cdot \\vec A) - \\nabla \\times \\nabla \\times \\vec A \\end{gather} \\] cylindrical \\[ \\begin{gather} \\nabla^{2}V=\\frac{1}{\\rho}\\frac{\\partial }{\\partial \\rho }\\left(\\rho V_\\rho\\right)+\\frac{1}{\\rho^{2}}V_{\\phi\\phi}+V_{zz} \\end{gather} \\] spherical \\[ \\begin{gather} \\nabla^{2}V=\\frac{1}{r^{2}}\\frac{\\partial }{\\partial r }\\left(r^{2} V_r\\right)+\\frac{1}{r^{2}\\sin\\theta}\\frac{\\partial }{\\partial \\phi}\\left(\\sin\\theta\\,V_\\theta\\right)+\\frac{1}{r^{2}\\sin^{2}\\theta}\\left(V_{\\phi\\phi}\\right) \\end{gather} \\] Curl \u00b6 Definition \\[ \\begin{gather} (\\nabla \\times \\vec A)\\cdot \\hat{a_n} = \\lim_{\\Delta S\\to0}\\frac{\\oint_c{\\vec A \\cdot d \\vec l}}{\\Delta S} \\end{gather} \\] Cartesian \\[ \\nabla \\times \\vec A= \\left| \\begin{matrix} \\hat {a_x} & \\hat {a_y} & \\hat {a_z} \\\\ \\frac{\\partial }{\\partial x} &\\frac{\\partial }{\\partial y} & \\frac{\\partial }{\\partial z} \\\\ A_x & A_y & A_z \\end{matrix}\\right| \\] cylindrical \\[ \\nabla \\times \\vec A= \\frac{1}{\\rho} \\left| \\begin{matrix} \\hat {a_\\rho} & \\rho\\hat {a_\\phi} & \\hat {a_z} \\\\ \\frac{\\partial }{\\partial \\rho} &\\frac{\\partial }{\\partial \\phi} & \\frac{\\partial }{\\partial z} \\\\ A_\\rho & \\rho A_\\phi & A_z \\end{matrix}\\right| \\] spherical \\[ \\nabla \\times \\vec A= \\frac{1}{r^{2}\\sin\\theta} \\left| \\begin{matrix} \\hat {a_r} & r\\hat {a_\\theta} & r\\sin\\theta\\,\\hat {a_\\phi} \\\\ \\frac{\\partial }{\\partial r} &\\frac{\\partial }{\\partial \\theta} & \\frac{\\partial }{\\partial \\phi} \\\\ A_r & rA_\\theta & r\\sin\\theta A_\\phi \\end{matrix}\\right| \\] Identities \u00b6 \\[ \\begin{gather} \\vec A \\times \\left(\\vec B \\times \\vec C\\right)=\\left(\\vec A \\cdot \\vec C\\right)\\vec B -\\left(\\vec A \\cdot \\vec B\\right)\\vec C \\\\\\\\\\\\ \\nabla \\cdot \\left(\\vec A \\times \\vec B\\right)=\\left(\\nabla \\times \\vec A\\right)\\cdot \\vec B - \\left(\\nabla \\times \\vec B\\right)\\cdot \\vec A \\\\\\\\\\\\ \\nabla \\times \\left(\\vec A \\times \\vec B\\right)=\\vec A\\left(\\nabla \\cdot \\vec B\\right)- \\vec B\\left(\\nabla \\cdot \\vec A\\right)+\\left(\\vec B \\cdot \\nabla\\right)\\vec A - \\left(\\vec A\\cdot \\nabla\\right)\\vec B \\end{gather} \\]","title":"Vector Calculus"},{"location":"Electromagnetics/Vector%20Calculus/#gradient","text":"","title":"Gradient"},{"location":"Electromagnetics/Vector%20Calculus/#cartesian","text":"\\[ \\begin{gather} \\nabla V = \\left<\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z}\\right> V \\end{gather} \\]","title":"Cartesian"},{"location":"Electromagnetics/Vector%20Calculus/#cylindrical","text":"\\[ \\begin{gather} \\nabla V = \\left<\\frac{\\partial }{\\partial \\rho}, \\frac{1}{\\rho}\\frac{\\partial }{\\partial \\phi}, \\frac{\\partial }{\\partial z}\\right> V \\end{gather} \\]","title":"cylindrical"},{"location":"Electromagnetics/Vector%20Calculus/#spherical","text":"\\[ \\begin{gather} \\nabla V = \\left<\\frac{\\partial }{\\partial r}, \\frac{1}{r}\\frac{\\partial }{\\partial \\theta}, \\frac{1}{r\\sin \\theta}\\frac{\\partial }{\\partial \\phi}\\right> V \\end{gather} \\]","title":"spherical"},{"location":"Electromagnetics/Vector%20Calculus/#divergence","text":"","title":"Divergence"},{"location":"Electromagnetics/Vector%20Calculus/#definition","text":"hint: Stoke's Theorem \\[ \\begin{gather} \\nabla \\cdot \\vec A \\equiv \\lim_{v\\to 0}\\frac{\\oint_S{\\vec A \\cdot d\\vec S}}{\\int_vdv} \\end{gather} \\]","title":"Definition"},{"location":"Electromagnetics/Vector%20Calculus/#physical+interpretation","text":"flux per unit volume measure of outgoingness of vector (\u767c\u6563\u50be\u5411) \\(\\nabla \\cdot \\vec A > 0\\) (source) \\(\\nabla \\cdot \\vec A < 0\\) (sink) proof: https://en.wikipedia.org/wiki/Del_in_cylindrical_and_spherical_coordinates","title":"Physical Interpretation"},{"location":"Electromagnetics/Vector%20Calculus/#cartesian_1","text":"\\[ \\begin{gather} \\nabla \\cdot \\vec D = \\frac{\\partial }{\\partial x}D_x + \\frac{\\partial }{\\partial y}D_y + \\frac{\\partial }{\\partial z}D_z \\end{gather} \\]","title":"Cartesian"},{"location":"Electromagnetics/Vector%20Calculus/#cylindrical_1","text":"\\[ \\begin{gather} \\nabla \\cdot \\vec D = \\frac{1}{\\rho}\\frac{\\partial }{\\partial \\rho}(\\rho D_\\rho) + \\frac{1}{\\rho}\\frac{\\partial }{\\partial \\phi}D_\\phi + \\frac{\\partial }{\\partial z}D_z \\end{gather} \\]","title":"cylindrical"},{"location":"Electromagnetics/Vector%20Calculus/#spherical_1","text":"\\[ \\begin{gather} \\nabla \\cdot \\vec D = \\frac{1}{r^{2}}\\frac{\\partial }{\\partial r}(r^{2}D_r) + \\frac{1}{r\\sin\\theta}\\frac{\\partial }{\\partial \\theta}(D_\\theta \\sin\\theta) + \\frac{1}{r\\sin\\theta}\\frac{\\partial }{\\partial \\phi}D_\\phi \\end{gather} \\]","title":"spherical"},{"location":"Electromagnetics/Vector%20Calculus/#laplacian","text":"\\[ \\begin{gather} \\nabla^2 V \\equiv \\nabla \\cdot (\\nabla V) \\\\\\\\ \\nabla^2 \\vec A \\equiv \\nabla (\\nabla \\cdot \\vec A) - \\nabla \\times \\nabla \\times \\vec A \\end{gather} \\] cylindrical \\[ \\begin{gather} \\nabla^{2}V=\\frac{1}{\\rho}\\frac{\\partial }{\\partial \\rho }\\left(\\rho V_\\rho\\right)+\\frac{1}{\\rho^{2}}V_{\\phi\\phi}+V_{zz} \\end{gather} \\] spherical \\[ \\begin{gather} \\nabla^{2}V=\\frac{1}{r^{2}}\\frac{\\partial }{\\partial r }\\left(r^{2} V_r\\right)+\\frac{1}{r^{2}\\sin\\theta}\\frac{\\partial }{\\partial \\phi}\\left(\\sin\\theta\\,V_\\theta\\right)+\\frac{1}{r^{2}\\sin^{2}\\theta}\\left(V_{\\phi\\phi}\\right) \\end{gather} \\]","title":"Laplacian"},{"location":"Electromagnetics/Vector%20Calculus/#curl","text":"Definition \\[ \\begin{gather} (\\nabla \\times \\vec A)\\cdot \\hat{a_n} = \\lim_{\\Delta S\\to0}\\frac{\\oint_c{\\vec A \\cdot d \\vec l}}{\\Delta S} \\end{gather} \\] Cartesian \\[ \\nabla \\times \\vec A= \\left| \\begin{matrix} \\hat {a_x} & \\hat {a_y} & \\hat {a_z} \\\\ \\frac{\\partial }{\\partial x} &\\frac{\\partial }{\\partial y} & \\frac{\\partial }{\\partial z} \\\\ A_x & A_y & A_z \\end{matrix}\\right| \\] cylindrical \\[ \\nabla \\times \\vec A= \\frac{1}{\\rho} \\left| \\begin{matrix} \\hat {a_\\rho} & \\rho\\hat {a_\\phi} & \\hat {a_z} \\\\ \\frac{\\partial }{\\partial \\rho} &\\frac{\\partial }{\\partial \\phi} & \\frac{\\partial }{\\partial z} \\\\ A_\\rho & \\rho A_\\phi & A_z \\end{matrix}\\right| \\] spherical \\[ \\nabla \\times \\vec A= \\frac{1}{r^{2}\\sin\\theta} \\left| \\begin{matrix} \\hat {a_r} & r\\hat {a_\\theta} & r\\sin\\theta\\,\\hat {a_\\phi} \\\\ \\frac{\\partial }{\\partial r} &\\frac{\\partial }{\\partial \\theta} & \\frac{\\partial }{\\partial \\phi} \\\\ A_r & rA_\\theta & r\\sin\\theta A_\\phi \\end{matrix}\\right| \\]","title":"Curl"},{"location":"Electromagnetics/Vector%20Calculus/#identities","text":"\\[ \\begin{gather} \\vec A \\times \\left(\\vec B \\times \\vec C\\right)=\\left(\\vec A \\cdot \\vec C\\right)\\vec B -\\left(\\vec A \\cdot \\vec B\\right)\\vec C \\\\\\\\\\\\ \\nabla \\cdot \\left(\\vec A \\times \\vec B\\right)=\\left(\\nabla \\times \\vec A\\right)\\cdot \\vec B - \\left(\\nabla \\times \\vec B\\right)\\cdot \\vec A \\\\\\\\\\\\ \\nabla \\times \\left(\\vec A \\times \\vec B\\right)=\\vec A\\left(\\nabla \\cdot \\vec B\\right)- \\vec B\\left(\\nabla \\cdot \\vec A\\right)+\\left(\\vec B \\cdot \\nabla\\right)\\vec A - \\left(\\vec A\\cdot \\nabla\\right)\\vec B \\end{gather} \\]","title":"Identities"},{"location":"Electromagnetics/Wave%20Guide/","text":"Parallel Plate Waveguide \u00b6 TE Waves \u00b6 \\[ \\begin{gather} \\vec E = E_0 \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_y} \\\\\\\\ \\vec H = - \\frac{E_0}{\\eta}\\sin{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} +\\frac{E_0}{\\eta}\\cos{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\end{gather} \\] cutoff wavelength \\[ \\begin{gather} a = m \\, \\frac{\\lambda_x}{2}=\\frac{m\\,\\displaystyle \\frac{\\lambda}{\\cos\\theta}}{2} \\\\\\\\ \\implies \\lambda =\\frac{2a}{m}\\cos\\theta \\\\\\\\ \\implies \\lambda_c = \\big[\\lambda\\big]_{\\cos\\theta=1} =\\frac{2a}{m} \\\\\\\\ \\implies \\cos\\theta = \\frac{\\lambda }{\\lambda_c} \\end{gather} \\] guide wavelength (in \\(z\\) direction) \\[ \\begin{gather} \\lambda_g = \\frac{2\\pi}{k_z}=\\frac{2\\pi}{k\\sin\\theta}=\\frac{\\lambda}{\\sin\\theta} \\\\\\\\ \\implies \\sin\\theta=\\frac{\\lambda}{\\lambda_g} \\end{gather} \\] by the \\(\\lambda_c\\) and \\(\\lambda_g\\) we can also write the equations as \\[ \\begin{gather} \\vec E = E_0 \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_y} \\\\\\\\ \\vec H = - \\frac{E_0}{\\eta}\\frac{\\lambda}{\\lambda_g} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} +\\frac{E_0}{\\eta}\\frac{\\lambda}{\\lambda_c} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\end{gather} \\] TM Waves \u00b6 \\[ \\begin{gather} \\vec H = H_0 \\cos{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_y} \\\\ \\end{gather} \\] \\[ \\begin{align} \\vec E &= \\eta H_0\\sin{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} + \\eta H_0\\cos{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\\\\\\\ &= \\eta H_0\\frac{\\lambda}{\\lambda_g} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} + \\eta H_0\\frac{\\lambda }{\\lambda_c} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\end{align} \\] Rectangular Waveguide \u00b6 \\(\\gamma\\) \\[ \\begin{gather} k^{2} = k_x^{2} + k_y^{2} + k_z^{2} = k_x^{2} + k_y^{2} + (-j\\gamma)^{2} \\\\\\\\ \\implies \\gamma^{2} = k_t^{2} - k^{2} \\end{gather} \\] TM mode \u00b6 tip notice that we assuming perfect conductors. thus, \\[ \\begin{gather} \\vec E_n = 0 \\\\\\\\ \\implies E_{zs}=0, \\qquad\\text{at boundary} \\end{gather} \\] \\[ \\begin{gather} E_{zs} = E_0\\sin{\\left(\\frac{m\\pi x}{a}\\right)}\\sin{\\left(\\frac{n\\pi y}{b}\\right)}\\,e^{-\\gamma z} \\end{gather} \\] TE mode \u00b6 \\[ \\begin{gather} H_{zs} = H_0 \\cos{\\left(\\frac{m\\pi x}{a}\\right)}\\cos{\\left(\\frac{n\\pi y}{b}\\right)}\\,e^{-\\gamma z} \\end{gather} \\] Resonators \u00b6 TM \\[ \\begin{align} E_{zs} &= \\frac{E_0}{2}\\sin{\\left(\\frac{m\\pi x}{a}\\right)}\\sin{\\left(\\frac{n\\pi y}{b}\\right)}\\left(e^{-jk_z z}+e^{jk_z z}\\right) \\\\\\\\ &= E_0\\sin{\\left(\\frac{m\\pi x}{a}\\right)}\\sin{\\left(\\frac{n\\pi y}{b}\\right)}\\cos{\\left(\\frac{p \\pi z}{d}\\right)} \\end{align} \\] TE \\[ \\begin{align} H_{zs} &= H_0\\cos{\\left(\\frac{m\\pi x}{a}\\right)}\\cos{\\left(\\frac{n\\pi y}{b}\\right)}\\sin{\\left(\\frac{p\\pi z}{d}\\right)} \\end{align} \\] Q Factors \u00b6 \\[ \\begin{gather} Q = 2\\pi\\frac{W}{P_L\\,T}= \\frac{\\omega W}{P_L} \\end{gather} \\] recall \\[ \\begin{gather} Q = \\frac{X}{R} = \\frac{\\omega L}{R} = \\frac{\\omega I^{2}L}{I^{2}R} = \\frac{\\omega W}{P} \\end{gather} \\] Dominant Mode \u00b6 the mode with lowest \\(f_c\\) (i.e. largest \\(\\lambda_c\\) ). Cylindrical \u00b6 \\(\\beta_c\\) \\[ \\begin{gather} k^{2} = \\beta_c^{2} + \\beta^{2} \\end{gather} \\] TM mode \u00b6 \\[ \\begin{gather} E_{zs} = J_n\\big[\\beta_c \\rho\\big] \\,(A_n \\cos{n\\phi} + B_n\\sin{n\\phi})e^{\\mp j\\beta z} \\end{gather} \\] TE mode \u00b6 \\[ \\begin{gather} H_{zs} = J_n\\big[\\beta_c \\rho\\big](A_n\\cos {n\\phi} + B_n \\sin{n\\phi})e^{\\mp j \\beta z} \\end{gather} \\] Dielectric Slab Waveguide \u00b6 cutoff wavelength \\[ \\begin{gather} \\lambda_c = \\frac{2d\\sqrt{\\varepsilon_{r1} - \\varepsilon_{r2}}}{m}\\,\\qquad m=0, 1, 2,\\dots \\end{gather} \\] thinking \\[ \\begin{gather} k\\cdot 2d \\cdot \\cos{\\theta_i} - 2\\angle{\\Gamma} = 2m\\pi \\\\\\\\ \\implies \\frac{2\\pi\\sqrt{\\varepsilon_{r1}}}{\\lambda_0}\\cdot d \\cdot \\cos{\\theta_i} - \\angle{\\Gamma} = m\\pi \\\\\\\\ \\implies \\angle{\\Gamma} = k\\cdot d \\cdot \\cos{\\theta_i} - m\\pi \\end{gather} \\] when cutoff \\(\\angle{\\Gamma} = 0\\) \\[ \\begin{gather} k_c \\,d\\cos{\\theta_i} = m\\pi \\\\\\\\ \\implies \\frac{2\\pi\\sqrt{\\varepsilon_{r1}}}{\\lambda_c}\\,d\\cos{\\theta_i} = m\\pi \\\\\\\\ \\frac{2\\pi\\sqrt{\\varepsilon_{r1}}}{\\lambda_c}\\,d\\sqrt{1-\\frac{\\varepsilon_2}{\\varepsilon_1}} = m\\pi \\\\\\\\ \\implies \\lambda_c = \\frac{2d\\sqrt{\\varepsilon_{r1} - \\varepsilon_{r2}}}{m} \\end{gather} \\]","title":"Wave Guide"},{"location":"Electromagnetics/Wave%20Guide/#parallel+plate+waveguide","text":"","title":"Parallel Plate Waveguide"},{"location":"Electromagnetics/Wave%20Guide/#te+waves","text":"\\[ \\begin{gather} \\vec E = E_0 \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_y} \\\\\\\\ \\vec H = - \\frac{E_0}{\\eta}\\sin{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} +\\frac{E_0}{\\eta}\\cos{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\end{gather} \\] cutoff wavelength \\[ \\begin{gather} a = m \\, \\frac{\\lambda_x}{2}=\\frac{m\\,\\displaystyle \\frac{\\lambda}{\\cos\\theta}}{2} \\\\\\\\ \\implies \\lambda =\\frac{2a}{m}\\cos\\theta \\\\\\\\ \\implies \\lambda_c = \\big[\\lambda\\big]_{\\cos\\theta=1} =\\frac{2a}{m} \\\\\\\\ \\implies \\cos\\theta = \\frac{\\lambda }{\\lambda_c} \\end{gather} \\] guide wavelength (in \\(z\\) direction) \\[ \\begin{gather} \\lambda_g = \\frac{2\\pi}{k_z}=\\frac{2\\pi}{k\\sin\\theta}=\\frac{\\lambda}{\\sin\\theta} \\\\\\\\ \\implies \\sin\\theta=\\frac{\\lambda}{\\lambda_g} \\end{gather} \\] by the \\(\\lambda_c\\) and \\(\\lambda_g\\) we can also write the equations as \\[ \\begin{gather} \\vec E = E_0 \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_y} \\\\\\\\ \\vec H = - \\frac{E_0}{\\eta}\\frac{\\lambda}{\\lambda_g} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} +\\frac{E_0}{\\eta}\\frac{\\lambda}{\\lambda_c} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\end{gather} \\]","title":"TE Waves"},{"location":"Electromagnetics/Wave%20Guide/#tm+waves","text":"\\[ \\begin{gather} \\vec H = H_0 \\cos{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_y} \\\\ \\end{gather} \\] \\[ \\begin{align} \\vec E &= \\eta H_0\\sin{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} + \\eta H_0\\cos{\\theta} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\\\\\\\ &= \\eta H_0\\frac{\\lambda}{\\lambda_g} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_x} + \\eta H_0\\frac{\\lambda }{\\lambda_c} \\sin{(k_x x)}\\sin{(\\omega t - k_z z)} \\,\\hat{a_z} \\end{align} \\]","title":"TM Waves"},{"location":"Electromagnetics/Wave%20Guide/#rectangular+waveguide","text":"\\(\\gamma\\) \\[ \\begin{gather} k^{2} = k_x^{2} + k_y^{2} + k_z^{2} = k_x^{2} + k_y^{2} + (-j\\gamma)^{2} \\\\\\\\ \\implies \\gamma^{2} = k_t^{2} - k^{2} \\end{gather} \\]","title":"Rectangular Waveguide"},{"location":"Electromagnetics/Wave%20Guide/#tm+mode","text":"tip notice that we assuming perfect conductors. thus, \\[ \\begin{gather} \\vec E_n = 0 \\\\\\\\ \\implies E_{zs}=0, \\qquad\\text{at boundary} \\end{gather} \\] \\[ \\begin{gather} E_{zs} = E_0\\sin{\\left(\\frac{m\\pi x}{a}\\right)}\\sin{\\left(\\frac{n\\pi y}{b}\\right)}\\,e^{-\\gamma z} \\end{gather} \\]","title":"TM mode"},{"location":"Electromagnetics/Wave%20Guide/#te+mode","text":"\\[ \\begin{gather} H_{zs} = H_0 \\cos{\\left(\\frac{m\\pi x}{a}\\right)}\\cos{\\left(\\frac{n\\pi y}{b}\\right)}\\,e^{-\\gamma z} \\end{gather} \\]","title":"TE mode"},{"location":"Electromagnetics/Wave%20Guide/#resonators","text":"TM \\[ \\begin{align} E_{zs} &= \\frac{E_0}{2}\\sin{\\left(\\frac{m\\pi x}{a}\\right)}\\sin{\\left(\\frac{n\\pi y}{b}\\right)}\\left(e^{-jk_z z}+e^{jk_z z}\\right) \\\\\\\\ &= E_0\\sin{\\left(\\frac{m\\pi x}{a}\\right)}\\sin{\\left(\\frac{n\\pi y}{b}\\right)}\\cos{\\left(\\frac{p \\pi z}{d}\\right)} \\end{align} \\] TE \\[ \\begin{align} H_{zs} &= H_0\\cos{\\left(\\frac{m\\pi x}{a}\\right)}\\cos{\\left(\\frac{n\\pi y}{b}\\right)}\\sin{\\left(\\frac{p\\pi z}{d}\\right)} \\end{align} \\]","title":"Resonators"},{"location":"Electromagnetics/Wave%20Guide/#q+factors","text":"\\[ \\begin{gather} Q = 2\\pi\\frac{W}{P_L\\,T}= \\frac{\\omega W}{P_L} \\end{gather} \\] recall \\[ \\begin{gather} Q = \\frac{X}{R} = \\frac{\\omega L}{R} = \\frac{\\omega I^{2}L}{I^{2}R} = \\frac{\\omega W}{P} \\end{gather} \\]","title":"Q Factors"},{"location":"Electromagnetics/Wave%20Guide/#dominant+mode","text":"the mode with lowest \\(f_c\\) (i.e. largest \\(\\lambda_c\\) ).","title":"Dominant Mode"},{"location":"Electromagnetics/Wave%20Guide/#cylindrical","text":"\\(\\beta_c\\) \\[ \\begin{gather} k^{2} = \\beta_c^{2} + \\beta^{2} \\end{gather} \\]","title":"Cylindrical"},{"location":"Electromagnetics/Wave%20Guide/#tm+mode_1","text":"\\[ \\begin{gather} E_{zs} = J_n\\big[\\beta_c \\rho\\big] \\,(A_n \\cos{n\\phi} + B_n\\sin{n\\phi})e^{\\mp j\\beta z} \\end{gather} \\]","title":"TM mode"},{"location":"Electromagnetics/Wave%20Guide/#te+mode_1","text":"\\[ \\begin{gather} H_{zs} = J_n\\big[\\beta_c \\rho\\big](A_n\\cos {n\\phi} + B_n \\sin{n\\phi})e^{\\mp j \\beta z} \\end{gather} \\]","title":"TE mode"},{"location":"Electromagnetics/Wave%20Guide/#dielectric+slab+waveguide","text":"cutoff wavelength \\[ \\begin{gather} \\lambda_c = \\frac{2d\\sqrt{\\varepsilon_{r1} - \\varepsilon_{r2}}}{m}\\,\\qquad m=0, 1, 2,\\dots \\end{gather} \\] thinking \\[ \\begin{gather} k\\cdot 2d \\cdot \\cos{\\theta_i} - 2\\angle{\\Gamma} = 2m\\pi \\\\\\\\ \\implies \\frac{2\\pi\\sqrt{\\varepsilon_{r1}}}{\\lambda_0}\\cdot d \\cdot \\cos{\\theta_i} - \\angle{\\Gamma} = m\\pi \\\\\\\\ \\implies \\angle{\\Gamma} = k\\cdot d \\cdot \\cos{\\theta_i} - m\\pi \\end{gather} \\] when cutoff \\(\\angle{\\Gamma} = 0\\) \\[ \\begin{gather} k_c \\,d\\cos{\\theta_i} = m\\pi \\\\\\\\ \\implies \\frac{2\\pi\\sqrt{\\varepsilon_{r1}}}{\\lambda_c}\\,d\\cos{\\theta_i} = m\\pi \\\\\\\\ \\frac{2\\pi\\sqrt{\\varepsilon_{r1}}}{\\lambda_c}\\,d\\sqrt{1-\\frac{\\varepsilon_2}{\\varepsilon_1}} = m\\pi \\\\\\\\ \\implies \\lambda_c = \\frac{2d\\sqrt{\\varepsilon_{r1} - \\varepsilon_{r2}}}{m} \\end{gather} \\]","title":"Dielectric Slab Waveguide"},{"location":"Electronics/Feedback/","text":"Miller Compensation \u00b6 \\[ \\begin{gather} \\frac{V_o}{I_i} = \\frac{(sC_f - g_m)R_1 R_2}{1+sX(s) + s^{2}Y(s)} \\\\\\\\ X(s) = R_1C_1 + R_2C_2 + R_1C_f(1+g_mR_2)+R_2C_f \\\\\\\\ Y(s) = (C_1C_2 + C_1C_f + C_2C_f)\\ R_1R_2 = \\binom{\\{C_1, C_2, C_f\\}}{2}\\cdot R_1R_2 \\end{gather} \\] DC gain \\[ \\begin{gather} A_0 = A(0) = -g_mR_1R_2 \\end{gather} \\] zero \\(z = g_m/C_f\\) by definition, when \\[ \\begin{gather} A(s)\\big|_{s=z} = A(z) = 0 \\end{gather} \\] implies there is no (AC)current go through \\(R_D\\) , thus we can easily have \\[ \\begin{gather} (V_i - V_o)sC_f= V_i\\ g_m \\\\\\\\ \\implies sC_f=g_m \\end{gather} \\] poles consider case without \\(C_f\\) \\[ \\begin{gather} 1+sX_o(s)+s^{2}Y_o(s) = (1+sR_1C_1)(1+sR_2C_s) \\\\\\\\ \\implies \\left\\{ \\begin{aligned} X_o(s) &=R_1C_1 + R_2C_2 \\\\\\\\ Y_o(s) &= R_1R_2C_1C_2 \\end{aligned} \\right. \\end{gather} \\] now consider \\(C_f\\) \\[ \\implies \\left\\{ \\begin{aligned} X(s) &= R_1C_1' + R_2C_2' = R_1(C_1 + C_{f-miller})+ R_2(C_2 +C_f) \\\\\\\\ Y(s) &= (C_1C_2 + C_1C_f + C_2C_f)\\ R_1R_2 \\end{aligned}\\right. \\] Pole Splitting \u00b6 \\[ \\begin{gather} D(s) = \\left(1+\\frac{s}{\\omega_{p1}'}\\right)\\left(1+\\frac{s}{\\omega_{p2}'}\\right)= 1+X(s)s+Y(s)s^{2} \\end{gather} \\] assume \\(\\omega_{p1}'\\) dominates ( \\(\\omega_{p1} \\ll \\omega_{p2}\\) ), then \\[ \\begin{gather} D(s) \\approx 1 + \\frac{s}{\\omega_{p1}} + \\frac{s^{2}}{\\omega_{p1}\\omega_{p2}} \\\\\\\\ \\implies \\left\\{ \\begin{aligned} \\omega_{p1} &= \\frac{1}{R_1C_1 + R_2C_2 + R_1C_f(1+g_mR_2)+R_2C_f} \\approx \\frac{1}{g_mR_1R_2C_f} \\\\\\\\ \\omega_{p2} &= \\frac{g_mC_f}{C_1C_2+C_1C_f+C_2C_f} \\end{aligned}\\right. \\end{gather} \\]","title":"Feedback"},{"location":"Electronics/Feedback/#miller+compensation","text":"\\[ \\begin{gather} \\frac{V_o}{I_i} = \\frac{(sC_f - g_m)R_1 R_2}{1+sX(s) + s^{2}Y(s)} \\\\\\\\ X(s) = R_1C_1 + R_2C_2 + R_1C_f(1+g_mR_2)+R_2C_f \\\\\\\\ Y(s) = (C_1C_2 + C_1C_f + C_2C_f)\\ R_1R_2 = \\binom{\\{C_1, C_2, C_f\\}}{2}\\cdot R_1R_2 \\end{gather} \\] DC gain \\[ \\begin{gather} A_0 = A(0) = -g_mR_1R_2 \\end{gather} \\] zero \\(z = g_m/C_f\\) by definition, when \\[ \\begin{gather} A(s)\\big|_{s=z} = A(z) = 0 \\end{gather} \\] implies there is no (AC)current go through \\(R_D\\) , thus we can easily have \\[ \\begin{gather} (V_i - V_o)sC_f= V_i\\ g_m \\\\\\\\ \\implies sC_f=g_m \\end{gather} \\] poles consider case without \\(C_f\\) \\[ \\begin{gather} 1+sX_o(s)+s^{2}Y_o(s) = (1+sR_1C_1)(1+sR_2C_s) \\\\\\\\ \\implies \\left\\{ \\begin{aligned} X_o(s) &=R_1C_1 + R_2C_2 \\\\\\\\ Y_o(s) &= R_1R_2C_1C_2 \\end{aligned} \\right. \\end{gather} \\] now consider \\(C_f\\) \\[ \\implies \\left\\{ \\begin{aligned} X(s) &= R_1C_1' + R_2C_2' = R_1(C_1 + C_{f-miller})+ R_2(C_2 +C_f) \\\\\\\\ Y(s) &= (C_1C_2 + C_1C_f + C_2C_f)\\ R_1R_2 \\end{aligned}\\right. \\]","title":"Miller Compensation"},{"location":"Electronics/Feedback/#pole+splitting","text":"\\[ \\begin{gather} D(s) = \\left(1+\\frac{s}{\\omega_{p1}'}\\right)\\left(1+\\frac{s}{\\omega_{p2}'}\\right)= 1+X(s)s+Y(s)s^{2} \\end{gather} \\] assume \\(\\omega_{p1}'\\) dominates ( \\(\\omega_{p1} \\ll \\omega_{p2}\\) ), then \\[ \\begin{gather} D(s) \\approx 1 + \\frac{s}{\\omega_{p1}} + \\frac{s^{2}}{\\omega_{p1}\\omega_{p2}} \\\\\\\\ \\implies \\left\\{ \\begin{aligned} \\omega_{p1} &= \\frac{1}{R_1C_1 + R_2C_2 + R_1C_f(1+g_mR_2)+R_2C_f} \\approx \\frac{1}{g_mR_1R_2C_f} \\\\\\\\ \\omega_{p2} &= \\frac{g_mC_f}{C_1C_2+C_1C_f+C_2C_f} \\end{aligned}\\right. \\end{gather} \\]","title":"Pole Splitting"},{"location":"Electronics/Ouput%20Stage/","text":"Terminology \u00b6 THD : Total Harmonic Distortion Class A \u00b6 Power delivered to load ( \\(P_L\\) ) \\[ \\begin{gather} P_L=V_{O-rms}\\cdot I_{L-rms}=\\frac{1}{2}\\frac{{V_{O-p}}^{2}}{R_L} \\end{gather} \\] Power drawn from supply ( \\(P_S\\) ) \\[ \\begin{align} P_S &= (P \\text{ from } M_1)+(P \\text{ from } M_2) \\\\\\\\ &=V_{DD}\\overline {I_{D1}}+V_{SS}I_{D2} \\\\\\\\ &= (V_{DD}+V_{SS})I \\end{align} \\] note consider the original definition of (AC) power \\[ \\begin{gather} P=\\frac{1}{2\\pi}\\int_0^{2\\pi}{v(t)i(t)\\,dt} \\end{gather} \\] in the \\(P_L\\) case, we have the vary \\(v_O\\) , thus we write \\[ \\begin{gather} P_L=\\frac{1}{2}V_pI_p\\cos(\\theta)=V_{rms}I_{rms} \\end{gather} \\] however, in the \\(P_S\\) case, we have an constant source \\(V_{DD}\\) and \\(V_{SS}\\) , thus \\[ \\begin{align} P_S &= \\frac{1}{2\\pi}\\int_0^{2\\pi}{V\\cdot i(t)\\,dt} \\\\\\\\ &= V\\cdot \\frac{1}{2\\pi}\\int_0^{2\\pi}{i(t)\\,dt} \\\\\\\\ &= V\\cdot I_{avg} \\end{align} \\] Class B \u00b6 Power delivered to load ( \\(P_L\\) ) ignoring crossover distortion \\[ \\begin{gather} P_L=\\frac{1}{2}\\frac{{V_{O-p}}^{2}}{R_L} \\end{gather} \\] Power drawn from supply ( \\(P_S\\) ) for each MOS, only conducting in half period \\[ \\begin{align} P_{Sn}=P_{Sp}&={V_{DD}\\cdot \\overline{I_D}} \\\\\\\\ &= V_{DD}\\cdot \\frac{1}{2\\pi}\\int_0^{2\\pi}{\\frac{V_{O-p}}{R_L}\\sin\\theta \\,d\\theta} \\\\\\\\ &= V_{DD}\\cdot \\frac{V_{O-p}}{\\pi R_L} \\end{align} \\] and for total power \\[ \\begin{gather} P_S=P_{Sp}+P_{Sn}=\\frac{2}{\\pi}\\frac{V_{O-p}}{R_L}V_{DD} \\end{gather} \\] efficiency \\(\\eta\\) \\[ \\begin{gather} \\eta = \\frac{P_L}{P_S}=\\frac{\\pi}{4}\\frac{V_{o-p}}{V_{DD}} \\end{gather} \\] dissipated power \\(P_D\\) \\[ \\begin{gather} P_D = P_S - P_L = \\frac{2}{\\pi}\\frac{V_{O-p}}{R_L}V_{DD} - \\frac{1}{2}\\frac{V_{O-p}^{2}}{R_L} \\end{gather} \\] \\(P_{D-max}\\) w.r.t \\(V_{o-p}\\) \\[ \\begin{gather} \\frac{d P_D}{d V_{o}}=\\frac{2}{\\pi}\\frac{V_{DD}}{R_L}-\\frac{V_o}{R_L}=0 \\\\\\\\ \\implies V_o=\\frac{2}{\\pi}V_{DD} \\\\\\\\ \\implies P_{D-max} = 2P_{Dn-max} = 2P_{Dp-max}=\\frac{2V_{DD}^{2}}{\\pi^{2}R_L} \\end{gather} \\] Class AB \u00b6 CS Buffer \u00b6 Output Resistance consider feedback (without \\(R_L\\) ) \\(R_{out}\\) \\[ \\begin{gather} R_{out-p}=\\frac{r_o}{1+\\mu g_{mp}\\,r_o} \\approx \\frac{1}{\\mu g_{mp}} \\\\\\\\ R_{out-n}=\\frac{r_o}{1+\\mu g_{mn}\\,r_o} \\approx \\frac{1}{\\mu g_{mn}} \\\\\\\\ \\implies R_{out}= R_{out-p} || R_{out-n}=\\frac{1}{\\mu(g_{mp}+g_{mn})} \\end{gather} \\] Gain Error \\(G_E\\) \\[ \\begin{gather} G_E\\equiv\\frac{v_o-v_i}{v_i}=\\frac{A}{1+A}-1=-\\frac{1}{1+A}\\approx \\frac{-1}{A}=\\frac{-1}{2\\mu g_m R_L} \\end{gather} \\] Class D \u00b6 Theoretically power-conversion efficiency is \\(100\\%\\) since the input 1/0 pulse make transistors act as on-off switch, when the transistor on, there is no cross voltage, but current pass through. On the other hand, when transistor off, there is a large cross voltage but no current. Power MOSFET \u00b6 High \\(V_t\\) : \\(\\quad 2V \\sim 4V\\) In saturation region: for low \\(V_{GS}\\) : \\(i_D \\propto {V_{GS}}^{2}\\) for high \\(V_{GS}\\) : \\(i_D \\propto {V_{GS}}\\) (the velocity saturation is due to the saturation of mobility \\(\\mu\\) ) Temperature Effects \u00b6 \\[ i_D=\\frac{1}{2}\\mu C_{ox} \\frac{W}{L}(V_{GS}-V_t)^2 \\] for \\(T \\uparrow\\) \\(\\implies\\) \\(\\mu\\downarrow\\) , \\(V_T \\downarrow\\) for low \\(V_{GS}\\) , \\(T \\uparrow \\implies i_D \\uparrow\\) ( \\(\\Delta\\big[(V_{GS}-V_t)^2\\big]\\) dominates) for high \\(V_{GS}\\) , \\(T\\uparrow \\implies i_D \\downarrow\\) ( \\(\\Delta\\big[\\mu\\big]\\) dominates) Thermal Resistance \u00b6 Junction temp. \\(T_J\\) Ambient temp. \\(T_A\\) Thermal resistance between junction and ambience. \\(\\theta_{JA}\\) Temperature here acts as voltage, and the dissipated power acts as current. Thus by Ohm's law we have \\[ \\begin{gather} T_{Jmax}-T_A=P_{Dmax}\\,\\theta_{JA} \\end{gather} \\] Two Stage CMOS op-amp \u00b6 Common-Mode \u00b6 \\[ \\begin{gather} A_{cm} \\approx \\frac{-1}{2g_{m3}\\,R_{SS}} \\\\\\\\ CMRR \\equiv \\frac{|A_d|}{|A_{cm}|}=g_{m1}(r_{o2}||r_{o4})\\cdot 2g_{m3}R_{SS} \\end{gather} \\] Input Common-Mode Range \u00b6 lower limit ( \\(M_1\\) leaving saturation, when \\(V_{OV1} = V_{DS1}\\) ) \\[ \\begin{gather} V_{ICM}+|V_{tp}| \\geq V_{D3}=-V_{SS}+V_{GS3} \\\\\\\\ V_{ICM} \\geq -V_{SS}+V_{GS3}-|V_{tp}| \\end{gather} \\] upper limit ( \\(M_5\\) leaving saturation) \\[ \\begin{gather} V_{DD}-V_{OV5} \\geq V_{ICM} +V_{SG1} \\\\\\\\ V_{ICM} \\leq V_{DD}-V_{OV5} - V_{SG1} \\end{gather} \\] Output Swing \u00b6 lower limit \\[ \\begin{gather} v_O \\leq -V_{SS}+V_{OV6} \\end{gather} \\] upper limit \\[ \\begin{gather} v_O \\geq V_{DD}-V_{OV7} \\end{gather} \\] PSRR \u00b6 definition \\[ \\begin{gather} \\text{PSRR}^{+} \\equiv \\frac{A_d}{A^{+}} & \\text{where } A^{+}\\equiv \\frac{v_o}{v_{dd}} \\\\\\\\ \\text{PSRR}^{-} \\equiv \\frac{A_d}{A^{-}} & \\text{where } A^{-}\\equiv \\frac{v_o}{v_{ss}} \\end{gather} \\] \\(\\text{PSRR}^{+} \\to \\infty\\) : just remember that \\(v_O\\) via second stage and \\(v_O\\) via first stage would cancel out each other. \\(\\text{PSRR}^{-}\\) \\(v_O\\) from first stage is \\(0\\) (don't know fucking why) \\(v_O\\) from second stage is \\[ \\begin{gather} v_o=v_{ss} \\frac{r_{o7}}{r_{o7}+r_{o6}} \\\\\\\\ \\text{PSRR}^{-}= \\frac{A_d}{A^{-}}=\\frac{g_{m1}(r_{o2}||r_{o4})g_{m6}(r_{o6}||r_{o7})}{\\frac{r_{o7}}{r_{o7}+r_{o6}}} \\end{gather} \\] Slew Rate \u00b6 when \\(|v_{id}| > \\sqrt2 V_{OV}\\) , the current would go through only one side of differential pair. \\[ \\begin{gather} v_o(t)=\\frac{Q_C}{C_C}=\\frac{I}{C_C}t \\\\\\\\ \\text{SR}=\\frac{I}{C_C}=\\frac{I}{G_{m1}/\\omega_t}=\\frac{I\\omega_t}{I/V_{OV}}=\\omega_tV_{OV} \\end{gather} \\] Folded-Cascode CMOS op-amp \u00b6 Input Common-Mode Range \u00b6 upper limit ( \\(M_1, M_2\\) leave saturation) \\[ \\begin{gather} V_{ICM}-V_{tn} \\leq V_{DD}-V_{OV9} \\\\\\\\ \\implies V_{ICM} \\leq V_{DD}-V_{OV9}+V_{tn} \\end{gather} \\] lower limit ( \\(M_{11}\\) leaves saturation) \\[ \\begin{gather} V_{ICM}-V_{GS1} \\geq -V_{SS} +V_{OV11} \\\\\\\\ \\implies V_{ICM}\\geq -V_{SS} +V_{OV11}+V_{GS1} \\end{gather} \\] Output Swing \u00b6 upper limit ( \\(M_4\\) leaves saturation) \\[ \\begin{gather} v_O \\leq V_{DD}-V_{OV10}-V_{OV4} \\end{gather} \\] lower limit ( \\(M_6\\) leaves saturation) \\[ \\begin{gather} v_O \\geq -V_{SS}+V_{GS7}+V_{GS5}-V_{tn6} \\end{gather} \\]","title":"Ouput Stage"},{"location":"Electronics/Ouput%20Stage/#terminology","text":"THD : Total Harmonic Distortion","title":"Terminology"},{"location":"Electronics/Ouput%20Stage/#class+a","text":"Power delivered to load ( \\(P_L\\) ) \\[ \\begin{gather} P_L=V_{O-rms}\\cdot I_{L-rms}=\\frac{1}{2}\\frac{{V_{O-p}}^{2}}{R_L} \\end{gather} \\] Power drawn from supply ( \\(P_S\\) ) \\[ \\begin{align} P_S &= (P \\text{ from } M_1)+(P \\text{ from } M_2) \\\\\\\\ &=V_{DD}\\overline {I_{D1}}+V_{SS}I_{D2} \\\\\\\\ &= (V_{DD}+V_{SS})I \\end{align} \\] note consider the original definition of (AC) power \\[ \\begin{gather} P=\\frac{1}{2\\pi}\\int_0^{2\\pi}{v(t)i(t)\\,dt} \\end{gather} \\] in the \\(P_L\\) case, we have the vary \\(v_O\\) , thus we write \\[ \\begin{gather} P_L=\\frac{1}{2}V_pI_p\\cos(\\theta)=V_{rms}I_{rms} \\end{gather} \\] however, in the \\(P_S\\) case, we have an constant source \\(V_{DD}\\) and \\(V_{SS}\\) , thus \\[ \\begin{align} P_S &= \\frac{1}{2\\pi}\\int_0^{2\\pi}{V\\cdot i(t)\\,dt} \\\\\\\\ &= V\\cdot \\frac{1}{2\\pi}\\int_0^{2\\pi}{i(t)\\,dt} \\\\\\\\ &= V\\cdot I_{avg} \\end{align} \\]","title":"Class A"},{"location":"Electronics/Ouput%20Stage/#class+b","text":"Power delivered to load ( \\(P_L\\) ) ignoring crossover distortion \\[ \\begin{gather} P_L=\\frac{1}{2}\\frac{{V_{O-p}}^{2}}{R_L} \\end{gather} \\] Power drawn from supply ( \\(P_S\\) ) for each MOS, only conducting in half period \\[ \\begin{align} P_{Sn}=P_{Sp}&={V_{DD}\\cdot \\overline{I_D}} \\\\\\\\ &= V_{DD}\\cdot \\frac{1}{2\\pi}\\int_0^{2\\pi}{\\frac{V_{O-p}}{R_L}\\sin\\theta \\,d\\theta} \\\\\\\\ &= V_{DD}\\cdot \\frac{V_{O-p}}{\\pi R_L} \\end{align} \\] and for total power \\[ \\begin{gather} P_S=P_{Sp}+P_{Sn}=\\frac{2}{\\pi}\\frac{V_{O-p}}{R_L}V_{DD} \\end{gather} \\] efficiency \\(\\eta\\) \\[ \\begin{gather} \\eta = \\frac{P_L}{P_S}=\\frac{\\pi}{4}\\frac{V_{o-p}}{V_{DD}} \\end{gather} \\] dissipated power \\(P_D\\) \\[ \\begin{gather} P_D = P_S - P_L = \\frac{2}{\\pi}\\frac{V_{O-p}}{R_L}V_{DD} - \\frac{1}{2}\\frac{V_{O-p}^{2}}{R_L} \\end{gather} \\] \\(P_{D-max}\\) w.r.t \\(V_{o-p}\\) \\[ \\begin{gather} \\frac{d P_D}{d V_{o}}=\\frac{2}{\\pi}\\frac{V_{DD}}{R_L}-\\frac{V_o}{R_L}=0 \\\\\\\\ \\implies V_o=\\frac{2}{\\pi}V_{DD} \\\\\\\\ \\implies P_{D-max} = 2P_{Dn-max} = 2P_{Dp-max}=\\frac{2V_{DD}^{2}}{\\pi^{2}R_L} \\end{gather} \\]","title":"Class B"},{"location":"Electronics/Ouput%20Stage/#class+ab","text":"","title":"Class AB"},{"location":"Electronics/Ouput%20Stage/#cs+buffer","text":"Output Resistance consider feedback (without \\(R_L\\) ) \\(R_{out}\\) \\[ \\begin{gather} R_{out-p}=\\frac{r_o}{1+\\mu g_{mp}\\,r_o} \\approx \\frac{1}{\\mu g_{mp}} \\\\\\\\ R_{out-n}=\\frac{r_o}{1+\\mu g_{mn}\\,r_o} \\approx \\frac{1}{\\mu g_{mn}} \\\\\\\\ \\implies R_{out}= R_{out-p} || R_{out-n}=\\frac{1}{\\mu(g_{mp}+g_{mn})} \\end{gather} \\] Gain Error \\(G_E\\) \\[ \\begin{gather} G_E\\equiv\\frac{v_o-v_i}{v_i}=\\frac{A}{1+A}-1=-\\frac{1}{1+A}\\approx \\frac{-1}{A}=\\frac{-1}{2\\mu g_m R_L} \\end{gather} \\]","title":"CS Buffer"},{"location":"Electronics/Ouput%20Stage/#class+d","text":"Theoretically power-conversion efficiency is \\(100\\%\\) since the input 1/0 pulse make transistors act as on-off switch, when the transistor on, there is no cross voltage, but current pass through. On the other hand, when transistor off, there is a large cross voltage but no current.","title":"Class D"},{"location":"Electronics/Ouput%20Stage/#power+mosfet","text":"High \\(V_t\\) : \\(\\quad 2V \\sim 4V\\) In saturation region: for low \\(V_{GS}\\) : \\(i_D \\propto {V_{GS}}^{2}\\) for high \\(V_{GS}\\) : \\(i_D \\propto {V_{GS}}\\) (the velocity saturation is due to the saturation of mobility \\(\\mu\\) )","title":"Power MOSFET"},{"location":"Electronics/Ouput%20Stage/#temperature+effects","text":"\\[ i_D=\\frac{1}{2}\\mu C_{ox} \\frac{W}{L}(V_{GS}-V_t)^2 \\] for \\(T \\uparrow\\) \\(\\implies\\) \\(\\mu\\downarrow\\) , \\(V_T \\downarrow\\) for low \\(V_{GS}\\) , \\(T \\uparrow \\implies i_D \\uparrow\\) ( \\(\\Delta\\big[(V_{GS}-V_t)^2\\big]\\) dominates) for high \\(V_{GS}\\) , \\(T\\uparrow \\implies i_D \\downarrow\\) ( \\(\\Delta\\big[\\mu\\big]\\) dominates)","title":"Temperature Effects"},{"location":"Electronics/Ouput%20Stage/#thermal+resistance","text":"Junction temp. \\(T_J\\) Ambient temp. \\(T_A\\) Thermal resistance between junction and ambience. \\(\\theta_{JA}\\) Temperature here acts as voltage, and the dissipated power acts as current. Thus by Ohm's law we have \\[ \\begin{gather} T_{Jmax}-T_A=P_{Dmax}\\,\\theta_{JA} \\end{gather} \\]","title":"Thermal Resistance"},{"location":"Electronics/Ouput%20Stage/#two+stage+cmos+op-amp","text":"","title":"Two Stage CMOS op-amp"},{"location":"Electronics/Ouput%20Stage/#common-mode","text":"\\[ \\begin{gather} A_{cm} \\approx \\frac{-1}{2g_{m3}\\,R_{SS}} \\\\\\\\ CMRR \\equiv \\frac{|A_d|}{|A_{cm}|}=g_{m1}(r_{o2}||r_{o4})\\cdot 2g_{m3}R_{SS} \\end{gather} \\]","title":"Common-Mode"},{"location":"Electronics/Ouput%20Stage/#input+common-mode+range","text":"lower limit ( \\(M_1\\) leaving saturation, when \\(V_{OV1} = V_{DS1}\\) ) \\[ \\begin{gather} V_{ICM}+|V_{tp}| \\geq V_{D3}=-V_{SS}+V_{GS3} \\\\\\\\ V_{ICM} \\geq -V_{SS}+V_{GS3}-|V_{tp}| \\end{gather} \\] upper limit ( \\(M_5\\) leaving saturation) \\[ \\begin{gather} V_{DD}-V_{OV5} \\geq V_{ICM} +V_{SG1} \\\\\\\\ V_{ICM} \\leq V_{DD}-V_{OV5} - V_{SG1} \\end{gather} \\]","title":"Input Common-Mode Range"},{"location":"Electronics/Ouput%20Stage/#output+swing","text":"lower limit \\[ \\begin{gather} v_O \\leq -V_{SS}+V_{OV6} \\end{gather} \\] upper limit \\[ \\begin{gather} v_O \\geq V_{DD}-V_{OV7} \\end{gather} \\]","title":"Output Swing"},{"location":"Electronics/Ouput%20Stage/#psrr","text":"definition \\[ \\begin{gather} \\text{PSRR}^{+} \\equiv \\frac{A_d}{A^{+}} & \\text{where } A^{+}\\equiv \\frac{v_o}{v_{dd}} \\\\\\\\ \\text{PSRR}^{-} \\equiv \\frac{A_d}{A^{-}} & \\text{where } A^{-}\\equiv \\frac{v_o}{v_{ss}} \\end{gather} \\] \\(\\text{PSRR}^{+} \\to \\infty\\) : just remember that \\(v_O\\) via second stage and \\(v_O\\) via first stage would cancel out each other. \\(\\text{PSRR}^{-}\\) \\(v_O\\) from first stage is \\(0\\) (don't know fucking why) \\(v_O\\) from second stage is \\[ \\begin{gather} v_o=v_{ss} \\frac{r_{o7}}{r_{o7}+r_{o6}} \\\\\\\\ \\text{PSRR}^{-}= \\frac{A_d}{A^{-}}=\\frac{g_{m1}(r_{o2}||r_{o4})g_{m6}(r_{o6}||r_{o7})}{\\frac{r_{o7}}{r_{o7}+r_{o6}}} \\end{gather} \\]","title":"PSRR"},{"location":"Electronics/Ouput%20Stage/#slew+rate","text":"when \\(|v_{id}| > \\sqrt2 V_{OV}\\) , the current would go through only one side of differential pair. \\[ \\begin{gather} v_o(t)=\\frac{Q_C}{C_C}=\\frac{I}{C_C}t \\\\\\\\ \\text{SR}=\\frac{I}{C_C}=\\frac{I}{G_{m1}/\\omega_t}=\\frac{I\\omega_t}{I/V_{OV}}=\\omega_tV_{OV} \\end{gather} \\]","title":"Slew Rate"},{"location":"Electronics/Ouput%20Stage/#folded-cascode+cmos+op-amp","text":"","title":"Folded-Cascode CMOS op-amp"},{"location":"Electronics/Ouput%20Stage/#input+common-mode+range_1","text":"upper limit ( \\(M_1, M_2\\) leave saturation) \\[ \\begin{gather} V_{ICM}-V_{tn} \\leq V_{DD}-V_{OV9} \\\\\\\\ \\implies V_{ICM} \\leq V_{DD}-V_{OV9}+V_{tn} \\end{gather} \\] lower limit ( \\(M_{11}\\) leaves saturation) \\[ \\begin{gather} V_{ICM}-V_{GS1} \\geq -V_{SS} +V_{OV11} \\\\\\\\ \\implies V_{ICM}\\geq -V_{SS} +V_{OV11}+V_{GS1} \\end{gather} \\]","title":"Input Common-Mode Range"},{"location":"Electronics/Ouput%20Stage/#output+swing_1","text":"upper limit ( \\(M_4\\) leaves saturation) \\[ \\begin{gather} v_O \\leq V_{DD}-V_{OV10}-V_{OV4} \\end{gather} \\] lower limit ( \\(M_6\\) leaves saturation) \\[ \\begin{gather} v_O \\geq -V_{SS}+V_{GS7}+V_{GS5}-V_{tn6} \\end{gather} \\]","title":"Output Swing"},{"location":"Engineering%20Math/1stODE/","text":"Separable Variable Method \u00b6 for any ODE satisfying \\[ \\frac{dy}{dx}=g(x)h(y) \\] linear Equation \u00b6 standard form \u00b6 \\[y'+P(x)y=f(x)\\] integrating factor \u00b6 \\[ M(x)=e^{\\int{P(x)dx}} \\] solving method \u00b6 standard form \\[y'+Py=f\\] multiple M(x) \\[y'\u22c5M+P\u22c5My=f\u22c5M\\] notice that \\[M'=P\u22c5M\\] thus (form 2.) \\[M\\cdot y'+M'\\cdot y=M\\cdot f\\] according to product rule \\[\\frac{d}{dx}(M\\cdot y)=M\\cdot f\\] integrate both sides \\[M\\cdot y = \\int{M\\cdot f dx} + c\\] check initial value and singular points 1 Exact Equation \u00b6 Any 1 st ODE can be written as \\[M(x, y)dx+N(x, y)dy=0\\] If \\(\\exists f(x, y)\\) , s.t. \\[df(x,y)=\\frac{\u2202f(x,y)}{\u2202x}dx+\\frac{\\partial f(x,y)}{\u2202y}dy=0\\] which means \\[\\frac{\u2202f(x, y)}{\u2202x}=M(x, y)\\] \\[\\text{and}\\] \\[\\frac{\u2202f(x, y)}{\u2202x}=N(x, y)\\] , then \\[M(x, y)dx+N(x, y)dy=0\\] is an Exact Equation . for an exact equation \\[M(x, y)dx+N(x, y)dy=0\\] due to \\[df(x,y)=\\frac{\u2202f(x,y)}{\u2202x}dx+\\frac{\\partial f(x,y)}{\u2202y}dy=0\\] we know that \\[ f(x,y) = c \\] solution to exact equation \u00b6 \\[M(x, y)dx+N(x, y)dy=0\\] check the 1 st ODE is exact equation \\[\\frac{\u2202M}{\u2202y}=\\frac{\u2202N}{\u2202x}=\\frac{\u2202f}{\u2202x\u2202y}\\] then \\[f(x,y) = \\int{M\\cdot dx} + g(y)\\] and we can get \\(g(y)\\) by \\[\\frac{\\partial}{\\partial y}f(x,y)=N(x, y)\\] solve \\[f(x, y)=c\\] modify non-exact equations to exact equations \u00b6 given a 1 st ODE which isn't exact equation \\[M(x,y)dx+N(x,y)dy=0\\] meaning \\[\\frac{\u2202M}{\u2202y}\\neq \\frac{\u2202N}{\u2202x}\\] we can try to find an integrating factor \\(\\mu\\) s.t. \\[ \\mu M dx + \\mu N dy = 0 \\] is an exact equation. check if this equation satisfy either 1. > > \\( \\(h(y)=\\frac{N_x-M_y}{M}\\) \\) > > is independent to \\(x\\) . (dependent to \\(y\\) alone) or 2. > > \\( \\(h(x)=\\frac{M_y-N_x}{N}\\) \\) > > is independent to \\(y\\) . (dependent to \\(x\\) alone) if neither of these condition were satisfied, the equation cannot be exact equation. otherwise, the equation can be multiplied by \\(\\mu\\) to become exact equation , and solve by the method mentioned above. \\[\\mu(y)=e^{\\int {h(y)dy}}\\] or \\[\\mu(x)=e^{\\int {h(x)dx}}\\] see proof . homogeneous Equation \u00b6 There are two different definition of homogeneous equation. homogeneous Linear Equation \u00b6 The constant term of linear Equation is zero In 1 st ODE case \\[y'+P(x)y=f(x)\\] The equation is homogeneous if f(x) = 0 see further definition . homogeneous 1st ODE \u00b6 \u4ee5\u4e0b\u5169\u7a2e\u5b9a\u7fa9\u7b49\u50f9\u3002 ODE in \\(y' = f(x,y)\\) form The equation is homogeneous if \\[y'=f(x,y)=f(tx,ty)\\] ODE in \\(Mdx + Ndy = 0\\) form If \\(M(x,y)\\) and \\(N(x,y)\\) are homogeneous functions of the same degree , then the 1 st ODE is homogeneous . Quick Check Guide : sum of powers (\u6307\u6578\u4e4b\u548c) Solution to 1st ODE \u00b6 Set \\(y=ux\\) , \\(dy = udx +xdu\\) , then use separable variable method Bernoulli's Equations \u00b6 aka \u767d\u52aa\u529b equation standard form \u00b6 \\[y'+P(x)y=f(x)\\cdot y^n\\] Solution \u00b6 Assume \\[u=y^{1-n}\\] according to chain rule, we can write \\[\\frac{dy}{dx}=\\frac{1}{1-n} u^{\\frac{n}{1-n}} \\frac{du}{dx}\\] \u4ee3\u5165 standard form \\[ \\begin{align} \\frac{1}{1-n} u^{\\frac{n}{1-n}} \\frac{du}{dx} &+ P(x)\\cdot u^{\\frac{1}{1-n}} &= f(x)\\cdot u^{\\frac{n}{1-n}} \\\\ \\frac{du}{dx} &+ P(x)(1-n)\\cdot u &= (1-n)f(x) \\end{align} \\] then use the method of solving linear 1 st ODE. Ax + By + C \u00b6 If the 1 st ODE has the form \\[\\frac{dy}{dx} = f(Ax+By+C),\\quad (B\\neq 0)\\] set \\(u=Ax+By+C\\) then \\[ \\begin{gather} du = Adx + Bdy\\\\ \\downarrow\\\\ \\frac{dy}{dx} = \\frac{1}{B}\\frac{du}{dx}-\\frac{A}{B}\\\\ \\downarrow\\\\ \\frac{1}{B}\\frac{du}{dx}-\\frac{A}{B}=f(u) \\end{gather} \\] and solve it by any method above. singular points \\(\\forall x\\) s.t. \\[P(x)\\rightarrow \\infty \\quad \\text{or} \\quad f(x) \\rightarrow \\infty\\] singular points cannot be solution. \u21a9","title":"1stODE"},{"location":"Engineering%20Math/1stODE/#separable+variable+method","text":"for any ODE satisfying \\[ \\frac{dy}{dx}=g(x)h(y) \\]","title":"Separable Variable Method"},{"location":"Engineering%20Math/1stODE/#linear+equation","text":"","title":"linear Equation"},{"location":"Engineering%20Math/1stODE/#standard+form","text":"\\[y'+P(x)y=f(x)\\]","title":"standard form"},{"location":"Engineering%20Math/1stODE/#integrating+factor","text":"\\[ M(x)=e^{\\int{P(x)dx}} \\]","title":"integrating factor"},{"location":"Engineering%20Math/1stODE/#solving+method","text":"standard form \\[y'+Py=f\\] multiple M(x) \\[y'\u22c5M+P\u22c5My=f\u22c5M\\] notice that \\[M'=P\u22c5M\\] thus (form 2.) \\[M\\cdot y'+M'\\cdot y=M\\cdot f\\] according to product rule \\[\\frac{d}{dx}(M\\cdot y)=M\\cdot f\\] integrate both sides \\[M\\cdot y = \\int{M\\cdot f dx} + c\\] check initial value and singular points 1","title":"solving method"},{"location":"Engineering%20Math/1stODE/#exact+equation","text":"Any 1 st ODE can be written as \\[M(x, y)dx+N(x, y)dy=0\\] If \\(\\exists f(x, y)\\) , s.t. \\[df(x,y)=\\frac{\u2202f(x,y)}{\u2202x}dx+\\frac{\\partial f(x,y)}{\u2202y}dy=0\\] which means \\[\\frac{\u2202f(x, y)}{\u2202x}=M(x, y)\\] \\[\\text{and}\\] \\[\\frac{\u2202f(x, y)}{\u2202x}=N(x, y)\\] , then \\[M(x, y)dx+N(x, y)dy=0\\] is an Exact Equation . for an exact equation \\[M(x, y)dx+N(x, y)dy=0\\] due to \\[df(x,y)=\\frac{\u2202f(x,y)}{\u2202x}dx+\\frac{\\partial f(x,y)}{\u2202y}dy=0\\] we know that \\[ f(x,y) = c \\]","title":"Exact Equation"},{"location":"Engineering%20Math/1stODE/#solution+to+exact+equation","text":"\\[M(x, y)dx+N(x, y)dy=0\\] check the 1 st ODE is exact equation \\[\\frac{\u2202M}{\u2202y}=\\frac{\u2202N}{\u2202x}=\\frac{\u2202f}{\u2202x\u2202y}\\] then \\[f(x,y) = \\int{M\\cdot dx} + g(y)\\] and we can get \\(g(y)\\) by \\[\\frac{\\partial}{\\partial y}f(x,y)=N(x, y)\\] solve \\[f(x, y)=c\\]","title":"solution to exact equation"},{"location":"Engineering%20Math/1stODE/#modify+non-exact+equations+to+exact+equations","text":"given a 1 st ODE which isn't exact equation \\[M(x,y)dx+N(x,y)dy=0\\] meaning \\[\\frac{\u2202M}{\u2202y}\\neq \\frac{\u2202N}{\u2202x}\\] we can try to find an integrating factor \\(\\mu\\) s.t. \\[ \\mu M dx + \\mu N dy = 0 \\] is an exact equation. check if this equation satisfy either 1. > > \\( \\(h(y)=\\frac{N_x-M_y}{M}\\) \\) > > is independent to \\(x\\) . (dependent to \\(y\\) alone) or 2. > > \\( \\(h(x)=\\frac{M_y-N_x}{N}\\) \\) > > is independent to \\(y\\) . (dependent to \\(x\\) alone) if neither of these condition were satisfied, the equation cannot be exact equation. otherwise, the equation can be multiplied by \\(\\mu\\) to become exact equation , and solve by the method mentioned above. \\[\\mu(y)=e^{\\int {h(y)dy}}\\] or \\[\\mu(x)=e^{\\int {h(x)dx}}\\] see proof .","title":"modify non-exact equations to exact equations"},{"location":"Engineering%20Math/1stODE/#homogeneous+equation","text":"There are two different definition of homogeneous equation.","title":"homogeneous Equation"},{"location":"Engineering%20Math/1stODE/#homogeneous+linear+equation","text":"The constant term of linear Equation is zero In 1 st ODE case \\[y'+P(x)y=f(x)\\] The equation is homogeneous if f(x) = 0 see further definition .","title":"homogeneous Linear Equation"},{"location":"Engineering%20Math/1stODE/#homogeneous+1st+ode","text":"\u4ee5\u4e0b\u5169\u7a2e\u5b9a\u7fa9\u7b49\u50f9\u3002 ODE in \\(y' = f(x,y)\\) form The equation is homogeneous if \\[y'=f(x,y)=f(tx,ty)\\] ODE in \\(Mdx + Ndy = 0\\) form If \\(M(x,y)\\) and \\(N(x,y)\\) are homogeneous functions of the same degree , then the 1 st ODE is homogeneous . Quick Check Guide : sum of powers (\u6307\u6578\u4e4b\u548c)","title":"homogeneous 1st ODE"},{"location":"Engineering%20Math/1stODE/#solution+to+1st+ode","text":"Set \\(y=ux\\) , \\(dy = udx +xdu\\) , then use separable variable method","title":"Solution to 1st ODE"},{"location":"Engineering%20Math/1stODE/#bernoullis+equations","text":"aka \u767d\u52aa\u529b equation","title":"Bernoulli's Equations"},{"location":"Engineering%20Math/1stODE/#standard+form_1","text":"\\[y'+P(x)y=f(x)\\cdot y^n\\]","title":"standard form"},{"location":"Engineering%20Math/1stODE/#solution","text":"Assume \\[u=y^{1-n}\\] according to chain rule, we can write \\[\\frac{dy}{dx}=\\frac{1}{1-n} u^{\\frac{n}{1-n}} \\frac{du}{dx}\\] \u4ee3\u5165 standard form \\[ \\begin{align} \\frac{1}{1-n} u^{\\frac{n}{1-n}} \\frac{du}{dx} &+ P(x)\\cdot u^{\\frac{1}{1-n}} &= f(x)\\cdot u^{\\frac{n}{1-n}} \\\\ \\frac{du}{dx} &+ P(x)(1-n)\\cdot u &= (1-n)f(x) \\end{align} \\] then use the method of solving linear 1 st ODE.","title":"Solution"},{"location":"Engineering%20Math/1stODE/#ax++by++c","text":"If the 1 st ODE has the form \\[\\frac{dy}{dx} = f(Ax+By+C),\\quad (B\\neq 0)\\] set \\(u=Ax+By+C\\) then \\[ \\begin{gather} du = Adx + Bdy\\\\ \\downarrow\\\\ \\frac{dy}{dx} = \\frac{1}{B}\\frac{du}{dx}-\\frac{A}{B}\\\\ \\downarrow\\\\ \\frac{1}{B}\\frac{du}{dx}-\\frac{A}{B}=f(u) \\end{gather} \\] and solve it by any method above. singular points \\(\\forall x\\) s.t. \\[P(x)\\rightarrow \\infty \\quad \\text{or} \\quad f(x) \\rightarrow \\infty\\] singular points cannot be solution. \u21a9","title":"Ax + By + C"},{"location":"Engineering%20Math/Fourier%20Integral/","text":"Fourier Integral \u00b6 Definition assume that \\[ \\begin{gather} \\int_{-\\infty}^{\\infty}{\\left|f(x)\\right|dx}\\ \\leftarrow\\ \\text{converge} \\end{gather} \\] converges , then \\[ f(x)=\\int_0^{\\infty}{\\big[A(\\omega)\\ \\cos{(\\omega x)}+B(\\omega )\\sin{(x)}\\big]\\ d\\omega} \\] \\[ \\begin{align} A(\\omega)=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}{f(x)\\ \\cos{(\\omega x)}\\ dx} \\qquad B(\\omega)=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}{f(x)\\ \\sin{(\\omega x)}\\ dx} \\end{align} \\] Fourier Cosine Integral \u00b6 \\[ f(x)=\\int_0^{\\infty}{A(\\omega)\\ \\cos{(\\omega x)}\\ d\\omega} \\] \\[ \\begin{align} A(\\omega)=\\frac{2}{\\pi}\\int_0^{\\infty}{f(x)\\ \\cos{(\\omega x)}\\ dx} \\end{align} \\] Fourier Sine Integral \u00b6 \\[ \\begin{align} f(x)=\\int_0^{\\infty}{B(\\omega)\\ \\sin{(\\omega x)}\\ d\\omega}\\\\\\\\ B(\\omega)=\\frac{2}{\\pi}\\int_0^{\\infty}{f(x)\\ \\sin{(\\omega x)}\\ dx} \\end{align} \\] Complex Fourier Integral \u00b6 Similar to complex fourier series . \\[ \\begin{align} f(x)=\\int_{-\\infty}^{\\infty}{C(\\omega)e^{i\\omega x}d\\omega}\\\\\\\\ C(\\omega)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}{f(x)e^{-i\\omega x}dx} \\end{align} \\]","title":"Fourier Integral"},{"location":"Engineering%20Math/Fourier%20Integral/#fourier+integral","text":"Definition assume that \\[ \\begin{gather} \\int_{-\\infty}^{\\infty}{\\left|f(x)\\right|dx}\\ \\leftarrow\\ \\text{converge} \\end{gather} \\] converges , then \\[ f(x)=\\int_0^{\\infty}{\\big[A(\\omega)\\ \\cos{(\\omega x)}+B(\\omega )\\sin{(x)}\\big]\\ d\\omega} \\] \\[ \\begin{align} A(\\omega)=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}{f(x)\\ \\cos{(\\omega x)}\\ dx} \\qquad B(\\omega)=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}{f(x)\\ \\sin{(\\omega x)}\\ dx} \\end{align} \\]","title":"Fourier Integral"},{"location":"Engineering%20Math/Fourier%20Integral/#fourier+cosine+integral","text":"\\[ f(x)=\\int_0^{\\infty}{A(\\omega)\\ \\cos{(\\omega x)}\\ d\\omega} \\] \\[ \\begin{align} A(\\omega)=\\frac{2}{\\pi}\\int_0^{\\infty}{f(x)\\ \\cos{(\\omega x)}\\ dx} \\end{align} \\]","title":"Fourier Cosine Integral"},{"location":"Engineering%20Math/Fourier%20Integral/#fourier+sine+integral","text":"\\[ \\begin{align} f(x)=\\int_0^{\\infty}{B(\\omega)\\ \\sin{(\\omega x)}\\ d\\omega}\\\\\\\\ B(\\omega)=\\frac{2}{\\pi}\\int_0^{\\infty}{f(x)\\ \\sin{(\\omega x)}\\ dx} \\end{align} \\]","title":"Fourier Sine Integral"},{"location":"Engineering%20Math/Fourier%20Integral/#complex+fourier+integral","text":"Similar to complex fourier series . \\[ \\begin{align} f(x)=\\int_{-\\infty}^{\\infty}{C(\\omega)e^{i\\omega x}d\\omega}\\\\\\\\ C(\\omega)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}{f(x)e^{-i\\omega x}dx} \\end{align} \\]","title":"Complex Fourier Integral"},{"location":"Engineering%20Math/Fourier%20Series/","text":"Fourier Series \u00b6 \\[ f(x)=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{\\bigg(a_n\\cos{\\frac{n\\pi}{p}x}+b_n\\sin{\\frac{n\\pi}{p}x}\\bigg)} \\] \\[ \\begin{align} \\\\ a_0 = \\frac{1}{p}\\ \\int_{P}{f(x)\\ dx} \\\\\\\\ a_n = \\frac{1}{p}\\int_{P}{f(x)\\cos{\\frac{n\\pi}{p}x}\\ dx}\\\\\\\\ b_n = \\frac{1}{p}\\int_{P}{f(x)\\sin{\\frac{n\\pi}{p}x}\\ dx}\\\\\\\\ \\end{align} \\] in which \\(a_0,\\ a_n,\\ b_n\\) are called as Fourier coefficients , \\(P\\) is any of full period of \\(f(x)\\) . Let \\(p\\) always be a half of period, then we can understand that \\[\\frac{\\pi}{p}=\\frac{2\\pi}{2p}=\\omega_0\\] Notice that the close form of \\(a_n\\) is often undefined on \\(0\\) . \\[ \\begin{align} \\lim_{n\\to 0}a_n\\to\\pm \\infty \\end{align} \\] Fourier Cosine and Sine Series \u00b6 \\(f(x)\\) is even => Fourier cosine series \\(f(x)\\) is odd \\(\\,\\) => Fourier sine series Tips cosine & sine series : interval is change into \\([-L,\\ L]\\) , set \\(p=L\\) Fourier series : interval \\([-p,p]\\) is change into \\([0,L]\\) , set \\(p=L/2\\) \\(p\\) is always a half of period. Fourier Cosine Series \u00b6 \\[ \\begin{align} f(x)&=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{a_n\\cos{\\frac{n\\pi}{p}x}} \\\\\\\\ \\end{align} \\] in which, \\[ \\begin{align} a_0 &= \\frac{1}{p}\\int_{-p}^{p}{f(x)\\ dx}= \\frac{\\mathbf{2}}{p}\\int_{\\mathbf{0}}^p{f(x)\\ dx}\\\\\\\\ a_n&=\\frac{1}{p}\\int_{-p}^{p}{f(x)\\ \\cos{\\frac{n\\pi}{p}}x\\ dx}=\\frac{\\mathbf{2}}{p}\\int_{\\mathbf{0}}^p{f(x)\\cos{\\frac{n\\pi}{p}x}\\ dx} \\end{align} \\] notice : 1. \\(f(x)\\) is even. 2. Take \\([-p, p]\\) for \\(P\\) . 3. Half range extension (compare to Fourier series). Fourier Sine Series \u00b6 \\[ \\begin{align} f(x)=\\sum_{n=1}^{\\infty}{b_n\\sin{\\frac{n\\pi}{p}x}} \\\\\\\\ \\end{align} \\] in which, \\[ \\begin{align} b_n=\\frac{1}{p}\\int_{-p}^p{f(x)\\sin{\\frac{n\\pi}{p}x}\\ dx} =\\frac{\\mathbf{2}}{p}\\int_{\\mathbf{0}}^p{f(x)\\sin{\\frac{n\\pi}{p}x}\\ dx} \\end{align} \\] notice : 1. \\(f(x)\\) is odd. ( \\(\\big[f(x) \\sin\\alpha x\\big]\\) will thus be even) 2. Take \\([-p,p]\\) for \\(P\\) 3. Half range extension (compare to Fourier series). Gibbs Phenomenon \u00b6 \\[ f(x)=\\frac{a_0}{2}+\\sum_{n=1}^{N}{\\bigg(a_n\\cos{\\frac{n\\pi}{p}x}+b_n\\sin{\\frac{n\\pi}{p}x}\\bigg)} \\] There will be \"overshooting\" near discontinuities when \\(N\\) isn't infinity. While \\(N \\to \\infty\\) , \"overshooting\" will be more and more narrow. And according to convergence theorem, for all \\(a\\) in the domain of \\(f(x)\\) , we have \\[ \\begin{align} \\lim_{n\\to \\infty}f(a)=\\frac{1}{2}\\big[f(a_-)+f(a_+)\\big] \\end{align} \\] Phase Angle Form \u00b6 Definition \\[ \\begin{align} f(x)=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{c_n\\ \\cos{(n\\omega_0 x+\\delta_n)}} \\end{align} \\] in which \\[ \\begin{align} \\omega_0 &= \\frac{\\pi}{p}\\\\\\\\ c_n&=\\sqrt{a_n^2+b_n^2}\\\\\\\\ \\delta_n&=\\tan^{-1}{(\\frac{-b_n}{a_n})} \\end{align} \\] The phase angle form is aka harmonic form . \\(c_n\\) is the \\(n\\) th harmonic amplitude, \\(\\delta_n\\) is the \\(n\\) th phase angle of \\(f(x)\\) , and the term \\(\\cos{(n\\omega_0 x+\\delta_n)}\\) is the \\(n\\) th harmonic of \\(f(x)\\) . ( \\(n\\) \u968e\u8ae7\u6ce2) Amplitude Spectrum \u00b6 Graph of the polar points \\((\\theta, r)= \\bigg[(n\\omega_0 , \\frac{c_n}{2}) \\ \\cup\\ (0, \\frac{a_0}{2})\\bigg]\\) in which \\(n\\omega_0\\) is the frequency and \\(\\frac{c_n}{2}\\) is the amplitude. Complex Fourier Series \u00b6 With Euler Formula \\[e^{ix}=\\cos{(x)}+i\\ \\sin{(x)}\\] , we can rewrite Fourier Series expansion as \\[ \\begin{align} f(x)&= d_0 + \\sum_{n=1}^{\\infty}{d_ne^{in\\omega_0 x}}+ \\sum_{n=1}^{\\infty}{\\overline{d_n}e^{-in\\omega_0 x}} \\\\\\\\ &=d_0+\\sum_{n=-\\infty,\\ n\\ne0}^{\\infty}{d_ne^{in\\omega_0 x}} \\end{align} \\] in which, \\[ \\begin{align} d_n=\\frac{1}{2}(a_n-ib_n)=\\frac{1}{2p}\\int_{P}{f(t)e^{-in\\omega_0 t}dt} \\end{align} \\] notice that \\(p\\) is also the half of period here, and that \\[ \\begin{align} \\overline{d_n}=d_{-n} \\end{align} \\] Amplitude Spectrum \u00b6 Graph of the polar points \\((\\theta, r)=(n\\omega_0 , |d_n|)\\) , in which \\(n\\omega_0\\) is the frequency and \\(|d_n|\\) is the amplitude.","title":"Fourier Series"},{"location":"Engineering%20Math/Fourier%20Series/#fourier+series","text":"\\[ f(x)=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{\\bigg(a_n\\cos{\\frac{n\\pi}{p}x}+b_n\\sin{\\frac{n\\pi}{p}x}\\bigg)} \\] \\[ \\begin{align} \\\\ a_0 = \\frac{1}{p}\\ \\int_{P}{f(x)\\ dx} \\\\\\\\ a_n = \\frac{1}{p}\\int_{P}{f(x)\\cos{\\frac{n\\pi}{p}x}\\ dx}\\\\\\\\ b_n = \\frac{1}{p}\\int_{P}{f(x)\\sin{\\frac{n\\pi}{p}x}\\ dx}\\\\\\\\ \\end{align} \\] in which \\(a_0,\\ a_n,\\ b_n\\) are called as Fourier coefficients , \\(P\\) is any of full period of \\(f(x)\\) . Let \\(p\\) always be a half of period, then we can understand that \\[\\frac{\\pi}{p}=\\frac{2\\pi}{2p}=\\omega_0\\] Notice that the close form of \\(a_n\\) is often undefined on \\(0\\) . \\[ \\begin{align} \\lim_{n\\to 0}a_n\\to\\pm \\infty \\end{align} \\]","title":"Fourier Series"},{"location":"Engineering%20Math/Fourier%20Series/#fourier+cosine+and+sine+series","text":"\\(f(x)\\) is even => Fourier cosine series \\(f(x)\\) is odd \\(\\,\\) => Fourier sine series Tips cosine & sine series : interval is change into \\([-L,\\ L]\\) , set \\(p=L\\) Fourier series : interval \\([-p,p]\\) is change into \\([0,L]\\) , set \\(p=L/2\\) \\(p\\) is always a half of period.","title":"Fourier Cosine and Sine Series"},{"location":"Engineering%20Math/Fourier%20Series/#fourier+cosine+series","text":"\\[ \\begin{align} f(x)&=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{a_n\\cos{\\frac{n\\pi}{p}x}} \\\\\\\\ \\end{align} \\] in which, \\[ \\begin{align} a_0 &= \\frac{1}{p}\\int_{-p}^{p}{f(x)\\ dx}= \\frac{\\mathbf{2}}{p}\\int_{\\mathbf{0}}^p{f(x)\\ dx}\\\\\\\\ a_n&=\\frac{1}{p}\\int_{-p}^{p}{f(x)\\ \\cos{\\frac{n\\pi}{p}}x\\ dx}=\\frac{\\mathbf{2}}{p}\\int_{\\mathbf{0}}^p{f(x)\\cos{\\frac{n\\pi}{p}x}\\ dx} \\end{align} \\] notice : 1. \\(f(x)\\) is even. 2. Take \\([-p, p]\\) for \\(P\\) . 3. Half range extension (compare to Fourier series).","title":"Fourier Cosine Series"},{"location":"Engineering%20Math/Fourier%20Series/#fourier+sine+series","text":"\\[ \\begin{align} f(x)=\\sum_{n=1}^{\\infty}{b_n\\sin{\\frac{n\\pi}{p}x}} \\\\\\\\ \\end{align} \\] in which, \\[ \\begin{align} b_n=\\frac{1}{p}\\int_{-p}^p{f(x)\\sin{\\frac{n\\pi}{p}x}\\ dx} =\\frac{\\mathbf{2}}{p}\\int_{\\mathbf{0}}^p{f(x)\\sin{\\frac{n\\pi}{p}x}\\ dx} \\end{align} \\] notice : 1. \\(f(x)\\) is odd. ( \\(\\big[f(x) \\sin\\alpha x\\big]\\) will thus be even) 2. Take \\([-p,p]\\) for \\(P\\) 3. Half range extension (compare to Fourier series).","title":"Fourier Sine Series"},{"location":"Engineering%20Math/Fourier%20Series/#gibbs+phenomenon","text":"\\[ f(x)=\\frac{a_0}{2}+\\sum_{n=1}^{N}{\\bigg(a_n\\cos{\\frac{n\\pi}{p}x}+b_n\\sin{\\frac{n\\pi}{p}x}\\bigg)} \\] There will be \"overshooting\" near discontinuities when \\(N\\) isn't infinity. While \\(N \\to \\infty\\) , \"overshooting\" will be more and more narrow. And according to convergence theorem, for all \\(a\\) in the domain of \\(f(x)\\) , we have \\[ \\begin{align} \\lim_{n\\to \\infty}f(a)=\\frac{1}{2}\\big[f(a_-)+f(a_+)\\big] \\end{align} \\]","title":"Gibbs Phenomenon"},{"location":"Engineering%20Math/Fourier%20Series/#phase+angle+form","text":"Definition \\[ \\begin{align} f(x)=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{c_n\\ \\cos{(n\\omega_0 x+\\delta_n)}} \\end{align} \\] in which \\[ \\begin{align} \\omega_0 &= \\frac{\\pi}{p}\\\\\\\\ c_n&=\\sqrt{a_n^2+b_n^2}\\\\\\\\ \\delta_n&=\\tan^{-1}{(\\frac{-b_n}{a_n})} \\end{align} \\] The phase angle form is aka harmonic form . \\(c_n\\) is the \\(n\\) th harmonic amplitude, \\(\\delta_n\\) is the \\(n\\) th phase angle of \\(f(x)\\) , and the term \\(\\cos{(n\\omega_0 x+\\delta_n)}\\) is the \\(n\\) th harmonic of \\(f(x)\\) . ( \\(n\\) \u968e\u8ae7\u6ce2)","title":"Phase Angle Form"},{"location":"Engineering%20Math/Fourier%20Series/#amplitude+spectrum","text":"Graph of the polar points \\((\\theta, r)= \\bigg[(n\\omega_0 , \\frac{c_n}{2}) \\ \\cup\\ (0, \\frac{a_0}{2})\\bigg]\\) in which \\(n\\omega_0\\) is the frequency and \\(\\frac{c_n}{2}\\) is the amplitude.","title":"Amplitude Spectrum"},{"location":"Engineering%20Math/Fourier%20Series/#complex+fourier+series","text":"With Euler Formula \\[e^{ix}=\\cos{(x)}+i\\ \\sin{(x)}\\] , we can rewrite Fourier Series expansion as \\[ \\begin{align} f(x)&= d_0 + \\sum_{n=1}^{\\infty}{d_ne^{in\\omega_0 x}}+ \\sum_{n=1}^{\\infty}{\\overline{d_n}e^{-in\\omega_0 x}} \\\\\\\\ &=d_0+\\sum_{n=-\\infty,\\ n\\ne0}^{\\infty}{d_ne^{in\\omega_0 x}} \\end{align} \\] in which, \\[ \\begin{align} d_n=\\frac{1}{2}(a_n-ib_n)=\\frac{1}{2p}\\int_{P}{f(t)e^{-in\\omega_0 t}dt} \\end{align} \\] notice that \\(p\\) is also the half of period here, and that \\[ \\begin{align} \\overline{d_n}=d_{-n} \\end{align} \\]","title":"Complex Fourier Series"},{"location":"Engineering%20Math/Fourier%20Series/#amplitude+spectrum_1","text":"Graph of the polar points \\((\\theta, r)=(n\\omega_0 , |d_n|)\\) , in which \\(n\\omega_0\\) is the frequency and \\(|d_n|\\) is the amplitude.","title":"Amplitude Spectrum"},{"location":"Engineering%20Math/Fourier%20Transform/","text":"Fourier Transform \u00b6 Complex form or exponential form of Fourier Integral \\[ \ud835\udd09\\big[f(t)\\big]=\\int^{\\infty}_{-\\infty}{e^{-i\\omega t}\\ f(t)\\ dt} \\] assume that \\[ \\begin{gather} \\int^{\\infty}_{-\\infty}{|f(x)|dx} \\end{gather} \\] converges Comparison compare to Laplace transform \\[\\begin{align} s \\to i\\omega\\\\\\\\ \\int_0^{\\infty} \\to \\int^{\\infty}_{-\\infty} \\end{align}\\] compare to Fourier Series Fourier Transform can be regarded as Fourier series whose \\(p \\to \\infty\\) . Amplitude Spectrum \u00b6 The amplitude spectrum of \\(f(t)\\) is a graph of \\(\\big|\ud835\udd09\\big[f(\\omega)\\big]\\big|\\) Fourier & Inverse Fourier Transform \u00b6 Fourier transform \\[ \ud835\udd09\\big[f(x)\\big]=\\int^{\\infty}_{-\\infty}{e^{-i\\omega x}\\ f(x)\\ dx} \\] inverse Fourier transform \\[ \ud835\udd09^{-1}\\big[F(\\omega)\\big]=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}{F(\\omega)\\ e^{i\\omega x}\\ d\\omega}=f(x) \\] Fourier Sine Transform \u00b6 Fourier sine transform \\[ \ud835\udd09_s\\big[f(x)\\big]=\\int_0^{\\infty}{f(x)\\ \\sin{(\\omega x)\\ dx=F(\\omega)}} \\] inverse Fourier sine transform \\[ \ud835\udd09_s^{-1}\\big[F(\\omega)\\big]=\\frac{2}{\\pi}\\int_0^{\\infty}{F(\\omega)\\,\\sin{(\\omega x)}\\ d\\omega}=f(x) \\] Fourier Cosine Transform \u00b6 Fourier cosine transform \\[ \ud835\udd09_c\\big[f(x)\\big]=\\int_0^{\\infty}{f(x)\\,\\cos{(\\omega x)}\\ dx}= F(\\omega) \\] inverse Fourier cosine transform \\[ \ud835\udd09_c^{-1}\\big[F(\\omega)\\big]=\\frac{2}{\\pi}\\int_0^{\\infty}{F(\\omega)\\ \\cos{(\\omega x)}\\ d\\omega}=f(x) \\] Fourier Transform properties \u00b6 Linearity \u00b6 \\[ \\begin{gather} \\alpha, \\beta \\in \u211d \\\\\\\\ \ud835\udd09\\big[\\alpha f(t)+\\beta g(t)\\big]=\\alpha\ud835\udd09\\big[f(t)\\big]+\ud835\udd09\\big[\\beta g(t)\\big] \\end{gather} \\] Time Shifting \u00b6 \\[ \\begin{gather} \ud835\udd09\\big[f(t-t_0)\\big]=e^{-i\\omega t_0} \\ \ud835\udd09\\big[f(t)\\big] \\end{gather} \\] Inverse Time Shifting \\[ \\begin{gather} \ud835\udd09^{-1}\\big[e^{-i\\omega t_0}F(\\omega)\\big](t)=f(t-t_0) \\end{gather} \\] Frequency Shifting \u00b6 \\[ \\begin{gather} \ud835\udd09\\big[e^{i\\omega_0 t}\\ f(t)\\big]=F(\\omega-\\omega_0) \\end{gather} \\] Inverse Frequency Shifting \\[ \\begin{align} \ud835\udd09^{-1}\\big[F(\\omega-\\omega_0)\\big]=e^{i\\omega_0 t} \\ f(t) \\end{align} \\] Scaling \u00b6 \\[ \\begin{gather} \ud835\udd09\\big[f(ct)\\big]=\\frac{1}{|c|}F(\\frac{\\omega}{c}) \\end{gather} \\] Inverse Scaling \\[ \\begin{gather} \ud835\udd09^{-1}\\big[F(\\frac{\\omega}{c})\\big]=|c|\\ f(ct) \\end{gather} \\] Time Reversal \u00b6 \\[ \\begin{gather} \ud835\udd09\\big[f(-t)\\big]=F(-\\omega)\\\\\\\\ \ud835\udd09^{-1}\\big[F(-\\omega)\\big]=f(-t) \\end{gather} \\] Symmetry \u00b6 \\[ \\begin{gather} \ud835\udd09\\big[F(t)\\big]=2\\pi f(-\\omega) \\end{gather} \\] modulation \u00b6 \\[ \\begin{align} \ud835\udd09\\big[f(t)\\cos{(\\omega_0 t)}\\big]&=\\frac{1}{2}[F(\\omega+\\omega_0)+F(\\omega-\\omega_0)]\\\\\\\\ \ud835\udd09\\big[f(t) \\sin(\\omega_0 t)\\big]&=\\frac{i}{2}[F(\\omega+\\omega_0)-F(\\omega-\\omega_0)] \\end{align} \\] particular example \u00b6 prof. Laplace Transform \\[ \\begin{gather} \ud835\udd09\\big[H(t)e^{-at}\\big]=\\frac{1}{a+i\\omega} \\end{gather} \\] Frequency Differentiation \u00b6 \\[ \\begin{gather} \ud835\udd09\\big[t^nf(t)\\big]=(i)^nF^{(n)} \\end{gather} \\] Transforms of Derivatives \u00b6 for Fourier transform \\[ \\begin{align} \ud835\udd09\\big[f'(x)\\big]=i\\omega F(\\omega) \\\\\\\\ \ud835\udd09\\big[f^{(n)}\\big]=(i\\omega)^{n}\\ F(\\omega) \\end{align} \\] for Fourier sine transform \\[ \\begin{align} \ud835\udd09_s\\big[f'(x)\\big]=-\\omega \\ \ud835\udd09_c\\big[f(x)\\big] \\\\\\\\ \ud835\udd09_s\\big[f''(x)\\big]= -\\omega^2\\ \ud835\udd09_s\\big[f(x)\\big]+\\omega f(0) \\end{align} \\] for Fourier cosine transform \\[ \\begin{align} \ud835\udd09_c\\big[f'(x)\\big]=\\omega \\, \ud835\udd09_s\\big[f(x)\\big]-f(0) \\\\\\\\ \ud835\udd09_c\\big[f''(x)\\big]=-\\omega^2\ud835\udd09_c\\big[f(x)\\big]-f'(0) \\end{align} \\] Transform of Integral \u00b6 \\[ \\begin{align} \ud835\udd09\\left[\\int_{-\\infty}^{t}{f(\\tau )d\\tau}\\right]=\\frac{1}{i\\omega}F(\\omega) \\end{align} \\] Convolution \u00b6 Definition \\[ \\begin{gather} f(t)*g(t)=\\int_{-\\infty}^{\\infty}{f(\\tau)\\ g(t-\\tau)\\ d\\tau} \\end{gather} \\] Property \\[ \\begin{align} f * g &= g * f \\\\\\\\ (\\alpha f+\\beta g)*h&=\\alpha(f*h)+\\beta(g*h) \\end{align} \\] Theorem \\[ \\begin{gather} \\int_{-\\infty}^{\\infty}{\\left[f(t)*g(t)\\right]\\ dt}=\\int^{\\infty}_{-\\infty}{f(t)dt}\\cdot \\int^{\\infty}_{-\\infty}{g(t)dt}\\\\\\\\ \ud835\udd09\\big[f(t) * g(t)\\big]=F(\\omega) \\ G(\\omega)\\\\\\\\ \ud835\udd09\\left[f(t)\\ g(t)\\right]=\\frac{1}{2\\pi}F(\\omega)*G(\\omega) \\end{gather} \\] Dirac Delta Function \u00b6 Definition \\[ \\begin{gather} \\delta(t)=\\lim_{a\\to 0}{\\left[\\frac{1}{2a}H(t+a)-H(t-a)\\right]} \\end{gather} \\] Fourier Transform \u00b6 \\[ \\begin{gather} \ud835\udd09\\left[\\delta(t)\\right]=\ud835\udd09\\left[\\lim_{a\\to 0}{\\left[\\frac{1}{2a}H(t+a)-H(t-a)\\right]}\\right]=\\lim_{a\\to 0}{\\frac{\\sin{(a\\omega)}}{a\\omega}}=1 \\end{gather} \\] Filtering \u00b6 \\[ \\begin{gather} \\int^{\\infty}_{-\\infty}{f(t)\\delta(t-t_0)\\ dt}=f(t_0) \\end{gather} \\] Discrete Fourier Transform \u00b6 Let \\(u=\\left\\{u_j\\right\\}^{N-1}_{j=0}\\) \\[ \\begin{gather} D[u]=U_k= \\sum_{j=0}^{N-1}{u_j\\exp{\\left(-i\\ \\omega_0 j\\ \\frac{T}{N}\\right)}} = \\sum_{j=0}^{N-1}{u_j\\exp{\\left(-i\\ 2\\pi\\ j\\ \\frac{1}{N}\\right)}} \\end{gather} \\] Linearity \u00b6 \\[ \\begin{gather} D\\left[au+bv\\right]=aU_k+bV_k \\end{gather} \\] Periodicity \u00b6 \\[ \\begin{gather} U_{k+N}=U_k \\end{gather} \\] Inverse N-point DFT \u00b6 \\[ \\begin{gather} u_j=\\frac{1}{N}\\sum_{j=0}^{N-1}{U_k\\exp{\\left(i\\ \\omega_0 j\\ \\frac{T}{N}\\right)}}=\\frac{1}{N}\\sum_{j=0}^{N-1}{U_k\\exp{\\left(i\\ 2\\pi \\ j\\ \\frac{1}{N}\\right)}} \\end{gather} \\] Complex Fourier Coefficients \u00b6 \\[ \\begin{gather} d_k \\approx \\frac{1}{N}\\cdot U_k \\end{gather} \\] note that the coefficient of complex Fourier \\(C_\\omega\\) \\[ \\begin{gather} C_\\omega=\\frac{1}{2\\pi}\\cdot \ud835\udd09\\left[f(t)\\right] \\end{gather} \\] Sampled Fourier Series \u00b6 \\[ \\begin{gather} S_M\\left(\\frac{jT}{n}\\right)\\approx\\frac{1}{N}\\sum_{k=0} ^{N-1}{V_k\\ e^{i\\omega_0 jkT/N }} \\end{gather} \\] in which \\[ V_k=\\left\\{ \\begin{align} &U_k &\\text{for }k=0, 1, \\dots, M \\\\\\\\ &0 &\\text{for }k=M+1, \\dots, N-M+1 \\\\\\\\ &U_k &\\text{for }k=N-M, \\dots, N-1 \\end{align}\\right. \\] Solving the BVP \u00b6 BVP (Boundary Value Problem) There are three possible condition below \\(-\\infty < x <\\infty\\) \\(\\Rightarrow\\) Fourier transform \\(0 < x < \\infty\\) and \\(u(x,y)\\bigg|_{x=0} = 0\\) \\(\\Rightarrow\\) Fourier sine transform \\(0 < x < \\infty\\) and \\(\\frac{\\partial}{\\partial x}u(x, y)\\bigg|_{x=0}=0\\) \\(\\Rightarrow\\) Fourier cosine transform Method of Separation of Variable \u00b6 kernel : assume that 1. \\[u(x,y)=X(x)\\ Y(y)\\] 2. \\[\"f(x)\"\\ =\\ \"f(y)\"\\ =\\ -\\lambda\\]","title":"Fourier Transform"},{"location":"Engineering%20Math/Fourier%20Transform/#fourier+transform","text":"Complex form or exponential form of Fourier Integral \\[ \ud835\udd09\\big[f(t)\\big]=\\int^{\\infty}_{-\\infty}{e^{-i\\omega t}\\ f(t)\\ dt} \\] assume that \\[ \\begin{gather} \\int^{\\infty}_{-\\infty}{|f(x)|dx} \\end{gather} \\] converges Comparison compare to Laplace transform \\[\\begin{align} s \\to i\\omega\\\\\\\\ \\int_0^{\\infty} \\to \\int^{\\infty}_{-\\infty} \\end{align}\\] compare to Fourier Series Fourier Transform can be regarded as Fourier series whose \\(p \\to \\infty\\) .","title":"Fourier Transform"},{"location":"Engineering%20Math/Fourier%20Transform/#amplitude+spectrum","text":"The amplitude spectrum of \\(f(t)\\) is a graph of \\(\\big|\ud835\udd09\\big[f(\\omega)\\big]\\big|\\)","title":"Amplitude Spectrum"},{"location":"Engineering%20Math/Fourier%20Transform/#fourier++inverse+fourier+transform","text":"Fourier transform \\[ \ud835\udd09\\big[f(x)\\big]=\\int^{\\infty}_{-\\infty}{e^{-i\\omega x}\\ f(x)\\ dx} \\] inverse Fourier transform \\[ \ud835\udd09^{-1}\\big[F(\\omega)\\big]=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}{F(\\omega)\\ e^{i\\omega x}\\ d\\omega}=f(x) \\]","title":"Fourier &amp; Inverse Fourier Transform"},{"location":"Engineering%20Math/Fourier%20Transform/#fourier+sine+transform","text":"Fourier sine transform \\[ \ud835\udd09_s\\big[f(x)\\big]=\\int_0^{\\infty}{f(x)\\ \\sin{(\\omega x)\\ dx=F(\\omega)}} \\] inverse Fourier sine transform \\[ \ud835\udd09_s^{-1}\\big[F(\\omega)\\big]=\\frac{2}{\\pi}\\int_0^{\\infty}{F(\\omega)\\,\\sin{(\\omega x)}\\ d\\omega}=f(x) \\]","title":"Fourier Sine Transform"},{"location":"Engineering%20Math/Fourier%20Transform/#fourier+cosine+transform","text":"Fourier cosine transform \\[ \ud835\udd09_c\\big[f(x)\\big]=\\int_0^{\\infty}{f(x)\\,\\cos{(\\omega x)}\\ dx}= F(\\omega) \\] inverse Fourier cosine transform \\[ \ud835\udd09_c^{-1}\\big[F(\\omega)\\big]=\\frac{2}{\\pi}\\int_0^{\\infty}{F(\\omega)\\ \\cos{(\\omega x)}\\ d\\omega}=f(x) \\]","title":"Fourier Cosine Transform"},{"location":"Engineering%20Math/Fourier%20Transform/#fourier+transform+properties","text":"","title":"Fourier Transform properties"},{"location":"Engineering%20Math/Fourier%20Transform/#linearity","text":"\\[ \\begin{gather} \\alpha, \\beta \\in \u211d \\\\\\\\ \ud835\udd09\\big[\\alpha f(t)+\\beta g(t)\\big]=\\alpha\ud835\udd09\\big[f(t)\\big]+\ud835\udd09\\big[\\beta g(t)\\big] \\end{gather} \\]","title":"Linearity"},{"location":"Engineering%20Math/Fourier%20Transform/#time+shifting","text":"\\[ \\begin{gather} \ud835\udd09\\big[f(t-t_0)\\big]=e^{-i\\omega t_0} \\ \ud835\udd09\\big[f(t)\\big] \\end{gather} \\] Inverse Time Shifting \\[ \\begin{gather} \ud835\udd09^{-1}\\big[e^{-i\\omega t_0}F(\\omega)\\big](t)=f(t-t_0) \\end{gather} \\]","title":"Time Shifting"},{"location":"Engineering%20Math/Fourier%20Transform/#frequency+shifting","text":"\\[ \\begin{gather} \ud835\udd09\\big[e^{i\\omega_0 t}\\ f(t)\\big]=F(\\omega-\\omega_0) \\end{gather} \\] Inverse Frequency Shifting \\[ \\begin{align} \ud835\udd09^{-1}\\big[F(\\omega-\\omega_0)\\big]=e^{i\\omega_0 t} \\ f(t) \\end{align} \\]","title":"Frequency Shifting"},{"location":"Engineering%20Math/Fourier%20Transform/#scaling","text":"\\[ \\begin{gather} \ud835\udd09\\big[f(ct)\\big]=\\frac{1}{|c|}F(\\frac{\\omega}{c}) \\end{gather} \\] Inverse Scaling \\[ \\begin{gather} \ud835\udd09^{-1}\\big[F(\\frac{\\omega}{c})\\big]=|c|\\ f(ct) \\end{gather} \\]","title":"Scaling"},{"location":"Engineering%20Math/Fourier%20Transform/#time+reversal","text":"\\[ \\begin{gather} \ud835\udd09\\big[f(-t)\\big]=F(-\\omega)\\\\\\\\ \ud835\udd09^{-1}\\big[F(-\\omega)\\big]=f(-t) \\end{gather} \\]","title":"Time Reversal"},{"location":"Engineering%20Math/Fourier%20Transform/#symmetry","text":"\\[ \\begin{gather} \ud835\udd09\\big[F(t)\\big]=2\\pi f(-\\omega) \\end{gather} \\]","title":"Symmetry"},{"location":"Engineering%20Math/Fourier%20Transform/#modulation","text":"\\[ \\begin{align} \ud835\udd09\\big[f(t)\\cos{(\\omega_0 t)}\\big]&=\\frac{1}{2}[F(\\omega+\\omega_0)+F(\\omega-\\omega_0)]\\\\\\\\ \ud835\udd09\\big[f(t) \\sin(\\omega_0 t)\\big]&=\\frac{i}{2}[F(\\omega+\\omega_0)-F(\\omega-\\omega_0)] \\end{align} \\]","title":"modulation"},{"location":"Engineering%20Math/Fourier%20Transform/#particular+example","text":"prof. Laplace Transform \\[ \\begin{gather} \ud835\udd09\\big[H(t)e^{-at}\\big]=\\frac{1}{a+i\\omega} \\end{gather} \\]","title":"particular example"},{"location":"Engineering%20Math/Fourier%20Transform/#frequency+differentiation","text":"\\[ \\begin{gather} \ud835\udd09\\big[t^nf(t)\\big]=(i)^nF^{(n)} \\end{gather} \\]","title":"Frequency Differentiation"},{"location":"Engineering%20Math/Fourier%20Transform/#transforms+of+derivatives","text":"for Fourier transform \\[ \\begin{align} \ud835\udd09\\big[f'(x)\\big]=i\\omega F(\\omega) \\\\\\\\ \ud835\udd09\\big[f^{(n)}\\big]=(i\\omega)^{n}\\ F(\\omega) \\end{align} \\] for Fourier sine transform \\[ \\begin{align} \ud835\udd09_s\\big[f'(x)\\big]=-\\omega \\ \ud835\udd09_c\\big[f(x)\\big] \\\\\\\\ \ud835\udd09_s\\big[f''(x)\\big]= -\\omega^2\\ \ud835\udd09_s\\big[f(x)\\big]+\\omega f(0) \\end{align} \\] for Fourier cosine transform \\[ \\begin{align} \ud835\udd09_c\\big[f'(x)\\big]=\\omega \\, \ud835\udd09_s\\big[f(x)\\big]-f(0) \\\\\\\\ \ud835\udd09_c\\big[f''(x)\\big]=-\\omega^2\ud835\udd09_c\\big[f(x)\\big]-f'(0) \\end{align} \\]","title":"Transforms of Derivatives"},{"location":"Engineering%20Math/Fourier%20Transform/#transform+of+integral","text":"\\[ \\begin{align} \ud835\udd09\\left[\\int_{-\\infty}^{t}{f(\\tau )d\\tau}\\right]=\\frac{1}{i\\omega}F(\\omega) \\end{align} \\]","title":"Transform of Integral"},{"location":"Engineering%20Math/Fourier%20Transform/#convolution","text":"Definition \\[ \\begin{gather} f(t)*g(t)=\\int_{-\\infty}^{\\infty}{f(\\tau)\\ g(t-\\tau)\\ d\\tau} \\end{gather} \\] Property \\[ \\begin{align} f * g &= g * f \\\\\\\\ (\\alpha f+\\beta g)*h&=\\alpha(f*h)+\\beta(g*h) \\end{align} \\] Theorem \\[ \\begin{gather} \\int_{-\\infty}^{\\infty}{\\left[f(t)*g(t)\\right]\\ dt}=\\int^{\\infty}_{-\\infty}{f(t)dt}\\cdot \\int^{\\infty}_{-\\infty}{g(t)dt}\\\\\\\\ \ud835\udd09\\big[f(t) * g(t)\\big]=F(\\omega) \\ G(\\omega)\\\\\\\\ \ud835\udd09\\left[f(t)\\ g(t)\\right]=\\frac{1}{2\\pi}F(\\omega)*G(\\omega) \\end{gather} \\]","title":"Convolution"},{"location":"Engineering%20Math/Fourier%20Transform/#dirac+delta+function","text":"Definition \\[ \\begin{gather} \\delta(t)=\\lim_{a\\to 0}{\\left[\\frac{1}{2a}H(t+a)-H(t-a)\\right]} \\end{gather} \\]","title":"Dirac Delta Function"},{"location":"Engineering%20Math/Fourier%20Transform/#fourier+transform_1","text":"\\[ \\begin{gather} \ud835\udd09\\left[\\delta(t)\\right]=\ud835\udd09\\left[\\lim_{a\\to 0}{\\left[\\frac{1}{2a}H(t+a)-H(t-a)\\right]}\\right]=\\lim_{a\\to 0}{\\frac{\\sin{(a\\omega)}}{a\\omega}}=1 \\end{gather} \\]","title":"Fourier Transform"},{"location":"Engineering%20Math/Fourier%20Transform/#filtering","text":"\\[ \\begin{gather} \\int^{\\infty}_{-\\infty}{f(t)\\delta(t-t_0)\\ dt}=f(t_0) \\end{gather} \\]","title":"Filtering"},{"location":"Engineering%20Math/Fourier%20Transform/#discrete+fourier+transform","text":"Let \\(u=\\left\\{u_j\\right\\}^{N-1}_{j=0}\\) \\[ \\begin{gather} D[u]=U_k= \\sum_{j=0}^{N-1}{u_j\\exp{\\left(-i\\ \\omega_0 j\\ \\frac{T}{N}\\right)}} = \\sum_{j=0}^{N-1}{u_j\\exp{\\left(-i\\ 2\\pi\\ j\\ \\frac{1}{N}\\right)}} \\end{gather} \\]","title":"Discrete Fourier Transform"},{"location":"Engineering%20Math/Fourier%20Transform/#linearity_1","text":"\\[ \\begin{gather} D\\left[au+bv\\right]=aU_k+bV_k \\end{gather} \\]","title":"Linearity"},{"location":"Engineering%20Math/Fourier%20Transform/#periodicity","text":"\\[ \\begin{gather} U_{k+N}=U_k \\end{gather} \\]","title":"Periodicity"},{"location":"Engineering%20Math/Fourier%20Transform/#inverse+n-point+dft","text":"\\[ \\begin{gather} u_j=\\frac{1}{N}\\sum_{j=0}^{N-1}{U_k\\exp{\\left(i\\ \\omega_0 j\\ \\frac{T}{N}\\right)}}=\\frac{1}{N}\\sum_{j=0}^{N-1}{U_k\\exp{\\left(i\\ 2\\pi \\ j\\ \\frac{1}{N}\\right)}} \\end{gather} \\]","title":"Inverse N-point DFT"},{"location":"Engineering%20Math/Fourier%20Transform/#complex+fourier+coefficients","text":"\\[ \\begin{gather} d_k \\approx \\frac{1}{N}\\cdot U_k \\end{gather} \\] note that the coefficient of complex Fourier \\(C_\\omega\\) \\[ \\begin{gather} C_\\omega=\\frac{1}{2\\pi}\\cdot \ud835\udd09\\left[f(t)\\right] \\end{gather} \\]","title":"Complex Fourier Coefficients"},{"location":"Engineering%20Math/Fourier%20Transform/#sampled+fourier+series","text":"\\[ \\begin{gather} S_M\\left(\\frac{jT}{n}\\right)\\approx\\frac{1}{N}\\sum_{k=0} ^{N-1}{V_k\\ e^{i\\omega_0 jkT/N }} \\end{gather} \\] in which \\[ V_k=\\left\\{ \\begin{align} &U_k &\\text{for }k=0, 1, \\dots, M \\\\\\\\ &0 &\\text{for }k=M+1, \\dots, N-M+1 \\\\\\\\ &U_k &\\text{for }k=N-M, \\dots, N-1 \\end{align}\\right. \\]","title":"Sampled Fourier Series"},{"location":"Engineering%20Math/Fourier%20Transform/#solving+the+bvp","text":"BVP (Boundary Value Problem) There are three possible condition below \\(-\\infty < x <\\infty\\) \\(\\Rightarrow\\) Fourier transform \\(0 < x < \\infty\\) and \\(u(x,y)\\bigg|_{x=0} = 0\\) \\(\\Rightarrow\\) Fourier sine transform \\(0 < x < \\infty\\) and \\(\\frac{\\partial}{\\partial x}u(x, y)\\bigg|_{x=0}=0\\) \\(\\Rightarrow\\) Fourier cosine transform","title":"Solving the BVP"},{"location":"Engineering%20Math/Fourier%20Transform/#method+of+separation+of+variable","text":"kernel : assume that 1. \\[u(x,y)=X(x)\\ Y(y)\\] 2. \\[\"f(x)\"\\ =\\ \"f(y)\"\\ =\\ -\\lambda\\]","title":"Method of Separation of Variable"},{"location":"Engineering%20Math/Heat%20Equations/","text":"Heat Equation \u00b6 1-D heat equation \\[ \\begin{gather} \\frac{\\partial U}{\\partial t} = k \\frac{\\partial^{2}U}{\\partial x^{2}} \\end{gather} \\] 3-D heat equation \\[ \\begin{gather} \\frac{\\partial U}{\\partial t} = k( \\frac{\\partial^{2}U}{\\partial x^{2}}+\\frac{\\partial^{2}U}{\\partial y^{2}} +\\frac{\\partial^{2}U}{\\partial z^{2}}) \\end{gather} \\] Principle \u00b6 \u6c92\u5fae\u5206\u5169\u7aef\u56fa\u5b9a \u2192 \\(\\sin\\) \u5169\u7aef\u56fa\u5b9a\u4e00\u6b21\u5fae\u5206\u7b49\u65bc0 \u2192 \\(\\cos\\) Finite Medium \u00b6 take \\[ \\begin{gather} U(x, 0) = f(x) \\\\\\\\ U_{t}(x, 0) = g(x) \\\\\\\\ X'' + \\lambda X = 0 \\\\\\\\ T' + \\lambda k T = 0 \\end{gather} \\] \u6c92\u5fae\u5206\u5169\u7aef\u56fa\u5b9a = 0 \u00b6 \u6c92\u5fae\u5206\u5169\u7aef\u56fa\u5b9a \u2192 \\(\\sin\\) \\(U(0, t) = U(L, t) = 0\\) \\[ \\begin{align} &\\lambda_{n} = \\frac{n^{2}\\pi^{2}}{L^{2}} & n=1, 2, 3 , \\dots \\end{align} \\] \\[ \\begin{gather} X_{n} = \\sin(\\sqrt{\\lambda_{n}}x) \\\\\\\\ T_{n} = c_{n}e^{-\\lambda t} \\end{gather} \\] \\[ \\begin{gather} c_{n} = \\frac{2}{L}\\int_{0}^{L}{f(x)\\sin(\\sqrt\\lambda x)dx} \\\\\\\\ U(x, t) = \\sum{c_{n}\\sin(\\sqrt{\\lambda }x)\\ e^{^{-k\\lambda t}}} \\end{gather} \\] \u5169\u7aef\u56fa\u5b9a\u4e00\u6b21\u5fae\u5206\u7b49\u65bc0 \u00b6 \u5169\u7aef\u56fa\u5b9a\u4e00\u6b21\u5fae\u5206\u7b49\u65bc0 \u2192 \\(\\cos\\) \\(U_{x}(0, t) = U_{x}(L, t) = 0\\) \\[ \\begin{align} &\\lambda_{n} = \\frac{n^{2}\\pi^{2}}{L^{2}} & n=0, 1, 2, 3 , \\dots \\end{align} \\] \\[ \\begin{gather} X_{n} = \\cos(\\sqrt{\\lambda_{n}}x) \\\\\\\\ T_{n} = c_{n}e^{-\\lambda t} \\\\\\\\ c_{n} = \\frac{2}{L}\\int_{0}^{L}{f(x)\\cos(\\sqrt{\\lambda}x)dx} \\\\\\\\ U(x, t) = \\frac{1}{2}c_{0} + \\sum_{n=1}^{\\infty}{c_{n}\\cos(\\sqrt{\\lambda}x)\\ e^{-k\\lambda t}} \\end{gather} \\] \u6709 A \u00b6 \\(U(0, t) = 0; \\quad U_{x}(L, t) = -AU(L, t)\\) \u7528\u4f5c\u5716\u89e3 \\(\\tan(\\alpha L) = -\\frac{\\alpha}{A}\\) , \u4ee4\u89e3\u70ba \\(A_{1}, A_{2}, \\dots\\) \u89e3 case 3 (case 1, case 2 are trivial) \\[f(x) = \\sum_{n=1}^{\\infty}{c_{n}\\sin(\\sqrt{\\lambda}x)}\\] \\[c_{n} = \\frac{(f(x), \\sin(\\sqrt{\\lambda}x))}{||\\sin(\\sqrt{\\lambda}x)||}\\] \u53c3\u8003 Eigenfunction expansion \\[ \\begin{gather} U(x, t) = \\sum_{n=1}^{\\infty}{c_{n}\\sin(\\sqrt{\\lambda}x)e^{-k\\lambda t}} \\end{gather} \\] \u5169\u7aef\u56fa\u5b9a\u4e0d\u70ba 0 \u00b6 \\(U(0, t) = T_{1}, \\quad U(L, t) = T_{2}\\) Set \\(U(x, t) = u(x, t) + \\phi(x)\\) Then \\(u_{t} = k(u_{xx}+\\phi''(x))\\) Try let \\(\\phi''(x) = 0\\) solve \\(u(x, t)\\) \u6709\u5916\u529b \u00b6 \\(U_{t} = k U_{xx}+F(x, t)\\) \u4e14\u5169\u7aef\u56fa\u5b9a\u70ba 0 \\(U(x, t) =\\) \u5916\u529b\u4fee\u6b63 + \u6c92\u5916\u529b\u89e3 \\[B_{n}(t) = \\frac{2}{L}\\int_{0}^{L}{F(x, t)\\sin(\\sqrt{\\lambda}x)dx}\\] \\[ \\begin{gather} b_{n} = \\int_{0}^{t}{e^{-k\\lambda (t-\\tau)}B_{n}(\\tau)\\ d\\tau} \\end{gather} \\] \\[ \\begin{gather} U(x, t) = \\sum_{n=1}^{\\infty}{b_{n}\\sin(\\sqrt{\\lambda_{n}}x)}+ \\sum_{n=1}^{\\infty}{c_{n}\\sin{\\sqrt{\\lambda_{n}}}\\ e^{-k\\lambda_{n}t}} \\end{gather} \\] Infinite Media \u00b6 \\[ \\begin{gather} \\lambda_{n} = \\omega^{2} \\end{gather} \\] \\[ \\begin{gather} U(x, t) = \\int_{o}^{\\infty}{[a_{\\omega}\\cos(\\omega x)+b_\\omega \\sin(\\omega x)]e^{-k\\omega^{2}t}\\ d\\omega} \\\\\\\\ a_{\\omega} = \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty}{f(x)\\cos(\\omega x) dx} \\\\\\\\ b_{\\omega} = \\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}{f(x)\\sin(\\omega x)dx} \\end{gather} \\] Half infinite Media \u00b6 \\(x \\in (0, \\infty)\\)","title":"Heat Equation"},{"location":"Engineering%20Math/Heat%20Equations/#heat+equation","text":"1-D heat equation \\[ \\begin{gather} \\frac{\\partial U}{\\partial t} = k \\frac{\\partial^{2}U}{\\partial x^{2}} \\end{gather} \\] 3-D heat equation \\[ \\begin{gather} \\frac{\\partial U}{\\partial t} = k( \\frac{\\partial^{2}U}{\\partial x^{2}}+\\frac{\\partial^{2}U}{\\partial y^{2}} +\\frac{\\partial^{2}U}{\\partial z^{2}}) \\end{gather} \\]","title":"Heat Equation"},{"location":"Engineering%20Math/Heat%20Equations/#principle","text":"\u6c92\u5fae\u5206\u5169\u7aef\u56fa\u5b9a \u2192 \\(\\sin\\) \u5169\u7aef\u56fa\u5b9a\u4e00\u6b21\u5fae\u5206\u7b49\u65bc0 \u2192 \\(\\cos\\)","title":"Principle"},{"location":"Engineering%20Math/Heat%20Equations/#finite+medium","text":"take \\[ \\begin{gather} U(x, 0) = f(x) \\\\\\\\ U_{t}(x, 0) = g(x) \\\\\\\\ X'' + \\lambda X = 0 \\\\\\\\ T' + \\lambda k T = 0 \\end{gather} \\]","title":"Finite Medium"},{"location":"Engineering%20Math/Heat%20Equations/#\u6c92\u5fae\u5206\u5169\u7aef\u56fa\u5b9a++0","text":"\u6c92\u5fae\u5206\u5169\u7aef\u56fa\u5b9a \u2192 \\(\\sin\\) \\(U(0, t) = U(L, t) = 0\\) \\[ \\begin{align} &\\lambda_{n} = \\frac{n^{2}\\pi^{2}}{L^{2}} & n=1, 2, 3 , \\dots \\end{align} \\] \\[ \\begin{gather} X_{n} = \\sin(\\sqrt{\\lambda_{n}}x) \\\\\\\\ T_{n} = c_{n}e^{-\\lambda t} \\end{gather} \\] \\[ \\begin{gather} c_{n} = \\frac{2}{L}\\int_{0}^{L}{f(x)\\sin(\\sqrt\\lambda x)dx} \\\\\\\\ U(x, t) = \\sum{c_{n}\\sin(\\sqrt{\\lambda }x)\\ e^{^{-k\\lambda t}}} \\end{gather} \\]","title":"\u6c92\u5fae\u5206\u5169\u7aef\u56fa\u5b9a = 0"},{"location":"Engineering%20Math/Heat%20Equations/#\u5169\u7aef\u56fa\u5b9a\u4e00\u6b21\u5fae\u5206\u7b49\u65bc0","text":"\u5169\u7aef\u56fa\u5b9a\u4e00\u6b21\u5fae\u5206\u7b49\u65bc0 \u2192 \\(\\cos\\) \\(U_{x}(0, t) = U_{x}(L, t) = 0\\) \\[ \\begin{align} &\\lambda_{n} = \\frac{n^{2}\\pi^{2}}{L^{2}} & n=0, 1, 2, 3 , \\dots \\end{align} \\] \\[ \\begin{gather} X_{n} = \\cos(\\sqrt{\\lambda_{n}}x) \\\\\\\\ T_{n} = c_{n}e^{-\\lambda t} \\\\\\\\ c_{n} = \\frac{2}{L}\\int_{0}^{L}{f(x)\\cos(\\sqrt{\\lambda}x)dx} \\\\\\\\ U(x, t) = \\frac{1}{2}c_{0} + \\sum_{n=1}^{\\infty}{c_{n}\\cos(\\sqrt{\\lambda}x)\\ e^{-k\\lambda t}} \\end{gather} \\]","title":"\u5169\u7aef\u56fa\u5b9a\u4e00\u6b21\u5fae\u5206\u7b49\u65bc0"},{"location":"Engineering%20Math/Heat%20Equations/#\u6709+a","text":"\\(U(0, t) = 0; \\quad U_{x}(L, t) = -AU(L, t)\\) \u7528\u4f5c\u5716\u89e3 \\(\\tan(\\alpha L) = -\\frac{\\alpha}{A}\\) , \u4ee4\u89e3\u70ba \\(A_{1}, A_{2}, \\dots\\) \u89e3 case 3 (case 1, case 2 are trivial) \\[f(x) = \\sum_{n=1}^{\\infty}{c_{n}\\sin(\\sqrt{\\lambda}x)}\\] \\[c_{n} = \\frac{(f(x), \\sin(\\sqrt{\\lambda}x))}{||\\sin(\\sqrt{\\lambda}x)||}\\] \u53c3\u8003 Eigenfunction expansion \\[ \\begin{gather} U(x, t) = \\sum_{n=1}^{\\infty}{c_{n}\\sin(\\sqrt{\\lambda}x)e^{-k\\lambda t}} \\end{gather} \\]","title":"\u6709 A"},{"location":"Engineering%20Math/Heat%20Equations/#\u5169\u7aef\u56fa\u5b9a\u4e0d\u70ba+0","text":"\\(U(0, t) = T_{1}, \\quad U(L, t) = T_{2}\\) Set \\(U(x, t) = u(x, t) + \\phi(x)\\) Then \\(u_{t} = k(u_{xx}+\\phi''(x))\\) Try let \\(\\phi''(x) = 0\\) solve \\(u(x, t)\\)","title":"\u5169\u7aef\u56fa\u5b9a\u4e0d\u70ba 0"},{"location":"Engineering%20Math/Heat%20Equations/#\u6709\u5916\u529b","text":"\\(U_{t} = k U_{xx}+F(x, t)\\) \u4e14\u5169\u7aef\u56fa\u5b9a\u70ba 0 \\(U(x, t) =\\) \u5916\u529b\u4fee\u6b63 + \u6c92\u5916\u529b\u89e3 \\[B_{n}(t) = \\frac{2}{L}\\int_{0}^{L}{F(x, t)\\sin(\\sqrt{\\lambda}x)dx}\\] \\[ \\begin{gather} b_{n} = \\int_{0}^{t}{e^{-k\\lambda (t-\\tau)}B_{n}(\\tau)\\ d\\tau} \\end{gather} \\] \\[ \\begin{gather} U(x, t) = \\sum_{n=1}^{\\infty}{b_{n}\\sin(\\sqrt{\\lambda_{n}}x)}+ \\sum_{n=1}^{\\infty}{c_{n}\\sin{\\sqrt{\\lambda_{n}}}\\ e^{-k\\lambda_{n}t}} \\end{gather} \\]","title":"\u6709\u5916\u529b"},{"location":"Engineering%20Math/Heat%20Equations/#infinite+media","text":"\\[ \\begin{gather} \\lambda_{n} = \\omega^{2} \\end{gather} \\] \\[ \\begin{gather} U(x, t) = \\int_{o}^{\\infty}{[a_{\\omega}\\cos(\\omega x)+b_\\omega \\sin(\\omega x)]e^{-k\\omega^{2}t}\\ d\\omega} \\\\\\\\ a_{\\omega} = \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty}{f(x)\\cos(\\omega x) dx} \\\\\\\\ b_{\\omega} = \\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}{f(x)\\sin(\\omega x)dx} \\end{gather} \\]","title":"Infinite Media"},{"location":"Engineering%20Math/Heat%20Equations/#half+infinite+media","text":"\\(x \\in (0, \\infty)\\)","title":"Half infinite Media"},{"location":"Engineering%20Math/Higher%20Order%20DE/","text":"homogeneous Equations \u00b6 Definition \u00b6 \\[a_n(x)\\frac{d^ny}{dx^n}+a_{n-1}(x)\\frac{d^{n-1}y}{dx^{n-1}}+\\dots + a_1(x)\\frac{dy}{dx}+a_0(x)y=g(x)\\] or be written as \\[a_n(x)D^n + a_{n-1}(x)D^{n-1}+\\dots +a_1(x)D + a_0(x)y=g(x)\\] \\(g(x) = 0 \\qquad \\rightarrow \\quad\\) homogeneous \\(g(x) \\neq 0 \\qquad \\rightarrow \\quad\\) non-homogeneous General Solution (Complementary Function) \u00b6 Theorem 4.1.5 For an n th order homogeneous liner DE \\(L(y) = 0\\) , if \\(y_1(t), y_2(t), \\dots , y_n(t)\\) are the solution to \\(L(y) = 0\\) \\(y_1(t), y_2(t), \\dots , y_n(t)\\) are linearly independent Determinate whether they are liner independent : Wronskian . Expression Any solution of the homogeneous liner DE can be expressed as \\[y=c_1y_1 + c_2y_2 + \\dots + c_ny_n\\] Any n th order homogeneous linear DE has n linearly independent solutions. fundamental set of solutions \\[y_1(t), y_2(t), \\dots , y_n(t)\\] general solution (aka complementary function ) \\[y=c_1y_1 + c_2y_2 + \\dots + c_ny_n\\] 2nd with an known solution \u00b6 2 nd order linear homogeneous DE with an known solution \\[a_2(x)y''+a_1(x)y'+a_0(x)y=0\\] By virtue of solution below, we can get another solution and therefore get complementary function . Conditions \u00b6 second order linear homogeneous one of the nontrivial solution \\(y_1(x)\\) has been known #### Standard Form \\[y''+P(x)y'+Q(x)y=0\\] Solution \u00b6 (\u76f4\u63a5\u80cc\uff01\uff01\uff01) \\[y_2(x) = y_1(x) \\int{\\frac{e^{-\\int P(x)dx}}{y_1^2(x)}dx}\\] see proof . (TODO) Linear DE with Constant Coefficients \u00b6 Homogeneous linear DE with constant coefficients \\[a_ny^{(n)}(x) + a_{n-1}y^{(n-1)}(x) + \\dots + a_1y'(x) + a_0y= 0\\] Condition \u00b6 homogeneous linear constant coefficients kernel concept \u00b6 Suppose the solutions has the form of \\(e^{mx}\\) Auxiliary Function \u00b6 change \\(y^{(n)}\\) into \\(m^n\\) \\[a_nm^n+a_{n-1}m^{n-1}+\\dots + a_1m+a_0=0\\] solve \\(m\\) . Solution to 2nd Order \u00b6 \\[ \\begin{gather} a_2y''(x) + a_1y'(x) + a_0y(x) = 0\\\\ \\downarrow \\\\ a_2 m^2 + a_1 m + a_0 = 0 \\end{gather} \\] Case 1 : \\(m_1 \\neq m_2, \\quad m_1, m_2 \\in \u211d\\) , (D > 0) \\[m_1, m_2 = \\frac{-a_1 \\pm \\sqrt{a_1^2-4a_2a_0}}{2a_2}\\] thus \\[y = c_1e^{m_1x} + c_2e^{m_2x}\\] if \\[ m_1, m_2 = \\alpha \\ \\pm \\ \\beta \\] we can also write \\[ y=e^{\\alpha x}(c_1\\cosh{\\beta x} + c_2\\sinh{\\beta x}) \\] Case 2 \\(m_1 = m_2\\) , (D = 0) \\[y_1 = e^{m_1x}\\] \\(y_2\\) can be find by the method mentioned above. then we found that \\[y_2 = x\\ e^{m_1x}\\] Case 3 \\(m_1 \\neq m_2\\) , \\(m_1\\) and \\(m_2\\) are conjugate(\u5171\u8edb) and complex, (D < 0) \\[ \\begin{gather} m_1 = \\alpha + i\\beta \\qquad m_2 = \\alpha - i\\beta \\\\ \\\\ \\alpha = -\\frac{a_1}{2a_2}, \\qquad \\beta = \\frac{\\sqrt{4a_2a_0 - a_1^2}}{2a_2} \\end{gather} \\] thus \\[ \\begin{gather} y = C_1e^{\\alpha x + i\\beta x} + C_2 e^{\\alpha x - i\\beta x} \\end{gather} \\] another form : proof \\[y = e^{\\alpha x}(c_1 \\cos{\\beta x} + c_2 \\sin{\\beta x})\\] Solution to Higher Order \u00b6 n th order ODE \\(p, q \\in [1, n], \\quad \\text{and} \\quad p \\neq q\\) Case 1 : \\(m_p \\neq m_q\\) \u90fd\u662f\u7368\u7acb\u89e3 Case 2 : \u6709\u91cd\u6839 (\u5728 \\(m_p\\) \u8655\u91cd\u6839 k \u500b) solution : \\(\\quad e^{m_px},\\quad x\\cdot e^{m_px},\\quad x^2\\cdot e^{m_px}, \\dots ,\\quad x^{k-1}\\cdot e^{m_px}\\) Case 3 : \u6709k\u5c0d\u8907\u6578\u89e3 solutions : \\[ \\begin{align} e^{\\alpha x} \\cos(\\beta x),\\quad xe^{\\alpha x} \\cos(\\beta x),\\quad x^2e^{\\alpha x} \\cos(\\beta x),\\dots , x^{k-1}e^{\\alpha x}\\cos (\\beta x) \\\\\\\\ e^{\\alpha x} \\sin(\\beta x),\\quad xe^{\\alpha x} \\sin(\\beta x),\\quad x^2e^{\\alpha x} \\sin(\\beta x),\\dots , x^{k-1}e^{\\alpha x}\\sin (\\beta x) \\end{align} \\] Cauchy-Euler Equation \u00b6 Cauchy-Euler Equation is homogeneous linear DE in the form below \\[a_n x^n y^{(n)}(x) + a_{n-1}x^{n-1}y^{(n-1)}(x) + \\dots + a_1 xy'(x) + a_0y(x) = g(x) \\] Kernel Concept \u00b6 guess the solution has the form \\(y(x) = x^m\\) then we can change \\(x^k\\ \\frac{d^k}{dx^k}\\) to \\(\\frac{m!}{(m-k)!}\\) Solution to 2nd Order \u00b6 \\[a_2x^2y''(x) + a_1xy'(x) + a_0y = 0\\] auxiliary function : \\[ \\begin{align} a_2m(m-1) &+ a_1m &+ a_0 &= 0\\\\\\\\ a_2m^2 &+ (a_1 - a_2)m &+ a_0 &= 0 \\end{align} \\] than solve roots with \\[ \\begin{align} m = \\frac{-b \\pm \\sqrt{D}}{2a} \\end{align} \\] Case 1 : ( \\(m_1 \\neq m_2, \\quad m_1, m_2 \\in \u211d\\) ) \\[y_c = c_1x^{m_1} + c_2 x^{m_2}\\] Case 2 : ( \\(m_1 = m_2\\) ) use the method of reduction of order \\[y_2(x) = y_1(x) \\int{\\frac{e^{-\\int P(x)dx}}{y_1^2(x)}dx}\\] \\[y_1(x) = x^{m_1}\\] \\[y_2(x) = x^{m_1} \\cdot \\ln{|x|}\\] \u76f4\u63a5\u80cc\u7d50\u8ad6 \\(\\uparrow\\) Case 3 : ( \\(D < 0\\) ) \\[m_1 = \\alpha + i\\beta, \\qquad m_2 = \\alpha - i\\beta\\] \u76f4\u63a5\u80cc \\[y_c = x^{\\alpha}\\big[c_1\\cos{(\\beta \\ln x)} + c_2 \\sin{(\\beta \\ln x)}\\big]\\] Solution to Higher Order \u00b6 Case 1 : \u7686\u552f\u4e00\u89e3 \\[x^{m_p}\\] Case 2 : \u5728 \\(m_p\\) \u8655\u6709 \\(k\\) \u500b\u91cd\u6839 \\[x^{m_0}, \\quad x^{m_0}\\ln x,\\quad x^{m_0}\\cdot (\\ln x)^2, \\dots ,\\quad x^{m_0}\\cdot (\\ln x)^{k-1} \\] Case 3 : \u6709\u4e00\u5c0d\u8907\u6578\u6839 \\[x^{\\alpha}\\cos{(\\beta \\ln x)}, \\quad x^{\\alpha}\\sin{(\\beta \\ln x)} \\] Case 4 : \u6709 \\(k\\) \u5c0d\u8907\u6578\u89e3 \\[ \\begin{align} x^{\\alpha}\\cos{(\\beta \\ln x)}, \\quad x^{\\alpha}\\cos{(\\beta \\ln x)} \\cdot \\ln x \\ , \\dots,\\ x^{\\alpha}\\cos{(\\beta \\ln x)}\\cdot (\\ln x)^{k-1} \\\\\\\\ x^{\\alpha}\\sin{(\\beta \\ln x)}, \\quad x^{\\alpha}\\sin{(\\beta \\ln x)}\\cdot \\ln x\\ , \\dots,\\ x^{\\alpha}\\sin{(\\beta \\ln x)}\\cdot (\\ln x)^{k-1} \\end{align} \\] Non-homogeneous Equations \u00b6 Concept \u00b6 we can solve any non-homogeneous equations by \\[ y = y_c + y_p\\] in which \\(y_c\\) is the complementary function and \\(y_p\\) is the particular solution to equation. Linear DE with Constant Coefficients \u00b6 Condition \u00b6 linear constant coefficient \\(g^{(k)} \\quad (k \\in \u2115 )\\) contain finite number of terms. 1 Solution \u00b6 Trial particular solutions \\(g(x)\\) Form of \\(y_p\\) \\(\\text{const}\\qquad \\qquad\\) \\(A\\) \\(5x + 7\\) \\(Ax + B\\) \\(3x^2 - 2\\) \\(Ax^2 + Bx + C\\) \\(x^3 - x + 1\\) \\(Ax^3 + Bx^2 + Cx + D\\) \\(\\sin{4x}\\) \\(A\\cos{4x} + B\\sin{4x}\\) \\(\\cos{2x}\\) \\(A\\cos{2x} + B\\sin{2x}\\) \\(e^{5x}\\) \\(Ae^{5x}\\) \\((9x - 2)e^{5x}\\) \\((Ax + B)\\ e^{5x}\\) \\(x^2 e^{5x}\\) \\((Ax^2 + Bx + C) \\ e^{5x}\\) \\(x\\ e^{3x}\\sin{4x}\\) \\((Ax + B)\\ e^{3x}\\ \\cos{4x} + (Cx + D)\\ e^{3x}\\ \\sin{4x}\\) e.g. \\[g(x) = e^{2x} + xe^{3x}\\] \\[ \\begin{align}\\\\ & y_{p_1} = A\\ e^{2x} & y_{p_2} = (Bx+C)\\ e^{3x}\\\\\\\\ \\end{align} \\] \\[y_p = y_{p_1} + y_{p_2} = A\\ e^{2x} + (Bx+C)\\ e^{3x}\\] Glitch \u00b6 Condition 1 : particular cannot belong to complementary function. e.g. \\[y'' -5y' +4y= 8e^x\\] complementary function : \\[ y_c = c_1e^x + c_2e^{4x}\\] \\[ \\begin{gather} \\because Ae^{x} \\in y_c \\end{gather} \\] therefore we guess the particular solution with an extra \\(x\\) . \\[y_p = Axe^x\\] Condition2 : \\[g(x), \\ g'(x), \\ g''(x), \\ g'''(x), \\dots\\] can only contain infinite number of terms. Any Linear DE \u00b6 \\[a_n(x)y^{(n)}(x) + a_{n-1}(x)y^{(n-1)}+\\dots + a_1(x)y'(x) + a_0(x)y=g(x) \\] Variation of Parameters \u00b6 Solution to 2nd Order \\[a_2(x)y''(x) + a_1(x)y'(x) + a_0(x)y(x) = g(x)\\] or in the form \\[ y''(x)+P(x)y'+Q(x)y=f(x) \\] Suppose the complementary function is \\[c_1y_1(x) + c_2y_2(x)\\] then assume particular solution as \\[y_p=u_1(x)\\ y_1(x)+u_2(x)\\ y_2(x)\\] and then we can get \\[ \\Bigg\\{ \\begin{eqnarray} y_1u_1' + y_2u_2' &= 0\\\\ y_1'u_1' + y_2'u_2' &= f(x) \\end{eqnarray} \\] \\[ \\begin{align} \\\\u_1'= \\frac{W_1}{W} & \\qquad & u_1= \\int{u_1dx} \\\\u_2' = \\frac{W_2}{W} & & u_2 = \\int{u_2dx} \\end{align} \\] in which \\[ \\begin{align} W = \\text{Wronskian} &= \\begin{vmatrix} y_1 & y_2 \\\\ y_1' & y_2' \\\\ \\end{vmatrix} \\\\\\\\ W_1 &= \\begin{vmatrix} 0 & y_2 \\\\ f(x) & y_2' \\end{vmatrix} \\\\\\\\ W_2 &= \\begin{vmatrix} y_1 & 0\\\\ y_1' & f(x) \\end{vmatrix} \\end{align} \\] ref : Wronskian Higher Order \u00b6 similar to the way above. \\(y_p\\) should be a linear combination of \\[g(x), \\ g'(x), \\ g''(x), \\ g'''(x), \\dots\\] therefore, we won't get any redundant term. \u21a9","title":"Higher Order DE"},{"location":"Engineering%20Math/Higher%20Order%20DE/#homogeneous+equations","text":"","title":"homogeneous Equations"},{"location":"Engineering%20Math/Higher%20Order%20DE/#definition","text":"\\[a_n(x)\\frac{d^ny}{dx^n}+a_{n-1}(x)\\frac{d^{n-1}y}{dx^{n-1}}+\\dots + a_1(x)\\frac{dy}{dx}+a_0(x)y=g(x)\\] or be written as \\[a_n(x)D^n + a_{n-1}(x)D^{n-1}+\\dots +a_1(x)D + a_0(x)y=g(x)\\] \\(g(x) = 0 \\qquad \\rightarrow \\quad\\) homogeneous \\(g(x) \\neq 0 \\qquad \\rightarrow \\quad\\) non-homogeneous","title":"Definition"},{"location":"Engineering%20Math/Higher%20Order%20DE/#general+solution+complementary+function","text":"Theorem 4.1.5 For an n th order homogeneous liner DE \\(L(y) = 0\\) , if \\(y_1(t), y_2(t), \\dots , y_n(t)\\) are the solution to \\(L(y) = 0\\) \\(y_1(t), y_2(t), \\dots , y_n(t)\\) are linearly independent Determinate whether they are liner independent : Wronskian . Expression Any solution of the homogeneous liner DE can be expressed as \\[y=c_1y_1 + c_2y_2 + \\dots + c_ny_n\\] Any n th order homogeneous linear DE has n linearly independent solutions. fundamental set of solutions \\[y_1(t), y_2(t), \\dots , y_n(t)\\] general solution (aka complementary function ) \\[y=c_1y_1 + c_2y_2 + \\dots + c_ny_n\\]","title":"General Solution (Complementary Function)"},{"location":"Engineering%20Math/Higher%20Order%20DE/#2nd+with+an+known+solution","text":"2 nd order linear homogeneous DE with an known solution \\[a_2(x)y''+a_1(x)y'+a_0(x)y=0\\] By virtue of solution below, we can get another solution and therefore get complementary function .","title":"2nd with an known solution"},{"location":"Engineering%20Math/Higher%20Order%20DE/#conditions","text":"second order linear homogeneous one of the nontrivial solution \\(y_1(x)\\) has been known #### Standard Form \\[y''+P(x)y'+Q(x)y=0\\]","title":"Conditions"},{"location":"Engineering%20Math/Higher%20Order%20DE/#solution","text":"(\u76f4\u63a5\u80cc\uff01\uff01\uff01) \\[y_2(x) = y_1(x) \\int{\\frac{e^{-\\int P(x)dx}}{y_1^2(x)}dx}\\] see proof . (TODO)","title":"Solution"},{"location":"Engineering%20Math/Higher%20Order%20DE/#linear+de+with+constant+coefficients","text":"Homogeneous linear DE with constant coefficients \\[a_ny^{(n)}(x) + a_{n-1}y^{(n-1)}(x) + \\dots + a_1y'(x) + a_0y= 0\\]","title":"Linear DE with Constant Coefficients"},{"location":"Engineering%20Math/Higher%20Order%20DE/#condition","text":"homogeneous linear constant coefficients","title":"Condition"},{"location":"Engineering%20Math/Higher%20Order%20DE/#kernel+concept","text":"Suppose the solutions has the form of \\(e^{mx}\\)","title":"kernel concept"},{"location":"Engineering%20Math/Higher%20Order%20DE/#auxiliary+function","text":"change \\(y^{(n)}\\) into \\(m^n\\) \\[a_nm^n+a_{n-1}m^{n-1}+\\dots + a_1m+a_0=0\\] solve \\(m\\) .","title":"Auxiliary Function"},{"location":"Engineering%20Math/Higher%20Order%20DE/#solution+to+2nd+order","text":"\\[ \\begin{gather} a_2y''(x) + a_1y'(x) + a_0y(x) = 0\\\\ \\downarrow \\\\ a_2 m^2 + a_1 m + a_0 = 0 \\end{gather} \\] Case 1 : \\(m_1 \\neq m_2, \\quad m_1, m_2 \\in \u211d\\) , (D > 0) \\[m_1, m_2 = \\frac{-a_1 \\pm \\sqrt{a_1^2-4a_2a_0}}{2a_2}\\] thus \\[y = c_1e^{m_1x} + c_2e^{m_2x}\\] if \\[ m_1, m_2 = \\alpha \\ \\pm \\ \\beta \\] we can also write \\[ y=e^{\\alpha x}(c_1\\cosh{\\beta x} + c_2\\sinh{\\beta x}) \\] Case 2 \\(m_1 = m_2\\) , (D = 0) \\[y_1 = e^{m_1x}\\] \\(y_2\\) can be find by the method mentioned above. then we found that \\[y_2 = x\\ e^{m_1x}\\] Case 3 \\(m_1 \\neq m_2\\) , \\(m_1\\) and \\(m_2\\) are conjugate(\u5171\u8edb) and complex, (D < 0) \\[ \\begin{gather} m_1 = \\alpha + i\\beta \\qquad m_2 = \\alpha - i\\beta \\\\ \\\\ \\alpha = -\\frac{a_1}{2a_2}, \\qquad \\beta = \\frac{\\sqrt{4a_2a_0 - a_1^2}}{2a_2} \\end{gather} \\] thus \\[ \\begin{gather} y = C_1e^{\\alpha x + i\\beta x} + C_2 e^{\\alpha x - i\\beta x} \\end{gather} \\] another form : proof \\[y = e^{\\alpha x}(c_1 \\cos{\\beta x} + c_2 \\sin{\\beta x})\\]","title":"Solution to 2nd Order"},{"location":"Engineering%20Math/Higher%20Order%20DE/#solution+to+higher+order","text":"n th order ODE \\(p, q \\in [1, n], \\quad \\text{and} \\quad p \\neq q\\) Case 1 : \\(m_p \\neq m_q\\) \u90fd\u662f\u7368\u7acb\u89e3 Case 2 : \u6709\u91cd\u6839 (\u5728 \\(m_p\\) \u8655\u91cd\u6839 k \u500b) solution : \\(\\quad e^{m_px},\\quad x\\cdot e^{m_px},\\quad x^2\\cdot e^{m_px}, \\dots ,\\quad x^{k-1}\\cdot e^{m_px}\\) Case 3 : \u6709k\u5c0d\u8907\u6578\u89e3 solutions : \\[ \\begin{align} e^{\\alpha x} \\cos(\\beta x),\\quad xe^{\\alpha x} \\cos(\\beta x),\\quad x^2e^{\\alpha x} \\cos(\\beta x),\\dots , x^{k-1}e^{\\alpha x}\\cos (\\beta x) \\\\\\\\ e^{\\alpha x} \\sin(\\beta x),\\quad xe^{\\alpha x} \\sin(\\beta x),\\quad x^2e^{\\alpha x} \\sin(\\beta x),\\dots , x^{k-1}e^{\\alpha x}\\sin (\\beta x) \\end{align} \\]","title":"Solution to Higher Order"},{"location":"Engineering%20Math/Higher%20Order%20DE/#cauchy-euler+equation","text":"Cauchy-Euler Equation is homogeneous linear DE in the form below \\[a_n x^n y^{(n)}(x) + a_{n-1}x^{n-1}y^{(n-1)}(x) + \\dots + a_1 xy'(x) + a_0y(x) = g(x) \\]","title":"Cauchy-Euler Equation"},{"location":"Engineering%20Math/Higher%20Order%20DE/#kernel+concept_1","text":"guess the solution has the form \\(y(x) = x^m\\) then we can change \\(x^k\\ \\frac{d^k}{dx^k}\\) to \\(\\frac{m!}{(m-k)!}\\)","title":"Kernel Concept"},{"location":"Engineering%20Math/Higher%20Order%20DE/#solution+to+2nd+order_1","text":"\\[a_2x^2y''(x) + a_1xy'(x) + a_0y = 0\\] auxiliary function : \\[ \\begin{align} a_2m(m-1) &+ a_1m &+ a_0 &= 0\\\\\\\\ a_2m^2 &+ (a_1 - a_2)m &+ a_0 &= 0 \\end{align} \\] than solve roots with \\[ \\begin{align} m = \\frac{-b \\pm \\sqrt{D}}{2a} \\end{align} \\] Case 1 : ( \\(m_1 \\neq m_2, \\quad m_1, m_2 \\in \u211d\\) ) \\[y_c = c_1x^{m_1} + c_2 x^{m_2}\\] Case 2 : ( \\(m_1 = m_2\\) ) use the method of reduction of order \\[y_2(x) = y_1(x) \\int{\\frac{e^{-\\int P(x)dx}}{y_1^2(x)}dx}\\] \\[y_1(x) = x^{m_1}\\] \\[y_2(x) = x^{m_1} \\cdot \\ln{|x|}\\] \u76f4\u63a5\u80cc\u7d50\u8ad6 \\(\\uparrow\\) Case 3 : ( \\(D < 0\\) ) \\[m_1 = \\alpha + i\\beta, \\qquad m_2 = \\alpha - i\\beta\\] \u76f4\u63a5\u80cc \\[y_c = x^{\\alpha}\\big[c_1\\cos{(\\beta \\ln x)} + c_2 \\sin{(\\beta \\ln x)}\\big]\\]","title":"Solution to 2nd Order"},{"location":"Engineering%20Math/Higher%20Order%20DE/#solution+to+higher+order_1","text":"Case 1 : \u7686\u552f\u4e00\u89e3 \\[x^{m_p}\\] Case 2 : \u5728 \\(m_p\\) \u8655\u6709 \\(k\\) \u500b\u91cd\u6839 \\[x^{m_0}, \\quad x^{m_0}\\ln x,\\quad x^{m_0}\\cdot (\\ln x)^2, \\dots ,\\quad x^{m_0}\\cdot (\\ln x)^{k-1} \\] Case 3 : \u6709\u4e00\u5c0d\u8907\u6578\u6839 \\[x^{\\alpha}\\cos{(\\beta \\ln x)}, \\quad x^{\\alpha}\\sin{(\\beta \\ln x)} \\] Case 4 : \u6709 \\(k\\) \u5c0d\u8907\u6578\u89e3 \\[ \\begin{align} x^{\\alpha}\\cos{(\\beta \\ln x)}, \\quad x^{\\alpha}\\cos{(\\beta \\ln x)} \\cdot \\ln x \\ , \\dots,\\ x^{\\alpha}\\cos{(\\beta \\ln x)}\\cdot (\\ln x)^{k-1} \\\\\\\\ x^{\\alpha}\\sin{(\\beta \\ln x)}, \\quad x^{\\alpha}\\sin{(\\beta \\ln x)}\\cdot \\ln x\\ , \\dots,\\ x^{\\alpha}\\sin{(\\beta \\ln x)}\\cdot (\\ln x)^{k-1} \\end{align} \\]","title":"Solution to Higher Order"},{"location":"Engineering%20Math/Higher%20Order%20DE/#non-homogeneous+equations","text":"","title":"Non-homogeneous Equations"},{"location":"Engineering%20Math/Higher%20Order%20DE/#concept","text":"we can solve any non-homogeneous equations by \\[ y = y_c + y_p\\] in which \\(y_c\\) is the complementary function and \\(y_p\\) is the particular solution to equation.","title":"Concept"},{"location":"Engineering%20Math/Higher%20Order%20DE/#linear+de+with+constant+coefficients_1","text":"","title":"Linear DE with Constant Coefficients"},{"location":"Engineering%20Math/Higher%20Order%20DE/#condition_1","text":"linear constant coefficient \\(g^{(k)} \\quad (k \\in \u2115 )\\) contain finite number of terms. 1","title":"Condition"},{"location":"Engineering%20Math/Higher%20Order%20DE/#solution_1","text":"Trial particular solutions \\(g(x)\\) Form of \\(y_p\\) \\(\\text{const}\\qquad \\qquad\\) \\(A\\) \\(5x + 7\\) \\(Ax + B\\) \\(3x^2 - 2\\) \\(Ax^2 + Bx + C\\) \\(x^3 - x + 1\\) \\(Ax^3 + Bx^2 + Cx + D\\) \\(\\sin{4x}\\) \\(A\\cos{4x} + B\\sin{4x}\\) \\(\\cos{2x}\\) \\(A\\cos{2x} + B\\sin{2x}\\) \\(e^{5x}\\) \\(Ae^{5x}\\) \\((9x - 2)e^{5x}\\) \\((Ax + B)\\ e^{5x}\\) \\(x^2 e^{5x}\\) \\((Ax^2 + Bx + C) \\ e^{5x}\\) \\(x\\ e^{3x}\\sin{4x}\\) \\((Ax + B)\\ e^{3x}\\ \\cos{4x} + (Cx + D)\\ e^{3x}\\ \\sin{4x}\\) e.g. \\[g(x) = e^{2x} + xe^{3x}\\] \\[ \\begin{align}\\\\ & y_{p_1} = A\\ e^{2x} & y_{p_2} = (Bx+C)\\ e^{3x}\\\\\\\\ \\end{align} \\] \\[y_p = y_{p_1} + y_{p_2} = A\\ e^{2x} + (Bx+C)\\ e^{3x}\\]","title":"Solution"},{"location":"Engineering%20Math/Higher%20Order%20DE/#glitch","text":"Condition 1 : particular cannot belong to complementary function. e.g. \\[y'' -5y' +4y= 8e^x\\] complementary function : \\[ y_c = c_1e^x + c_2e^{4x}\\] \\[ \\begin{gather} \\because Ae^{x} \\in y_c \\end{gather} \\] therefore we guess the particular solution with an extra \\(x\\) . \\[y_p = Axe^x\\] Condition2 : \\[g(x), \\ g'(x), \\ g''(x), \\ g'''(x), \\dots\\] can only contain infinite number of terms.","title":"Glitch"},{"location":"Engineering%20Math/Higher%20Order%20DE/#any+linear+de","text":"\\[a_n(x)y^{(n)}(x) + a_{n-1}(x)y^{(n-1)}+\\dots + a_1(x)y'(x) + a_0(x)y=g(x) \\]","title":"Any Linear DE"},{"location":"Engineering%20Math/Higher%20Order%20DE/#variation+of+parameters","text":"Solution to 2nd Order \\[a_2(x)y''(x) + a_1(x)y'(x) + a_0(x)y(x) = g(x)\\] or in the form \\[ y''(x)+P(x)y'+Q(x)y=f(x) \\] Suppose the complementary function is \\[c_1y_1(x) + c_2y_2(x)\\] then assume particular solution as \\[y_p=u_1(x)\\ y_1(x)+u_2(x)\\ y_2(x)\\] and then we can get \\[ \\Bigg\\{ \\begin{eqnarray} y_1u_1' + y_2u_2' &= 0\\\\ y_1'u_1' + y_2'u_2' &= f(x) \\end{eqnarray} \\] \\[ \\begin{align} \\\\u_1'= \\frac{W_1}{W} & \\qquad & u_1= \\int{u_1dx} \\\\u_2' = \\frac{W_2}{W} & & u_2 = \\int{u_2dx} \\end{align} \\] in which \\[ \\begin{align} W = \\text{Wronskian} &= \\begin{vmatrix} y_1 & y_2 \\\\ y_1' & y_2' \\\\ \\end{vmatrix} \\\\\\\\ W_1 &= \\begin{vmatrix} 0 & y_2 \\\\ f(x) & y_2' \\end{vmatrix} \\\\\\\\ W_2 &= \\begin{vmatrix} y_1 & 0\\\\ y_1' & f(x) \\end{vmatrix} \\end{align} \\] ref : Wronskian","title":"Variation of Parameters"},{"location":"Engineering%20Math/Higher%20Order%20DE/#higher+order","text":"similar to the way above. \\(y_p\\) should be a linear combination of \\[g(x), \\ g'(x), \\ g''(x), \\ g'''(x), \\dots\\] therefore, we won't get any redundant term. \u21a9","title":"Higher Order"},{"location":"Engineering%20Math/Laplace%20Transform/","text":"Laplace Transform \u00b6 Definition \u00b6 \\[F(s) = L\\big\\{f(t)\\big\\} = \\int_{0}^{\\infty}{e^{-st}f(t)\\ dt} \\] The Laplace Transforms of Basic Functions \u00b6 \\(\\qquad f(t)\\qquad\\) \\(\\qquad F(s)\\qquad\\) \\(1\\) \\(1/s\\) \\(t^n\\) \\(\\frac{n!}{s^{n+1}}\\) \\(\\exp{(at)}\\) \\(\\frac{1}{s-a}\\) \\(t^ne^{at}\\) \\(\\frac{n!}{(s-a)^{n+1}}\\) \\(\\sin{(kt)}\\) \\(\\frac{k}{s^2 + k^2}\\) \\(\\cos{(kt)}\\) \\(\\frac{s}{s^2+k^2}\\) \\(\\sinh{(kt)}\\) \\(\\frac{k}{s^2-k^2}\\) \\(\\cosh{(kt)}\\) \\(\\frac{s}{s^2-k^2}\\) Constraints \u00b6 constraint 1 For a function \\(f(t)\\) , \\(\\exists\\) constant \\(c\\) , \\(M > 0\\) , and \\(T>0\\) s.t. \\[|f(t)| \\leq M\\ e^{ct}, \\ \\forall \\ t > T \\] \\(\\rightarrow f(t)\\) \u4e0d\u80fd\u8dd1\u5f97\u6bd4 \\(\\exp {(ct)}\\) \u5feb constraint 2 \\(f(t)\\) should be piecewise continuous on \\([0, \\infty)\\) \\(\\rightarrow\\ \\forall \\ t \\in [0, \\infty)\\) , \\(\\quad f(t)\\) \u70ba discontinuous \u7684\u9ede\u6709\u9650 note : these constraints are sufficient conditions \u4e94\u5927\u5b9a\u7406 Five Theorem \u00b6 Transforms of Derivatives \u00b6 \\[ \\begin{align} L\\big\\{f'(t)\\big\\} &= \\int_0^{\\infty}{e^{-st}\\ f'(t)\\ dt} = \\Bigg[e^{-st}\\ f(t)\\Bigg]^{\\infty}_0 + s\\ \\int_0^{\\infty}{e^{-st}\\ f(t)\\ dt} \\\\\\\\ &=sL\\big\\{f(t)\\big\\} - f(0) \\\\\\\\\\\\ L\\big\\{f''(t)\\big\\} &= s^2\\ F(s)-sf(0)-f'(0) \\\\\\\\ L\\big\\{f'''(t)\\big\\} &= s^3F(s) - s^2f(0)-sf'(0)-f''(0) \\\\\\\\ L\\big\\{f^{(n)}(t)\\big\\} &= s^nF(s)-s^{n-1}f(0) - s^{n-2}f'(0)-\\dots -sf^{(n-2)}-f^{(n-1)}(0) \\end{align} \\] First Translation Theorem \u00b6 translation for \\(s\\) \\[ L\\big\\{e^{at}\\ f(t)\\big\\}=F(s-a) \\] proof : \\[ \\begin{align} L\\big\\{e^{at}\\ f(t)\\big\\}&=\\int_0^{\\infty}{e^{-st}\\ e^{at}\\ f(t)\\ dt} = \\int_0^{\\infty}{e^{-(s-a)t}\\ f(t)\\ dt} \\\\\\\\ &= F(s-a) \\end{align} \\] Second Translation Theorem \u00b6 translation for \\(t\\) \\[ L\\big\\{f(t-a)\\cdot u(t-a)\\big\\} = e^{-as}\\ F(s) \\] or \\[ L\\big\\{g(t)\\ u(t-a)\\big\\}=e^{-as}\\ L\\big\\{g(t+a)\\big\\} \\] in which \\(u(t)\\) is an unit step function . proof : \\[ \\begin{align} L\\big\\{f(t-a)\\ u(t-a)\\big\\} &= \\int_0^{\\infty}{e^{-st}\\ f(t-a)\\ u(t-a)\\ dt}\\\\\\\\ &=\\int_a^{\\infty}{e^{-st}\\ f(t-a)\\ dt} \\qquad (\\text{let }t_1=t-a)\\\\\\\\ &= \\int_0^{\\infty}{e^{-s(t_1+a)}\\ f(t_1)\\ dt_1} = e^{-as}\\ F(s) \\end{align} \\] Derivatives of Transforms \u00b6 \\[ L\\big\\{t^n\\ f(t)\\big\\} = (-1)^n\\ \\frac{d^n}{ds^n}F(s) \\] Convolution \u00b6 Definition : \\[ \\begin{gather} f(t)*g(t)=\\int_{-\\infty}^{\\infty}{f(\\tau)\\ g(t-\\tau)\\ d\\tau} \\end{gather} \\] When \\(f(t)=0\\) for \\(t<0\\) and \\(g(t)=0\\) for \\(t<0\\) We have the same definition as below. \\[ \\begin{gather} f(t)*g(t)=\\int_0^t{f(\\tau)\\ g(t-\\tau)\\ d\\tau} \\end{gather} \\] Convolution Theorem \u00b6 \\[ L\\big\\{f(t)*g(t)\\big\\}=F(s)\\ G(s) \\] Integration \u00b6 from theorem above \\[ L\\bigg\\{\\int_0^t{f(\\tau)\\ d\\tau}\\bigg\\}=L\\big\\{f(t)*1\\big\\}=\\frac{F(s)}{s} \\] Transform of a Periodic Function \u00b6 When \\[f(t+T)=f(t)\\] then \\[ L\\big\\{f(t)\\big\\}=\\frac{1}{1-e^{-sT}}\\int_0^T{e^{-st}\\ f(t)\\ dt} \\] \\[ \\begin{align} \\\\\\\\ \\frac{1}{1-e^{-sT}} \\end{align} \\] \u4f86\u81ea\u7121\u7aae\u7b49\u6bd4\u7d1a\u6578 Dirac Delta Function \u00b6 \\[ \\delta(t-t_0)= \\Bigg\\{ \\begin{eqnarray} \\infty \\quad \\text{for}\\ t=t_0\\\\ 0 \\quad \\text{for}\\ t\\neq t_0 \\end{eqnarray} \\] Integration \u00b6 \\[\\int_{-\\infty}^{\\infty}{\\delta (t-t_0)\\ dt}=1\\] Sifting \u00b6 \\[ \\int_p^q{f(t)\\ \\delta (t-t_0)\\ dt}=f(t_0) \\] \\[t_0 \\in [p,q]\\] Laplace Transform \u00b6 \\[ L\\big\\{\\delta (t-t_0)\\big\\}=e^{-t_0s } \\] \\[ \\text{when}\\ t_0 >0 \\] proof : \\[ \\begin{align} \\because \\ \\delta(t-t_0)&=u'(t-t_0)\\\\\\\\ \\therefore \\ L\\big\\{\\delta(t-t_0)\\big\\}&=sL\\big\\{u(t-t_0)\\big\\}-u(0)\\\\\\\\ \\end{align} \\] due to second translation theorem , we have \\[ \\begin{align} L\\big\\{\\delta(t-t_0)\\big\\}=\\frac{e^{-t_0s}}{s}\\ s=e^{-t_0s} \\end{align} \\]","title":"Laplace Transform"},{"location":"Engineering%20Math/Laplace%20Transform/#laplace+transform","text":"","title":"Laplace Transform"},{"location":"Engineering%20Math/Laplace%20Transform/#definition","text":"\\[F(s) = L\\big\\{f(t)\\big\\} = \\int_{0}^{\\infty}{e^{-st}f(t)\\ dt} \\]","title":"Definition"},{"location":"Engineering%20Math/Laplace%20Transform/#the+laplace+transforms+of+basic+functions","text":"\\(\\qquad f(t)\\qquad\\) \\(\\qquad F(s)\\qquad\\) \\(1\\) \\(1/s\\) \\(t^n\\) \\(\\frac{n!}{s^{n+1}}\\) \\(\\exp{(at)}\\) \\(\\frac{1}{s-a}\\) \\(t^ne^{at}\\) \\(\\frac{n!}{(s-a)^{n+1}}\\) \\(\\sin{(kt)}\\) \\(\\frac{k}{s^2 + k^2}\\) \\(\\cos{(kt)}\\) \\(\\frac{s}{s^2+k^2}\\) \\(\\sinh{(kt)}\\) \\(\\frac{k}{s^2-k^2}\\) \\(\\cosh{(kt)}\\) \\(\\frac{s}{s^2-k^2}\\)","title":"The Laplace Transforms of Basic Functions"},{"location":"Engineering%20Math/Laplace%20Transform/#constraints","text":"constraint 1 For a function \\(f(t)\\) , \\(\\exists\\) constant \\(c\\) , \\(M > 0\\) , and \\(T>0\\) s.t. \\[|f(t)| \\leq M\\ e^{ct}, \\ \\forall \\ t > T \\] \\(\\rightarrow f(t)\\) \u4e0d\u80fd\u8dd1\u5f97\u6bd4 \\(\\exp {(ct)}\\) \u5feb constraint 2 \\(f(t)\\) should be piecewise continuous on \\([0, \\infty)\\) \\(\\rightarrow\\ \\forall \\ t \\in [0, \\infty)\\) , \\(\\quad f(t)\\) \u70ba discontinuous \u7684\u9ede\u6709\u9650 note : these constraints are sufficient conditions \u4e94\u5927\u5b9a\u7406","title":"Constraints"},{"location":"Engineering%20Math/Laplace%20Transform/#five+theorem","text":"","title":"Five Theorem"},{"location":"Engineering%20Math/Laplace%20Transform/#transforms+of+derivatives","text":"\\[ \\begin{align} L\\big\\{f'(t)\\big\\} &= \\int_0^{\\infty}{e^{-st}\\ f'(t)\\ dt} = \\Bigg[e^{-st}\\ f(t)\\Bigg]^{\\infty}_0 + s\\ \\int_0^{\\infty}{e^{-st}\\ f(t)\\ dt} \\\\\\\\ &=sL\\big\\{f(t)\\big\\} - f(0) \\\\\\\\\\\\ L\\big\\{f''(t)\\big\\} &= s^2\\ F(s)-sf(0)-f'(0) \\\\\\\\ L\\big\\{f'''(t)\\big\\} &= s^3F(s) - s^2f(0)-sf'(0)-f''(0) \\\\\\\\ L\\big\\{f^{(n)}(t)\\big\\} &= s^nF(s)-s^{n-1}f(0) - s^{n-2}f'(0)-\\dots -sf^{(n-2)}-f^{(n-1)}(0) \\end{align} \\]","title":"Transforms of Derivatives"},{"location":"Engineering%20Math/Laplace%20Transform/#first+translation+theorem","text":"translation for \\(s\\) \\[ L\\big\\{e^{at}\\ f(t)\\big\\}=F(s-a) \\] proof : \\[ \\begin{align} L\\big\\{e^{at}\\ f(t)\\big\\}&=\\int_0^{\\infty}{e^{-st}\\ e^{at}\\ f(t)\\ dt} = \\int_0^{\\infty}{e^{-(s-a)t}\\ f(t)\\ dt} \\\\\\\\ &= F(s-a) \\end{align} \\]","title":"First Translation Theorem"},{"location":"Engineering%20Math/Laplace%20Transform/#second+translation+theorem","text":"translation for \\(t\\) \\[ L\\big\\{f(t-a)\\cdot u(t-a)\\big\\} = e^{-as}\\ F(s) \\] or \\[ L\\big\\{g(t)\\ u(t-a)\\big\\}=e^{-as}\\ L\\big\\{g(t+a)\\big\\} \\] in which \\(u(t)\\) is an unit step function . proof : \\[ \\begin{align} L\\big\\{f(t-a)\\ u(t-a)\\big\\} &= \\int_0^{\\infty}{e^{-st}\\ f(t-a)\\ u(t-a)\\ dt}\\\\\\\\ &=\\int_a^{\\infty}{e^{-st}\\ f(t-a)\\ dt} \\qquad (\\text{let }t_1=t-a)\\\\\\\\ &= \\int_0^{\\infty}{e^{-s(t_1+a)}\\ f(t_1)\\ dt_1} = e^{-as}\\ F(s) \\end{align} \\]","title":"Second Translation Theorem"},{"location":"Engineering%20Math/Laplace%20Transform/#derivatives+of+transforms","text":"\\[ L\\big\\{t^n\\ f(t)\\big\\} = (-1)^n\\ \\frac{d^n}{ds^n}F(s) \\]","title":"Derivatives of Transforms"},{"location":"Engineering%20Math/Laplace%20Transform/#convolution","text":"Definition : \\[ \\begin{gather} f(t)*g(t)=\\int_{-\\infty}^{\\infty}{f(\\tau)\\ g(t-\\tau)\\ d\\tau} \\end{gather} \\] When \\(f(t)=0\\) for \\(t<0\\) and \\(g(t)=0\\) for \\(t<0\\) We have the same definition as below. \\[ \\begin{gather} f(t)*g(t)=\\int_0^t{f(\\tau)\\ g(t-\\tau)\\ d\\tau} \\end{gather} \\]","title":"Convolution"},{"location":"Engineering%20Math/Laplace%20Transform/#convolution+theorem","text":"\\[ L\\big\\{f(t)*g(t)\\big\\}=F(s)\\ G(s) \\]","title":"Convolution Theorem"},{"location":"Engineering%20Math/Laplace%20Transform/#integration","text":"from theorem above \\[ L\\bigg\\{\\int_0^t{f(\\tau)\\ d\\tau}\\bigg\\}=L\\big\\{f(t)*1\\big\\}=\\frac{F(s)}{s} \\]","title":"Integration"},{"location":"Engineering%20Math/Laplace%20Transform/#transform+of+a+periodic+function","text":"When \\[f(t+T)=f(t)\\] then \\[ L\\big\\{f(t)\\big\\}=\\frac{1}{1-e^{-sT}}\\int_0^T{e^{-st}\\ f(t)\\ dt} \\] \\[ \\begin{align} \\\\\\\\ \\frac{1}{1-e^{-sT}} \\end{align} \\] \u4f86\u81ea\u7121\u7aae\u7b49\u6bd4\u7d1a\u6578","title":"Transform of a Periodic Function"},{"location":"Engineering%20Math/Laplace%20Transform/#dirac+delta+function","text":"\\[ \\delta(t-t_0)= \\Bigg\\{ \\begin{eqnarray} \\infty \\quad \\text{for}\\ t=t_0\\\\ 0 \\quad \\text{for}\\ t\\neq t_0 \\end{eqnarray} \\]","title":"Dirac Delta Function"},{"location":"Engineering%20Math/Laplace%20Transform/#integration_1","text":"\\[\\int_{-\\infty}^{\\infty}{\\delta (t-t_0)\\ dt}=1\\]","title":"Integration"},{"location":"Engineering%20Math/Laplace%20Transform/#sifting","text":"\\[ \\int_p^q{f(t)\\ \\delta (t-t_0)\\ dt}=f(t_0) \\] \\[t_0 \\in [p,q]\\]","title":"Sifting"},{"location":"Engineering%20Math/Laplace%20Transform/#laplace+transform_1","text":"\\[ L\\big\\{\\delta (t-t_0)\\big\\}=e^{-t_0s } \\] \\[ \\text{when}\\ t_0 >0 \\] proof : \\[ \\begin{align} \\because \\ \\delta(t-t_0)&=u'(t-t_0)\\\\\\\\ \\therefore \\ L\\big\\{\\delta(t-t_0)\\big\\}&=sL\\big\\{u(t-t_0)\\big\\}-u(0)\\\\\\\\ \\end{align} \\] due to second translation theorem , we have \\[ \\begin{align} L\\big\\{\\delta(t-t_0)\\big\\}=\\frac{e^{-t_0s}}{s}\\ s=e^{-t_0s} \\end{align} \\]","title":"Laplace Transform"},{"location":"Engineering%20Math/Potential%20Equation/","text":"Potential Equation \u00b6 Laplace's Equation \\[ \\begin{align} &\\nabla^{2} U=0 \\\\\\\\ &\\frac{\\partial^{2} U}{\\partial x^{2}}+\\frac{\\partial^2 U}{\\partial y^{2}} =0 & \\text{2D} \\\\\\\\ &\\frac{\\partial^{2} U}{\\partial x^{2}}+\\frac{\\partial^2 U}{\\partial y^{2}}+\\frac{\\partial^2 U}{\\partial z^{2}}=0 & \\text{3D} \\end{align} \\] 2D Laplaces's Equation \u00b6 Polar Coordinate \u00b6 \\[z(x, y) = z(r\\cos{\\theta}, r\\sin{\\theta})=U(r, \\theta)\\] \\[\\frac{d(\\tan^{-1}(u))}{du} = \\frac{1}{1+u^{2}}\\] \\[ \\begin{gather} \\nabla^{2}z=U_{rr}+\\frac{1}{r}U_{r}+\\frac{1}{r^{2}}U_{\\theta\\theta} \\end{gather} \\] Cylindrical Coordinate \u00b6 \\[V(x, y, z) = V(r\\cos{\\theta}, r\\sin{\\theta}, z) = U(r, \\theta, z)\\] \\[ \\begin{gather} \\nabla^{2}=U_{rr}+\\frac{1}{r}U_{r}+\\frac{1}{r^{2}}U_{\\theta\\theta}+U_{zz} \\end{gather} \\] Spherical Coordinate \u00b6 \\[ \\begin{gather} x = \\rho \\cos(\\theta)\\sin(\\phi) \\\\\\\\ y = \\rho \\sin(\\theta)\\sin(\\phi) \\\\\\\\ z= \\rho \\cos(\\phi) \\end{gather} \\] \\[ \\begin{gather} \\nabla^{2}V = U_{\\rho\\rho}+\\frac{2}{\\rho}U_{\\rho}+\\frac{1}{\\rho^{2}\\sin\\phi}U_{\\theta\\theta}+\\frac{1}{\\rho^{2}}U_{\\phi\\phi}+\\frac{\\cot\\phi}{\\rho^{2}}U_{\\phi} \\end{gather} \\] Dirichlet Problem \u00b6 Rectangle \u00b6 \\[ \\begin{gather} \\nabla^{2}U(x,y) = 0 \\end{gather} \\] \u4e00\u6b21\u53ea\u80fd\u89e3\u4e00\u500b\u908a\u4e0d\u70ba 0 \u7684 case \u82e5\u6709\u5169\u500b\u908a\u4ee5\u4e0a\u4e0d\u70ba 0 \uff0c\u5206\u6210\u82e5\u5e72\u5b50\u554f\u984c\u4f86\u89e3 \\(u(x, y) = u_{1}(x, y) + u_{2}(x, t) + \\dots\\) Disk \u00b6 \\[ \\begin{gather} \\nabla^{2}U(r\\cos\\theta, r\\sin\\theta) = 0 \\end{gather} \\] Cauchy-Euler Equation let \\[u(r, \\theta) = P(r)\\ Q(\\theta)\\] then \\[ \\begin{align} & Q'' + \\lambda Q = 0 & \\#1 \\ ODE \\\\\\\\ & r^{2}P'' + rP' - P\\lambda = 0 & \\#2 \\ ODE \\end{align} \\] characteristic eq. of ODE #2: \\[ \\begin{gather} m^{2}+(1-1)m-\\lambda=0 \\\\\\\\ \\implies m = \\pm \\sqrt{\\lambda} \\end{gather} \\] Upper Half-Plane \u00b6 \\[ \\begin{align} &\\nabla^{2}u(x, y) = 0 & \\text{for } -\\infty < x < \\infty,\\quad y > 0 \\\\\\\\ &u(x, 0) = f(x) & \\text{for } -\\infty < x < \\infty \\end{align} \\] Quarter-Plane \u00b6 \\[ \\begin{align} &\\nabla^{2}u(x, y) = 0 & \\text{for } 0 < x < \\infty,\\quad y > 0 \\\\\\\\ &u(x, 0) = f(x) & \\text{for } 0 \\le x < \\infty \\\\\\\\ &u(0, y) = 0 & \\text{ for } y \\ge 0 \\end{align} \\] convert to Upper Half-Plane problem, let \\[ g(x) = \\left\\{ \\begin{align} & f(x) & \\text{for } x \\geq 0 \\\\\\\\ &\\text{anything} & \\text{for } x <0 \\end{align}\\right. \\] then solve \\[ \\begin{align} &\\nabla^{2}u(x, y) = 0 & \\text{for } -\\infty < x < \\infty,\\quad y > 0 \\\\\\\\ &u(x, 0) = g(x) & \\text{for } -\\infty < x < \\infty \\end{align} \\] Sphere in static state \u00b6 assume \\(u\\) is independent of \\(\\theta\\) \\[ \\begin{gather} \\frac{\\partial U}{\\partial t} = \\nabla^{2} U=0 \\end{gather} \\] let \\(u(\\rho, \\phi) = X(\\rho)\\ \\Phi(\\phi)\\) then \\[ \\begin{align} & \\Phi'' + \\cot(\\phi)\\Phi'+\\lambda\\Phi = 0 & \\#1\\ ODE \\\\\\\\ & \\rho^{2}X''+2\\rho X' -\\lambda X =0 & \\# 2\\ ODE \\end{align} \\] solve ODE #1 \u00b6 ODE # 1 can be converted to \\[ \\begin{gather} \\left[(1-x^{2})G'(x)\\right]+\\lambda G(x) = 0 \\\\\\\\ G(x) = \\Phi(\\phi), \\qquad (x = \\cos(\\phi)) \\end{gather} \\] which is a Legendre's Equation, then \\[ \\begin{gather} \\lambda_{n} =n(n+1), \\qquad n = 0, 1, 2, \\dots \\\\\\\\ \\Phi_{n}(\\phi) = G_{n}(x) = P_{n}(\\cos(\\phi)) \\end{gather} \\] solve ODE #2 \u00b6 \\[ \\begin{gather} \\rho^{2}X''+2\\rho X' -\\lambda X =0 \\\\\\\\ m^{2}+(2-1)m- n(n+1) =0 \\\\\\\\ \\implies m = -(n+1),\\ n \\end{gather} \\] and finally we get, \\[ \\begin{gather} u(\\rho, \\phi) = \\sum_{n=0}^{\\infty}{a_{n}\\rho^{n}P_{n}\\big(\\cos(\\phi)\\big)} \\\\\\\\ u(R, \\phi) = f(\\phi) = \\sum_{n=0}^{\\infty}{a_{n}R^{n}P_{n}\\big(\\cos(\\phi)\\big)} \\\\\\\\ \\implies a_{n}R^{n} = \\frac{<f\\big(\\cos^{-1}(x)\\big), P_{n}(x)>}{<P_{n}(x),P_{n}(x)>} \\end{gather} \\] ref : Fourier-Legendre Expansion \\[ \\begin{gather} a_{n}R^{n} = \\frac{2n+1}{2}\\int_{-1}^{1}{f(\\cos^{-1}(x))P_{n}(x)dx} \\end{gather} \\] Neumann Problem \u00b6 \\[ \\begin{align} &\\nabla^{2} u(x, y) = 0 & \\text{for } (x, y) \\in D \\\\\\\\ &\\frac{\\partial u}{\\partial n}(x, y)= g(x, y) & \\text{for } (x, y) \\in \\partial D \\end{align} \\] \\(D\\) \u8868\u793a\u5b9a\u7fa9\u5340\u57df\uff08\u9762\uff09\u3001 \\(\\partial D\\) \u8868\u793a boundary of \\(D\\) \\(\\frac{\\partial u}{\\partial n}\\) \u8868\u793a\u5c0d\u6cd5\u5411\u91cf\u5fae\u5206 necessary condition : \\[ \\begin{gather} \\oint_{\\partial D}{\\frac{\\partial u}{\\partial n}ds} = 0 \\end{gather} \\] \u4e0d\u6eff\u8db3\u5247\u7121\u89e3\uff08\u50b3\u8aaa\u4e2d\u6703\u8003\uff09","title":"Potential Equation"},{"location":"Engineering%20Math/Potential%20Equation/#potential+equation","text":"Laplace's Equation \\[ \\begin{align} &\\nabla^{2} U=0 \\\\\\\\ &\\frac{\\partial^{2} U}{\\partial x^{2}}+\\frac{\\partial^2 U}{\\partial y^{2}} =0 & \\text{2D} \\\\\\\\ &\\frac{\\partial^{2} U}{\\partial x^{2}}+\\frac{\\partial^2 U}{\\partial y^{2}}+\\frac{\\partial^2 U}{\\partial z^{2}}=0 & \\text{3D} \\end{align} \\]","title":"Potential Equation"},{"location":"Engineering%20Math/Potential%20Equation/#2d+laplacess+equation","text":"","title":"2D Laplaces's Equation"},{"location":"Engineering%20Math/Potential%20Equation/#polar+coordinate","text":"\\[z(x, y) = z(r\\cos{\\theta}, r\\sin{\\theta})=U(r, \\theta)\\] \\[\\frac{d(\\tan^{-1}(u))}{du} = \\frac{1}{1+u^{2}}\\] \\[ \\begin{gather} \\nabla^{2}z=U_{rr}+\\frac{1}{r}U_{r}+\\frac{1}{r^{2}}U_{\\theta\\theta} \\end{gather} \\]","title":"Polar Coordinate"},{"location":"Engineering%20Math/Potential%20Equation/#cylindrical+coordinate","text":"\\[V(x, y, z) = V(r\\cos{\\theta}, r\\sin{\\theta}, z) = U(r, \\theta, z)\\] \\[ \\begin{gather} \\nabla^{2}=U_{rr}+\\frac{1}{r}U_{r}+\\frac{1}{r^{2}}U_{\\theta\\theta}+U_{zz} \\end{gather} \\]","title":"Cylindrical Coordinate"},{"location":"Engineering%20Math/Potential%20Equation/#spherical+coordinate","text":"\\[ \\begin{gather} x = \\rho \\cos(\\theta)\\sin(\\phi) \\\\\\\\ y = \\rho \\sin(\\theta)\\sin(\\phi) \\\\\\\\ z= \\rho \\cos(\\phi) \\end{gather} \\] \\[ \\begin{gather} \\nabla^{2}V = U_{\\rho\\rho}+\\frac{2}{\\rho}U_{\\rho}+\\frac{1}{\\rho^{2}\\sin\\phi}U_{\\theta\\theta}+\\frac{1}{\\rho^{2}}U_{\\phi\\phi}+\\frac{\\cot\\phi}{\\rho^{2}}U_{\\phi} \\end{gather} \\]","title":"Spherical Coordinate"},{"location":"Engineering%20Math/Potential%20Equation/#dirichlet+problem","text":"","title":"Dirichlet Problem"},{"location":"Engineering%20Math/Potential%20Equation/#rectangle","text":"\\[ \\begin{gather} \\nabla^{2}U(x,y) = 0 \\end{gather} \\] \u4e00\u6b21\u53ea\u80fd\u89e3\u4e00\u500b\u908a\u4e0d\u70ba 0 \u7684 case \u82e5\u6709\u5169\u500b\u908a\u4ee5\u4e0a\u4e0d\u70ba 0 \uff0c\u5206\u6210\u82e5\u5e72\u5b50\u554f\u984c\u4f86\u89e3 \\(u(x, y) = u_{1}(x, y) + u_{2}(x, t) + \\dots\\)","title":"Rectangle"},{"location":"Engineering%20Math/Potential%20Equation/#disk","text":"\\[ \\begin{gather} \\nabla^{2}U(r\\cos\\theta, r\\sin\\theta) = 0 \\end{gather} \\] Cauchy-Euler Equation let \\[u(r, \\theta) = P(r)\\ Q(\\theta)\\] then \\[ \\begin{align} & Q'' + \\lambda Q = 0 & \\#1 \\ ODE \\\\\\\\ & r^{2}P'' + rP' - P\\lambda = 0 & \\#2 \\ ODE \\end{align} \\] characteristic eq. of ODE #2: \\[ \\begin{gather} m^{2}+(1-1)m-\\lambda=0 \\\\\\\\ \\implies m = \\pm \\sqrt{\\lambda} \\end{gather} \\]","title":"Disk"},{"location":"Engineering%20Math/Potential%20Equation/#upper+half-plane","text":"\\[ \\begin{align} &\\nabla^{2}u(x, y) = 0 & \\text{for } -\\infty < x < \\infty,\\quad y > 0 \\\\\\\\ &u(x, 0) = f(x) & \\text{for } -\\infty < x < \\infty \\end{align} \\]","title":"Upper Half-Plane"},{"location":"Engineering%20Math/Potential%20Equation/#quarter-plane","text":"\\[ \\begin{align} &\\nabla^{2}u(x, y) = 0 & \\text{for } 0 < x < \\infty,\\quad y > 0 \\\\\\\\ &u(x, 0) = f(x) & \\text{for } 0 \\le x < \\infty \\\\\\\\ &u(0, y) = 0 & \\text{ for } y \\ge 0 \\end{align} \\] convert to Upper Half-Plane problem, let \\[ g(x) = \\left\\{ \\begin{align} & f(x) & \\text{for } x \\geq 0 \\\\\\\\ &\\text{anything} & \\text{for } x <0 \\end{align}\\right. \\] then solve \\[ \\begin{align} &\\nabla^{2}u(x, y) = 0 & \\text{for } -\\infty < x < \\infty,\\quad y > 0 \\\\\\\\ &u(x, 0) = g(x) & \\text{for } -\\infty < x < \\infty \\end{align} \\]","title":"Quarter-Plane"},{"location":"Engineering%20Math/Potential%20Equation/#sphere+in+static+state","text":"assume \\(u\\) is independent of \\(\\theta\\) \\[ \\begin{gather} \\frac{\\partial U}{\\partial t} = \\nabla^{2} U=0 \\end{gather} \\] let \\(u(\\rho, \\phi) = X(\\rho)\\ \\Phi(\\phi)\\) then \\[ \\begin{align} & \\Phi'' + \\cot(\\phi)\\Phi'+\\lambda\\Phi = 0 & \\#1\\ ODE \\\\\\\\ & \\rho^{2}X''+2\\rho X' -\\lambda X =0 & \\# 2\\ ODE \\end{align} \\]","title":"Sphere in static state"},{"location":"Engineering%20Math/Potential%20Equation/#solve+ode+1","text":"ODE # 1 can be converted to \\[ \\begin{gather} \\left[(1-x^{2})G'(x)\\right]+\\lambda G(x) = 0 \\\\\\\\ G(x) = \\Phi(\\phi), \\qquad (x = \\cos(\\phi)) \\end{gather} \\] which is a Legendre's Equation, then \\[ \\begin{gather} \\lambda_{n} =n(n+1), \\qquad n = 0, 1, 2, \\dots \\\\\\\\ \\Phi_{n}(\\phi) = G_{n}(x) = P_{n}(\\cos(\\phi)) \\end{gather} \\]","title":"solve ODE #1"},{"location":"Engineering%20Math/Potential%20Equation/#solve+ode+2","text":"\\[ \\begin{gather} \\rho^{2}X''+2\\rho X' -\\lambda X =0 \\\\\\\\ m^{2}+(2-1)m- n(n+1) =0 \\\\\\\\ \\implies m = -(n+1),\\ n \\end{gather} \\] and finally we get, \\[ \\begin{gather} u(\\rho, \\phi) = \\sum_{n=0}^{\\infty}{a_{n}\\rho^{n}P_{n}\\big(\\cos(\\phi)\\big)} \\\\\\\\ u(R, \\phi) = f(\\phi) = \\sum_{n=0}^{\\infty}{a_{n}R^{n}P_{n}\\big(\\cos(\\phi)\\big)} \\\\\\\\ \\implies a_{n}R^{n} = \\frac{<f\\big(\\cos^{-1}(x)\\big), P_{n}(x)>}{<P_{n}(x),P_{n}(x)>} \\end{gather} \\] ref : Fourier-Legendre Expansion \\[ \\begin{gather} a_{n}R^{n} = \\frac{2n+1}{2}\\int_{-1}^{1}{f(\\cos^{-1}(x))P_{n}(x)dx} \\end{gather} \\]","title":"solve ODE #2"},{"location":"Engineering%20Math/Potential%20Equation/#neumann+problem","text":"\\[ \\begin{align} &\\nabla^{2} u(x, y) = 0 & \\text{for } (x, y) \\in D \\\\\\\\ &\\frac{\\partial u}{\\partial n}(x, y)= g(x, y) & \\text{for } (x, y) \\in \\partial D \\end{align} \\] \\(D\\) \u8868\u793a\u5b9a\u7fa9\u5340\u57df\uff08\u9762\uff09\u3001 \\(\\partial D\\) \u8868\u793a boundary of \\(D\\) \\(\\frac{\\partial u}{\\partial n}\\) \u8868\u793a\u5c0d\u6cd5\u5411\u91cf\u5fae\u5206 necessary condition : \\[ \\begin{gather} \\oint_{\\partial D}{\\frac{\\partial u}{\\partial n}ds} = 0 \\end{gather} \\] \u4e0d\u6eff\u8db3\u5247\u7121\u89e3\uff08\u50b3\u8aaa\u4e2d\u6703\u8003\uff09","title":"Neumann Problem"},{"location":"Engineering%20Math/Series%20Solution/","text":"Power Series Solution \u00b6 Analytic \u00b6 A function \\(f(x)\\) is analytic at \\(x_0\\) if \\(f(x)\\) has a power series representation in some open intervals about \\(x_0\\) , that is \\[ \\begin{gather} f(x) = \\sum_{n=0}^{\\infty}{a_n\\ (x-x_0)^n} \\quad \\text{ for } x \\in (x_0-h, x_0+h) \\end{gather} \\] where \\(a_n\\) are the Taylor coefficients of \\(f(x)\\) at \\(x_0\\) : \\[ \\begin{gather} a_n=\\frac{1}{ n!}f^{(n)}(x_0) \\end{gather} \\] Frobenius Solutions \u00b6 for second linear ODE \\[ \\begin{gather} P(x)y'' + Q(x)y' + R(x)y=F(x) \\\\\\\\ \\text{or} \\\\\\\\ y''+p(x)y'+q(x)y=f(x) \\end{gather} \\] Ordinary Points \u00b6 \\(x_0\\) is an ordinary point if \\(P(x_0) \\neq 0\\) , and \\(\\frac{Q}{P}\\) , \\(\\frac{R}{P}\\) , and \\(\\frac{F}{P}\\) are analytic at \\(x_0\\) . Singular Points \u00b6 \\(x_0\\) is a singular point if \\(x_0\\) is not an ordinary point. Regular Singular Points \u00b6 \\(x_0\\) is a regular singular point if \\(x_0\\) is a singular point and the function \\((x-x_0)\\frac{Q}{P}\\) and \\((x-x_0)^2\\frac{R}{P}\\) are analytic at \\(x_0\\) . Irregular Singular Points \u00b6 A singular point \\(x_0\\) is a irregular point if it's not a regular singular points. Frobenius Series \u00b6 \\[ \\begin{gather} y(x)=\\sum_{n=0}^{\\infty}{c_n(x-x_0)^{n+r}} \\end{gather} \\] r may be negative or even non-integer notice that \\[ \\begin{gather} y' = \\sum_{n=\\bf{0}}^{\\infty}c_n(n+r)(x-x_0)^{n+r-1} \\end{gather} \\] summation start from \\(n=0\\) instead of \\(n=1\\) Frobenius Methods \u00b6 the second order ODE \\[ \\begin{gather} P\\ y'' + Q\\ y' + R\\ y=F(x) \\end{gather} \\] which \\(x_0\\) is a Regular Singular Points by solving it with \\[y(x)=\\sum_{n=0}^{\\infty}{c_n(x-x_0)^{n+r}}\\] we can get two root \\(r_1, r_2\\) Case \\(|r_1-r_2| \\notin \u2115\\) 2 linearly independent Frobenius solutions \\(y_1(x)=\\sum{c_n(x-x_0)^{n+r_1}}\\) \\(y_2(x) =\\sum{c'_n(x-x_0)^{n+r_2}}\\) \\(c_0,\\ c'_0 \\neq 0\\) Case \\(r_1 = r_2\\) 2 linearly independent Frobenius solutions \\(y_1(x)=\\sum{c_n(x-x_0)^{n+r_1}}\\) \\(y_2(x) = y_1(x)\\cdot \\ln{(x)} + \\sum{c'_n(x-x_0)^{n+r_1}}\\) \\(c_0,\\ c'_0 \\neq 0\\) Case \\(|r_1 - r_2| \\in \u2115\\) \\(y_1(x)=\\sum{c_n(x-x_0)^{n+r_1}}\\) \\(y_2(x) = k\\ y_1(x)\\ \\ln{(x)}+\\sum{c'_n(x-x_0)^{n+r_2}}\\) \\(c_0,\\ c'_0 \\neq 0\\)","title":"Series Solution"},{"location":"Engineering%20Math/Series%20Solution/#power+series+solution","text":"","title":"Power Series Solution"},{"location":"Engineering%20Math/Series%20Solution/#analytic","text":"A function \\(f(x)\\) is analytic at \\(x_0\\) if \\(f(x)\\) has a power series representation in some open intervals about \\(x_0\\) , that is \\[ \\begin{gather} f(x) = \\sum_{n=0}^{\\infty}{a_n\\ (x-x_0)^n} \\quad \\text{ for } x \\in (x_0-h, x_0+h) \\end{gather} \\] where \\(a_n\\) are the Taylor coefficients of \\(f(x)\\) at \\(x_0\\) : \\[ \\begin{gather} a_n=\\frac{1}{ n!}f^{(n)}(x_0) \\end{gather} \\]","title":"Analytic"},{"location":"Engineering%20Math/Series%20Solution/#frobenius+solutions","text":"for second linear ODE \\[ \\begin{gather} P(x)y'' + Q(x)y' + R(x)y=F(x) \\\\\\\\ \\text{or} \\\\\\\\ y''+p(x)y'+q(x)y=f(x) \\end{gather} \\]","title":"Frobenius Solutions"},{"location":"Engineering%20Math/Series%20Solution/#ordinary+points","text":"\\(x_0\\) is an ordinary point if \\(P(x_0) \\neq 0\\) , and \\(\\frac{Q}{P}\\) , \\(\\frac{R}{P}\\) , and \\(\\frac{F}{P}\\) are analytic at \\(x_0\\) .","title":"Ordinary Points"},{"location":"Engineering%20Math/Series%20Solution/#singular+points","text":"\\(x_0\\) is a singular point if \\(x_0\\) is not an ordinary point.","title":"Singular Points"},{"location":"Engineering%20Math/Series%20Solution/#regular+singular+points","text":"\\(x_0\\) is a regular singular point if \\(x_0\\) is a singular point and the function \\((x-x_0)\\frac{Q}{P}\\) and \\((x-x_0)^2\\frac{R}{P}\\) are analytic at \\(x_0\\) .","title":"Regular Singular Points"},{"location":"Engineering%20Math/Series%20Solution/#irregular+singular+points","text":"A singular point \\(x_0\\) is a irregular point if it's not a regular singular points.","title":"Irregular Singular Points"},{"location":"Engineering%20Math/Series%20Solution/#frobenius+series","text":"\\[ \\begin{gather} y(x)=\\sum_{n=0}^{\\infty}{c_n(x-x_0)^{n+r}} \\end{gather} \\] r may be negative or even non-integer notice that \\[ \\begin{gather} y' = \\sum_{n=\\bf{0}}^{\\infty}c_n(n+r)(x-x_0)^{n+r-1} \\end{gather} \\] summation start from \\(n=0\\) instead of \\(n=1\\)","title":"Frobenius Series"},{"location":"Engineering%20Math/Series%20Solution/#frobenius+methods","text":"the second order ODE \\[ \\begin{gather} P\\ y'' + Q\\ y' + R\\ y=F(x) \\end{gather} \\] which \\(x_0\\) is a Regular Singular Points by solving it with \\[y(x)=\\sum_{n=0}^{\\infty}{c_n(x-x_0)^{n+r}}\\] we can get two root \\(r_1, r_2\\) Case \\(|r_1-r_2| \\notin \u2115\\) 2 linearly independent Frobenius solutions \\(y_1(x)=\\sum{c_n(x-x_0)^{n+r_1}}\\) \\(y_2(x) =\\sum{c'_n(x-x_0)^{n+r_2}}\\) \\(c_0,\\ c'_0 \\neq 0\\) Case \\(r_1 = r_2\\) 2 linearly independent Frobenius solutions \\(y_1(x)=\\sum{c_n(x-x_0)^{n+r_1}}\\) \\(y_2(x) = y_1(x)\\cdot \\ln{(x)} + \\sum{c'_n(x-x_0)^{n+r_1}}\\) \\(c_0,\\ c'_0 \\neq 0\\) Case \\(|r_1 - r_2| \\in \u2115\\) \\(y_1(x)=\\sum{c_n(x-x_0)^{n+r_1}}\\) \\(y_2(x) = k\\ y_1(x)\\ \\ln{(x)}+\\sum{c'_n(x-x_0)^{n+r_2}}\\) \\(c_0,\\ c'_0 \\neq 0\\)","title":"Frobenius Methods"},{"location":"Engineering%20Math/Special%20Functions/","text":"Legendre Polynomials \u00b6 \\[ \\begin{gather} (1-x^2)y''-2xy'+\\lambda y=0 \\\\\\\\ (1-x^2)y'' - 2xy' + n(n+1)y=0 \\\\\\\\ \\big[(1-x^2)y'\\big]+\\lambda y = 0 \\end{gather} \\] solve the equation with \\(y = \\sum a_m x^m\\) , we can get the conclusion that \\[ \\begin{gather} \\lambda_{n} = n(n+1), \\qquad \\text{for } n = 0, 1,2 , \\dots \\\\\\\\ a_{m+2} = \\frac{m(m+1)-\\lambda_{n}}{(m+2)(m+1)}a_m \\end{gather} \\] thus \\(a_{n+2} = a_{n+4} = \\dots = 0\\) Normalization \u00b6 Let the polynomial \\(P_0(x)=1\\) , \\(P_1(x) = x\\) Recurrence relation : (\u80cc) \\[\\begin{align} (n+1)P_{n+1} = (2n+1)x P_n - nP_{n-1} \\end{align} \\] Fourier-Legendre Expansion \u00b6 \\(f(x)\\) is defined for \\(-1 < x< 1\\) \\[ \\begin{gather} f(x) = \\sum_{n= 0}^{\\infty}{c_{n}P_{n}(x)} \\\\\\\\ c_{n} = \\frac{(f, P_{n})}{(P_{n}, P_{n})} = \\frac{\\int_{-1}^{1}{f(x)}P_{n}(x)dx}{\\int_{-1}^{1}{P^{2}_{n}(x)dx}} \\\\\\\\ \\int_{-1}^{1}{P^{2}_{n}(x)dx} = \\frac{2}{2n+1} \\end{gather} \\] Bessel Functions \u00b6 definition \\[ \\begin{align} x^2y'' + xy' + (x^2 - \\nu^2)y = 0 \\end{align} \\] solution ( for \\(J_\\nu\\) and \\(J_{-\\nu}\\) are independent) \\(2\\nu \\notin \u2115\\) \\(2\\nu\\) is odd positive integer \\[ \\begin{gather} y(x) = c_1 J_\\nu(x) + c_2J_{-\\nu}(x) \\end{gather} \\] solution ( for \\(J_\\nu\\) and \\(J_{-\\nu}\\) are not independent) \\(2\\nu\\) is even positive integer \\[ \\begin{gather} y(x) = c_1 J_\\nu(x) + c_2Y_\\nu(x) \\end{gather} \\] \\(J_\\nu\\) is called a Bessel function of the 1st kind of order \\(\\nu\\) \\[ \\begin{gather} J_\\nu = \\sum_{n=0}^{\\infty}{\\frac{(-1)^n}{2^{n+\\nu}n!\\ \\Gamma(n+\\nu+1)}x^{2n+\\nu}} \\end{gather} \\] \\(Y_\\nu \\rightarrow Y_{0}(0) = -\\infty\\) Recurrence Relations \u00b6 \\[ \\begin{gather} \\frac{d}{dx}(x^\\nu J_\\nu) = x^\\nu J_{\\nu-1} \\\\\\\\ \\frac{d}{dx}(x^{-\\nu}J_\\nu)=-x^{-\\nu}J_{\\nu+1} \\end{gather} \\]","title":"Special Functions"},{"location":"Engineering%20Math/Special%20Functions/#legendre+polynomials","text":"\\[ \\begin{gather} (1-x^2)y''-2xy'+\\lambda y=0 \\\\\\\\ (1-x^2)y'' - 2xy' + n(n+1)y=0 \\\\\\\\ \\big[(1-x^2)y'\\big]+\\lambda y = 0 \\end{gather} \\] solve the equation with \\(y = \\sum a_m x^m\\) , we can get the conclusion that \\[ \\begin{gather} \\lambda_{n} = n(n+1), \\qquad \\text{for } n = 0, 1,2 , \\dots \\\\\\\\ a_{m+2} = \\frac{m(m+1)-\\lambda_{n}}{(m+2)(m+1)}a_m \\end{gather} \\] thus \\(a_{n+2} = a_{n+4} = \\dots = 0\\)","title":"Legendre Polynomials"},{"location":"Engineering%20Math/Special%20Functions/#normalization","text":"Let the polynomial \\(P_0(x)=1\\) , \\(P_1(x) = x\\) Recurrence relation : (\u80cc) \\[\\begin{align} (n+1)P_{n+1} = (2n+1)x P_n - nP_{n-1} \\end{align} \\]","title":"Normalization"},{"location":"Engineering%20Math/Special%20Functions/#fourier-legendre+expansion","text":"\\(f(x)\\) is defined for \\(-1 < x< 1\\) \\[ \\begin{gather} f(x) = \\sum_{n= 0}^{\\infty}{c_{n}P_{n}(x)} \\\\\\\\ c_{n} = \\frac{(f, P_{n})}{(P_{n}, P_{n})} = \\frac{\\int_{-1}^{1}{f(x)}P_{n}(x)dx}{\\int_{-1}^{1}{P^{2}_{n}(x)dx}} \\\\\\\\ \\int_{-1}^{1}{P^{2}_{n}(x)dx} = \\frac{2}{2n+1} \\end{gather} \\]","title":"Fourier-Legendre Expansion"},{"location":"Engineering%20Math/Special%20Functions/#bessel+functions","text":"definition \\[ \\begin{align} x^2y'' + xy' + (x^2 - \\nu^2)y = 0 \\end{align} \\] solution ( for \\(J_\\nu\\) and \\(J_{-\\nu}\\) are independent) \\(2\\nu \\notin \u2115\\) \\(2\\nu\\) is odd positive integer \\[ \\begin{gather} y(x) = c_1 J_\\nu(x) + c_2J_{-\\nu}(x) \\end{gather} \\] solution ( for \\(J_\\nu\\) and \\(J_{-\\nu}\\) are not independent) \\(2\\nu\\) is even positive integer \\[ \\begin{gather} y(x) = c_1 J_\\nu(x) + c_2Y_\\nu(x) \\end{gather} \\] \\(J_\\nu\\) is called a Bessel function of the 1st kind of order \\(\\nu\\) \\[ \\begin{gather} J_\\nu = \\sum_{n=0}^{\\infty}{\\frac{(-1)^n}{2^{n+\\nu}n!\\ \\Gamma(n+\\nu+1)}x^{2n+\\nu}} \\end{gather} \\] \\(Y_\\nu \\rightarrow Y_{0}(0) = -\\infty\\)","title":"Bessel Functions"},{"location":"Engineering%20Math/Special%20Functions/#recurrence+relations","text":"\\[ \\begin{gather} \\frac{d}{dx}(x^\\nu J_\\nu) = x^\\nu J_{\\nu-1} \\\\\\\\ \\frac{d}{dx}(x^{-\\nu}J_\\nu)=-x^{-\\nu}J_{\\nu+1} \\end{gather} \\]","title":"Recurrence Relations"},{"location":"Engineering%20Math/Sturm-Liouville%20problem/","text":"Sturm-Liouville problem \u00b6 aka S.L.P definition \\[ \\begin{gather} (ry')' + (q+\\lambda p)y = 0 \\\\\\\\ p>0 \\text{ and } r>0 \\quad \\text{on } (a, b) \\end{gather} \\] eigenvalue \\(\\lambda\\) S.L.P has a nontrivial solution for \\(\\lambda\\) , then the \\(\\lambda\\) is an eigenvalue eigenfunction the solution for the corresponding S.L.P is called eigenfunction ## Regular SLP boundary condition \\[ \\begin{align} &A_1 y(a) + A_2 y'(a) = 0 \\\\\\\\ &B_1 y(b) + B_2 y'(b) = 0 \\end{align} \\] in which, \\[ \\begin{align} &A_1^2 +A_2^2 \\neq 0 \\\\\\\\ &B_1^2 + B_2^2 \\neq 0 \\end{align} \\] Periodic SLP \u00b6 define on \\([a, b]\\) \\(r(a) = r(b)\\) \\(y(a) = y(b)\\) , \\(y'(a) = y'(b)\\) Singular SLP \u00b6 \\(r(a) = 0\\) , or \\(r(b)=0\\) , (or both) SL Theorem \u00b6 if \\(\\phi\\) is an eigenfunction, then \\(c\\ \\phi\\) is also an eigenfunction \\(\\forall c \\in \u211d\\) \\(\\lambda_n \\neq \\lambda_m\\) let SLP define on \\((a, b)\\) , then \\[ \\begin{gather} (\\phi_n, \\phi_m)=\\int_a^b{p(x)\\ \\phi_n(x)\\ \\phi_m(x)\\ dx} = 0 \\end{gather} \\] in which \\(p\\) is weighted function Eigenfunction expansion \u00b6 With series solution, we can write any function in \\[ \\begin{align} f(x) = \\sum_{n =0}^{\\infty}{a_n x^n} \\end{align} \\] likewise, we can also write \\[ \\begin{align} f(x) = \\sum_{m=0}^{\\infty}{a_m y_m(x)} \\end{align} \\] in which \\(y\\) is an eigenfunction, multiply this equation by \\(p(x)y_{n}\\) \\[ \\begin{gather} p(x)\\ f(x)\\ y_{n} = \\sum_{m=0}^{\\infty}{a_{m}\\ y_{n}\\ y_{m}} \\end{gather} \\] and integrate to get \\[ \\begin{align} (f, y_m) &= \\sum_{n=1}^{\\infty}{a_n (y_n, y_m)} \\\\\\\\ &=a_m\\ (y_m, y_m) \\\\\\\\ a_m &= \\frac{(f, y_m)}{(y_m, y_m)}=\\frac{(f, y_m)}{||y_m||^2} \\end{align} \\]","title":"Sturm-Liouville problem"},{"location":"Engineering%20Math/Sturm-Liouville%20problem/#sturm-liouville+problem","text":"aka S.L.P definition \\[ \\begin{gather} (ry')' + (q+\\lambda p)y = 0 \\\\\\\\ p>0 \\text{ and } r>0 \\quad \\text{on } (a, b) \\end{gather} \\] eigenvalue \\(\\lambda\\) S.L.P has a nontrivial solution for \\(\\lambda\\) , then the \\(\\lambda\\) is an eigenvalue eigenfunction the solution for the corresponding S.L.P is called eigenfunction ## Regular SLP boundary condition \\[ \\begin{align} &A_1 y(a) + A_2 y'(a) = 0 \\\\\\\\ &B_1 y(b) + B_2 y'(b) = 0 \\end{align} \\] in which, \\[ \\begin{align} &A_1^2 +A_2^2 \\neq 0 \\\\\\\\ &B_1^2 + B_2^2 \\neq 0 \\end{align} \\]","title":"Sturm-Liouville problem"},{"location":"Engineering%20Math/Sturm-Liouville%20problem/#periodic+slp","text":"define on \\([a, b]\\) \\(r(a) = r(b)\\) \\(y(a) = y(b)\\) , \\(y'(a) = y'(b)\\)","title":"Periodic SLP"},{"location":"Engineering%20Math/Sturm-Liouville%20problem/#singular+slp","text":"\\(r(a) = 0\\) , or \\(r(b)=0\\) , (or both)","title":"Singular SLP"},{"location":"Engineering%20Math/Sturm-Liouville%20problem/#sl+theorem","text":"if \\(\\phi\\) is an eigenfunction, then \\(c\\ \\phi\\) is also an eigenfunction \\(\\forall c \\in \u211d\\) \\(\\lambda_n \\neq \\lambda_m\\) let SLP define on \\((a, b)\\) , then \\[ \\begin{gather} (\\phi_n, \\phi_m)=\\int_a^b{p(x)\\ \\phi_n(x)\\ \\phi_m(x)\\ dx} = 0 \\end{gather} \\] in which \\(p\\) is weighted function","title":"SL Theorem"},{"location":"Engineering%20Math/Sturm-Liouville%20problem/#eigenfunction+expansion","text":"With series solution, we can write any function in \\[ \\begin{align} f(x) = \\sum_{n =0}^{\\infty}{a_n x^n} \\end{align} \\] likewise, we can also write \\[ \\begin{align} f(x) = \\sum_{m=0}^{\\infty}{a_m y_m(x)} \\end{align} \\] in which \\(y\\) is an eigenfunction, multiply this equation by \\(p(x)y_{n}\\) \\[ \\begin{gather} p(x)\\ f(x)\\ y_{n} = \\sum_{m=0}^{\\infty}{a_{m}\\ y_{n}\\ y_{m}} \\end{gather} \\] and integrate to get \\[ \\begin{align} (f, y_m) &= \\sum_{n=1}^{\\infty}{a_n (y_n, y_m)} \\\\\\\\ &=a_m\\ (y_m, y_m) \\\\\\\\ a_m &= \\frac{(f, y_m)}{(y_m, y_m)}=\\frac{(f, y_m)}{||y_m||^2} \\end{align} \\]","title":"Eigenfunction expansion"},{"location":"Engineering%20Math/Wave%20Equations/","text":"Wave Equation \u00b6 1-D Wave Equation \u00b6 \\[ \\begin{gather} y(x, t) \\\\\\\\ \\frac{\\partial^2 y}{\\partial t^{2}}=c^{2}\\frac{\\partial^2 y}{\\partial x^{2}} \\end{gather} \\] Separate Variable Method \u00b6 \u524d\u63d0 wave equation is on a closed interval \\([0, L]\\) assume that \\[ \\begin{gather} y(x, t) = X(x)T(t) \\end{gather} \\] then \\[ \\begin{gather} X_{n}(x) = \\sin{(\\frac{n\\pi x}{p})} \\\\\\\\ T_{n}(x) = a_{n}\\cos{(\\frac{n\\pi c}{p}t)}+b_{n}\\sin{(\\frac{n\\pi c}{p}t)} \\end{gather} \\] finally we have \\[ \\begin{gather} y(x, t) = \\sum_{n=1}^{\\infty}{y_{n}(x,t)} = \\sum_{n =1}^{\\infty}{X_{n}(x)\\ T_n(t)} \\end{gather} \\] zero initial velocity \u00b6 \\[y_{t}(x, 0) = 0\\] let \\(f(x)\\) be an initial displacement function \\[ \\begin{gather} f(x) = y(x, 0) \\end{gather} \\] then we can get \\[ \\begin{gather} T_{n}(t) = a_{n} \\cos{(\\frac{n\\pi c }{p}t)} \\end{gather} \\] and finally, \\[ \\begin{gather} y(x, t) = \\sum_{n=1}^{\\infty}{a_{n}\\sin(\\frac{n\\pi x}{p})\\cos{\\frac{n\\pi c}{p}t}} \\end{gather} \\] notice that p is a half of period ( \\([-p, p]\\) ) in which \\(a_{n}\\) is the sine Fourier series coefficient of \\(f(x) = y(x, 0) = \\sum{a_{n}\\sin(\\frac{n\\pi x}{p})}\\) \\[ \\begin{gather} a_{n} = \\frac{2}{p}\\int_{0}^{p}{f(x)\\sin{(\\frac{n \\pi x}{p})}dx} \\end{gather} \\] zero initial displacement \u00b6 \\[y(x, 0) = 0\\] let \\(g(x)\\) be the initial velocity function \\[ \\begin{gather} g(x) = y_{t}(x, 0) \\end{gather} \\] then we can get \\[ \\begin{gather} T_{n}(t) = b_{n}\\sin{(\\frac{n\\pi c}{p}t)} \\end{gather} \\] in which \\(\\frac{n\\pi c}{p}b_{n}\\) is the Fourier sine series coefficient of \\(g(x)\\) \\[ \\begin{gather} \\frac{n \\pi c }{p}b_{n} = \\frac{2}{p}\\int_{0}^{p}{g(x)\\sin{(\\frac{n \\pi x}{p})}dx} \\end{gather} \\] finally, \\[ \\begin{align} y(x, t) &= \\sum_{n=1}^{\\infty}{y_{n}(x, t) } \\\\\\\\ &=\\sum_{n=1}^{\\infty}{b_{n}\\sin{(\\frac{n\\pi x}{p})}\\sin{(\\frac{n\\pi c t}{p})}} \\end{align} \\] nonzero initial velocity and displacement \u00b6 in the situation, assume \\[y(x, t) = y_{1}(x, t) + y_{2}(x, t)\\] which \\(y_{1}(x, t)\\) assume zero initial velocity, and that \\(y_{2}(x, t)\\) assume zero initial displacement and that is same as what we have discussed above. with External Force \u00b6 consider wave equation in this form \\[ \\begin{gather} \\frac{\\partial^2 y}{\\partial t^{2}} = \\frac{\\partial^2 y}{\\partial x^{2}}+Ax \\end{gather} \\] \\(y(x, t)\\) can not be separated. so we let \\[ \\begin{gather} y(x, t) = Y(x, t) + \\phi(x) \\end{gather} \\] assume that \\(Y(x, t)\\) can be separated, which means we can get the answer with the method above. \u4ee3\u5165 \\(y(x, t)\\) , we can get \\[ \\begin{gather} \\frac{\\partial^2 Y}{\\partial t^{2}} = \\frac{\\partial^2 Y}{\\partial t^{2}}+\\frac{\\partial^2 \\phi}{\\partial t^{2}}+Ax \\end{gather} \\] therefore \\[ \\begin{gather} \\phi''(x) + Ax =0 \\end{gather} \\] Infinite Medium \u00b6 for the wave equations which are define in \\((-\\infty, \\infty)\\) as usual, denote the initial displacement as \\(f(x)\\) and the initial velocity as \\(g(x)\\) first we have to know that the eigenvalue only exists when \\(\\omega \\geq 0\\) solve two ODE we can get \\[ \\begin{gather} X_{\\omega}(x) = a_{\\omega}\\cos{(\\omega x)}+ b_{\\omega }\\sin{(\\omega x)} \\\\\\\\ T_{\\omega}(t) = A_{\\omega}\\cos{(\\omega ct)} + B_{\\omega}\\sin{(\\omega ct)} \\end{gather} \\] then the most important thing is that \\[ \\begin{gather} y(x, t) = \\int_{0}^{\\infty}{y_{\\omega}(x, t) \\ d\\omega} = \\int_{0}^{\\infty}{X_{\\omega}(x)T_{\\omega}(t)\\ d\\omega } \\end{gather} \\] Semi-infinite Medium \u00b6 for the wave equations which are define in \\([0, \\infty)\\) , thus \\[ \\begin{gather} y(0, t) = 0 \\text{ for } t > 0 \\end{gather} \\] as usual, denote the initial displacement as \\(f(x)\\) and the initial velocity as \\(g(x)\\) \\[ \\begin{gather} X(x) = a\\cos{(\\omega x)}+ b\\sin{(\\omega x)} \\\\\\\\ T(t) = c_{\\omega}\\cos{(\\omega ct)} + d_{\\omega}\\sin{(\\omega ct)} \\end{gather} \\] %% since \\(X(0) = 0\\) , thus \\[ \\begin{gather} X(x) = b\\sin{(\\omega x)} \\end{gather} \\] %% Finite V.S. Infinite Medium \u00b6 Finite Infinite Fourier sine series Fourier integral \\(\\frac{n\\pi}{p}\\) \\(\\omega\\) \\(X(x)\\) \\(\\sin{(\\frac{n\\pi x}{p})}\\) \\(a\\cos{(\\omega x)}+b\\sin{(\\omega x)}\\) \\(T(t)\\) when \\(v(0)=0\\) \\(a_{n}\\cos{(\\frac{n\\pi c}{p}t)}\\) \\(A_{\\omega}\\cos{(\\omega ct)}\\) \\(T(t)\\) when \\(d(0)=0\\) \\(b_{n}\\sin{(\\frac{n\\pi c}{p}t)}\\) \\(B_{\\omega}\\sin{(\\omega c t)}\\) Characteristics and d'Alembert's Solution \u00b6 one special solution for these condition \\[ \\begin{gather} u_{tt} = c^{2}u_{xx} \\quad -\\infty < x < \\infty, t > 0 \\\\\\\\ u(x, 0) = f(x) \\quad -\\infty < x < \\infty \\\\\\\\ u_{t}(x, 0) = g(x) \\quad -\\infty < x < \\infty \\end{gather} \\] \u80cc \\[ \\begin{gather} u(x, t) = \\frac{1}{2}\\big[f(x-ct) + f(x+ct)\\big] + \\frac{1}{2c} \\int_{x-ct}^{x+ct} g(k) \\ dk \\end{gather} \\] Forced Wave Motion \u00b6 \\[ \\begin{gather} \\text{for }\\quad -\\infty < x < \\infty,\\quad t > 0 \\\\\\\\ u_{tt} = c^{2}u_{xx}+F(x, t) \\\\\\\\ u(x, 0) = f(x) \\\\\\\\ u_{t}(x, 0) = g(x) \\end{gather} \\] solution \\[ \\begin{gather} u(x, t) = \\frac{1}{2} \\big[f(x-ct)+f(x+ct)\\big]+\\frac{1}{2c}\\int_{x-ct}^{x+ct}{g(x)\\ dx} + \\frac{1}{2c}\\iint_{\\Delta}F(x, t)\\ dx\\ dt \\end{gather} \\] \\(\\Delta\\) is characteristics triangle \\[ \\begin{gather} x-ct = k_{1} \\\\\\\\ x+ct = k_{2} \\end{gather} \\] \\(k_{1}\\) , \\(k_{2}\\) can be any real constants. 2-D Wave Equation \u00b6 \\[ \\begin{gather} \\frac{\\partial z}{\\partial t^{2}}=c^{2} \\left(\\frac{\\partial^2 z}{\\partial x^{2}}+\\frac{\\partial^2 z}{\\partial y^{2}}\\right) \\end{gather} \\]","title":"Wave Equation"},{"location":"Engineering%20Math/Wave%20Equations/#wave+equation","text":"","title":"Wave Equation"},{"location":"Engineering%20Math/Wave%20Equations/#1-d+wave+equation","text":"\\[ \\begin{gather} y(x, t) \\\\\\\\ \\frac{\\partial^2 y}{\\partial t^{2}}=c^{2}\\frac{\\partial^2 y}{\\partial x^{2}} \\end{gather} \\]","title":"1-D Wave Equation"},{"location":"Engineering%20Math/Wave%20Equations/#separate+variable+method","text":"\u524d\u63d0 wave equation is on a closed interval \\([0, L]\\) assume that \\[ \\begin{gather} y(x, t) = X(x)T(t) \\end{gather} \\] then \\[ \\begin{gather} X_{n}(x) = \\sin{(\\frac{n\\pi x}{p})} \\\\\\\\ T_{n}(x) = a_{n}\\cos{(\\frac{n\\pi c}{p}t)}+b_{n}\\sin{(\\frac{n\\pi c}{p}t)} \\end{gather} \\] finally we have \\[ \\begin{gather} y(x, t) = \\sum_{n=1}^{\\infty}{y_{n}(x,t)} = \\sum_{n =1}^{\\infty}{X_{n}(x)\\ T_n(t)} \\end{gather} \\]","title":"Separate Variable Method"},{"location":"Engineering%20Math/Wave%20Equations/#zero+initial+velocity","text":"\\[y_{t}(x, 0) = 0\\] let \\(f(x)\\) be an initial displacement function \\[ \\begin{gather} f(x) = y(x, 0) \\end{gather} \\] then we can get \\[ \\begin{gather} T_{n}(t) = a_{n} \\cos{(\\frac{n\\pi c }{p}t)} \\end{gather} \\] and finally, \\[ \\begin{gather} y(x, t) = \\sum_{n=1}^{\\infty}{a_{n}\\sin(\\frac{n\\pi x}{p})\\cos{\\frac{n\\pi c}{p}t}} \\end{gather} \\] notice that p is a half of period ( \\([-p, p]\\) ) in which \\(a_{n}\\) is the sine Fourier series coefficient of \\(f(x) = y(x, 0) = \\sum{a_{n}\\sin(\\frac{n\\pi x}{p})}\\) \\[ \\begin{gather} a_{n} = \\frac{2}{p}\\int_{0}^{p}{f(x)\\sin{(\\frac{n \\pi x}{p})}dx} \\end{gather} \\]","title":"zero initial velocity"},{"location":"Engineering%20Math/Wave%20Equations/#zero+initial+displacement","text":"\\[y(x, 0) = 0\\] let \\(g(x)\\) be the initial velocity function \\[ \\begin{gather} g(x) = y_{t}(x, 0) \\end{gather} \\] then we can get \\[ \\begin{gather} T_{n}(t) = b_{n}\\sin{(\\frac{n\\pi c}{p}t)} \\end{gather} \\] in which \\(\\frac{n\\pi c}{p}b_{n}\\) is the Fourier sine series coefficient of \\(g(x)\\) \\[ \\begin{gather} \\frac{n \\pi c }{p}b_{n} = \\frac{2}{p}\\int_{0}^{p}{g(x)\\sin{(\\frac{n \\pi x}{p})}dx} \\end{gather} \\] finally, \\[ \\begin{align} y(x, t) &= \\sum_{n=1}^{\\infty}{y_{n}(x, t) } \\\\\\\\ &=\\sum_{n=1}^{\\infty}{b_{n}\\sin{(\\frac{n\\pi x}{p})}\\sin{(\\frac{n\\pi c t}{p})}} \\end{align} \\]","title":"zero initial displacement"},{"location":"Engineering%20Math/Wave%20Equations/#nonzero+initial+velocity+and+displacement","text":"in the situation, assume \\[y(x, t) = y_{1}(x, t) + y_{2}(x, t)\\] which \\(y_{1}(x, t)\\) assume zero initial velocity, and that \\(y_{2}(x, t)\\) assume zero initial displacement and that is same as what we have discussed above.","title":"nonzero initial velocity and displacement"},{"location":"Engineering%20Math/Wave%20Equations/#with+external+force","text":"consider wave equation in this form \\[ \\begin{gather} \\frac{\\partial^2 y}{\\partial t^{2}} = \\frac{\\partial^2 y}{\\partial x^{2}}+Ax \\end{gather} \\] \\(y(x, t)\\) can not be separated. so we let \\[ \\begin{gather} y(x, t) = Y(x, t) + \\phi(x) \\end{gather} \\] assume that \\(Y(x, t)\\) can be separated, which means we can get the answer with the method above. \u4ee3\u5165 \\(y(x, t)\\) , we can get \\[ \\begin{gather} \\frac{\\partial^2 Y}{\\partial t^{2}} = \\frac{\\partial^2 Y}{\\partial t^{2}}+\\frac{\\partial^2 \\phi}{\\partial t^{2}}+Ax \\end{gather} \\] therefore \\[ \\begin{gather} \\phi''(x) + Ax =0 \\end{gather} \\]","title":"with External Force"},{"location":"Engineering%20Math/Wave%20Equations/#infinite+medium","text":"for the wave equations which are define in \\((-\\infty, \\infty)\\) as usual, denote the initial displacement as \\(f(x)\\) and the initial velocity as \\(g(x)\\) first we have to know that the eigenvalue only exists when \\(\\omega \\geq 0\\) solve two ODE we can get \\[ \\begin{gather} X_{\\omega}(x) = a_{\\omega}\\cos{(\\omega x)}+ b_{\\omega }\\sin{(\\omega x)} \\\\\\\\ T_{\\omega}(t) = A_{\\omega}\\cos{(\\omega ct)} + B_{\\omega}\\sin{(\\omega ct)} \\end{gather} \\] then the most important thing is that \\[ \\begin{gather} y(x, t) = \\int_{0}^{\\infty}{y_{\\omega}(x, t) \\ d\\omega} = \\int_{0}^{\\infty}{X_{\\omega}(x)T_{\\omega}(t)\\ d\\omega } \\end{gather} \\]","title":"Infinite Medium"},{"location":"Engineering%20Math/Wave%20Equations/#semi-infinite+medium","text":"for the wave equations which are define in \\([0, \\infty)\\) , thus \\[ \\begin{gather} y(0, t) = 0 \\text{ for } t > 0 \\end{gather} \\] as usual, denote the initial displacement as \\(f(x)\\) and the initial velocity as \\(g(x)\\) \\[ \\begin{gather} X(x) = a\\cos{(\\omega x)}+ b\\sin{(\\omega x)} \\\\\\\\ T(t) = c_{\\omega}\\cos{(\\omega ct)} + d_{\\omega}\\sin{(\\omega ct)} \\end{gather} \\] %% since \\(X(0) = 0\\) , thus \\[ \\begin{gather} X(x) = b\\sin{(\\omega x)} \\end{gather} \\] %%","title":"Semi-infinite Medium"},{"location":"Engineering%20Math/Wave%20Equations/#finite+vs+infinite+medium","text":"Finite Infinite Fourier sine series Fourier integral \\(\\frac{n\\pi}{p}\\) \\(\\omega\\) \\(X(x)\\) \\(\\sin{(\\frac{n\\pi x}{p})}\\) \\(a\\cos{(\\omega x)}+b\\sin{(\\omega x)}\\) \\(T(t)\\) when \\(v(0)=0\\) \\(a_{n}\\cos{(\\frac{n\\pi c}{p}t)}\\) \\(A_{\\omega}\\cos{(\\omega ct)}\\) \\(T(t)\\) when \\(d(0)=0\\) \\(b_{n}\\sin{(\\frac{n\\pi c}{p}t)}\\) \\(B_{\\omega}\\sin{(\\omega c t)}\\)","title":"Finite V.S. Infinite Medium"},{"location":"Engineering%20Math/Wave%20Equations/#characteristics+and+dalemberts+solution","text":"one special solution for these condition \\[ \\begin{gather} u_{tt} = c^{2}u_{xx} \\quad -\\infty < x < \\infty, t > 0 \\\\\\\\ u(x, 0) = f(x) \\quad -\\infty < x < \\infty \\\\\\\\ u_{t}(x, 0) = g(x) \\quad -\\infty < x < \\infty \\end{gather} \\] \u80cc \\[ \\begin{gather} u(x, t) = \\frac{1}{2}\\big[f(x-ct) + f(x+ct)\\big] + \\frac{1}{2c} \\int_{x-ct}^{x+ct} g(k) \\ dk \\end{gather} \\]","title":"Characteristics and d'Alembert's Solution"},{"location":"Engineering%20Math/Wave%20Equations/#forced+wave+motion","text":"\\[ \\begin{gather} \\text{for }\\quad -\\infty < x < \\infty,\\quad t > 0 \\\\\\\\ u_{tt} = c^{2}u_{xx}+F(x, t) \\\\\\\\ u(x, 0) = f(x) \\\\\\\\ u_{t}(x, 0) = g(x) \\end{gather} \\] solution \\[ \\begin{gather} u(x, t) = \\frac{1}{2} \\big[f(x-ct)+f(x+ct)\\big]+\\frac{1}{2c}\\int_{x-ct}^{x+ct}{g(x)\\ dx} + \\frac{1}{2c}\\iint_{\\Delta}F(x, t)\\ dx\\ dt \\end{gather} \\] \\(\\Delta\\) is characteristics triangle \\[ \\begin{gather} x-ct = k_{1} \\\\\\\\ x+ct = k_{2} \\end{gather} \\] \\(k_{1}\\) , \\(k_{2}\\) can be any real constants.","title":"Forced Wave Motion"},{"location":"Engineering%20Math/Wave%20Equations/#2-d+wave+equation","text":"\\[ \\begin{gather} \\frac{\\partial z}{\\partial t^{2}}=c^{2} \\left(\\frac{\\partial^2 z}{\\partial x^{2}}+\\frac{\\partial^2 z}{\\partial y^{2}}\\right) \\end{gather} \\]","title":"2-D Wave Equation"},{"location":"Engineering%20Math/minority/Homogeneous%20Function/","text":"If \\[g(tx,ty) = t^{\\alpha}\\cdot g(x,y)\\] then \\(g(x,y)\\) is a homogeneous function of degree \\(\\alpha\\) given \\[M(tx,ty)=t^{\\alpha}\\cdot M(x, y)\\] \\[N(sx,sy)=s^{\\beta}\\cdot N(x, y)\\] satisfy \\( \\(\\alpha = \\beta\\) \\) >","title":"Homogeneous Function"},{"location":"Engineering%20Math/minority/IVP/","text":"IVP \u00b6 IVP (Initial Value Problem) e.g. \\[y'=f(x, y),\\] \\[y(x_0)=y_0\\] With exact one solution iff \\[\\lim_{(x, y)\u2192(x_0, y_0)} f(x, y)\\] and \\[\\lim_{(x, y)\u2192(x_0, y_0)} \\frac{\u2202}{\u2202y}f(x, y)\\] are continuous","title":"IVP"},{"location":"Engineering%20Math/minority/IVP/#ivp","text":"IVP (Initial Value Problem) e.g. \\[y'=f(x, y),\\] \\[y(x_0)=y_0\\] With exact one solution iff \\[\\lim_{(x, y)\u2192(x_0, y_0)} f(x, y)\\] and \\[\\lim_{(x, y)\u2192(x_0, y_0)} \\frac{\u2202}{\u2202y}f(x, y)\\] are continuous","title":"IVP"},{"location":"Engineering%20Math/minority/Proof%20to%20Modified%20EE/","text":"Proof to Modified Exact Equation Method \u00b6 given \\[Mdx+Ndy=0\\] try to use the factor \\(\\mu\\) to make the 1 st ODE the exact equation \\[ \\begin{gather} \\mu Mdx+\\mu Ndy=0\\\\ \\downarrow\\\\ \\frac{\\partial \\mu M}{\\partial y}=\\frac{\\partial \\mu N}{\\partial x}\\\\ \\downarrow\\\\ \\mu_y M + \\mu M_y = \\mu_x N + \\mu N_x\\\\ \\downarrow\\\\ \\mu = \\frac{\\mu_x N - \\mu_y M}{M_y - N_x} \\end{gather} \\] assume \\(\\mu\\) and \\((M_y-N_x)/M\\) is only dependent to \\(y\\) \\[\\therefore\\mu_x = 0\\] implies \\[\\mu = \\frac{M}{N_x-M_y}\\frac{d\\mu}{dy}\\] using separable varable method \\[\\mu(y) = e^{\\int {\\frac{(N_x-M_y)}{M}dy}}\\] simiar solution in the case that assume \\(\\mu\\) is only dependent to x.","title":"Proof to Modified EE"},{"location":"Engineering%20Math/minority/Proof%20to%20Modified%20EE/#proof+to+modified+exact+equation+method","text":"given \\[Mdx+Ndy=0\\] try to use the factor \\(\\mu\\) to make the 1 st ODE the exact equation \\[ \\begin{gather} \\mu Mdx+\\mu Ndy=0\\\\ \\downarrow\\\\ \\frac{\\partial \\mu M}{\\partial y}=\\frac{\\partial \\mu N}{\\partial x}\\\\ \\downarrow\\\\ \\mu_y M + \\mu M_y = \\mu_x N + \\mu N_x\\\\ \\downarrow\\\\ \\mu = \\frac{\\mu_x N - \\mu_y M}{M_y - N_x} \\end{gather} \\] assume \\(\\mu\\) and \\((M_y-N_x)/M\\) is only dependent to \\(y\\) \\[\\therefore\\mu_x = 0\\] implies \\[\\mu = \\frac{M}{N_x-M_y}\\frac{d\\mu}{dy}\\] using separable varable method \\[\\mu(y) = e^{\\int {\\frac{(N_x-M_y)}{M}dy}}\\] simiar solution in the case that assume \\(\\mu\\) is only dependent to x.","title":"Proof to Modified Exact Equation Method"},{"location":"Engineering%20Math/minority/Step%20Function/","text":"Step Function \u00b6 \\(u(t)\\) : unit step function The unit step function acts as a switch.","title":"Step Function"},{"location":"Engineering%20Math/minority/Step%20Function/#step+function","text":"\\(u(t)\\) : unit step function The unit step function acts as a switch.","title":"Step Function"},{"location":"Engineering%20Math/minority/Wronskian/","text":"Wronskian \u00b6 \\[ \\begin{gather} W(y_1, y_2, \\dots , y_n) = det \\begin{bmatrix} y_1 & y_2 & \\dots & y_n\\\\ y_1' & y_2' & \\dots & y_n'\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_1^{(n-1)} & y_2^{(n-1)} & \\dots & y_n^{(n-1)} \\end{bmatrix} \\end{gather} \\] \\[W \\neq 0 \\qquad \\rightarrow \\qquad \\text{linearly independent}\\]","title":"Wronskian"},{"location":"Engineering%20Math/minority/Wronskian/#wronskian","text":"\\[ \\begin{gather} W(y_1, y_2, \\dots , y_n) = det \\begin{bmatrix} y_1 & y_2 & \\dots & y_n\\\\ y_1' & y_2' & \\dots & y_n'\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ y_1^{(n-1)} & y_2^{(n-1)} & \\dots & y_n^{(n-1)} \\end{bmatrix} \\end{gather} \\] \\[W \\neq 0 \\qquad \\rightarrow \\qquad \\text{linearly independent}\\]","title":"Wronskian"},{"location":"Engineering%20Math/minority/proof2/","text":"\\[ \\begin{align} y &= C_1e^{\\alpha x + i\\beta x} + C_2 e^{\\alpha x - i\\beta x} \\\\ \\\\ &= e^{\\alpha x}(C_1 e^{i\\beta x} + C_2 e^{-i\\beta x}) \\\\\\\\ &= e^{\\alpha x}\\Big(C_1(\\cos{\\beta x} + i\\sin{\\beta x}) + C_2(\\cos{\\beta x} - i\\sin{\\beta x})\\Big) \\\\\\\\ &= e^{\\alpha x}(c_1\\cos{\\beta x} + c_2\\sin{\\beta x}) \\qquad \\qquad(c_1 = C_1 + C_2, \\quad c_2 = C_1 - C_2) \\end{align} \\] note : \\[ \\begin{gather} e^{ix} = \\cos x + i\\sin x\\\\\\\\\\ \\sin(x) = \\frac{e^{ix} - e^{-ix}}{2i} \\\\\\\\ \\cos(x) = \\frac{e^{ix} + e^{-ix}}{2} \\\\ \\end{gather} \\]","title":"Proof2"},{"location":"Linear%20Algebra/Linear_Algebra/","text":"Vector Spaces \u00b6 Subspace \u00b6 Definition: skip due to complication That's say a vector space \\(\\mathbf V = \\text{Span}\\set \\mathbf {v_1, v_2, v_3}\\) , then the subspace \\(\\mathbf H=\\text{Span}\\set\\mathbf{v_1, v_2},\\quad\\text{Span}\\set\\mathbf{v_2, v_3}, \\dots\\) Thus it's intuitive that subspace \\(\\mathbf H\\) have the property: \\(\\set\\mathbf 0 \\in \\mathbf H\\) Null Space \u00b6 Definition for homogeneous equation \\[ \\begin{gather} \\mathbf {Ax}= \\mathbf 0 \\\\\\\\ \\text{Nul } \\mathbf A = \\set {\\mathbf x | \\mathbf x \\in \\mathbf R^{n} \\,\\cap\\, \\mathbf {Ax=0}} \\end{gather} \\] example \\[ \\begin{gather} A= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\qquad u= \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} \\\\\\\\ \\mathbf {Au}= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{gather} \\] Thus \\(\\mathbf u\\) is in \\(\\text{Nul }\\mathbf A\\) . Properties \u00b6 \\(\\text{Nul }\\mathbf A\\) is namely the solution space of \\(\\mathbf x\\) for the matrix \\(\\mathbf A_{m\\times n}\\) , \\(\\text{Nul }\\mathbf A\\) is a subspace of \\(\u211d^{n}\\) since \\(\\mathbf {Ax=0}\\) , thus \\(\\mathbf x\\) can be written as \\[ \\begin{gather} \\mathbf x= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\end{gather} \\] so this property is intuitive. Column Space \u00b6 Definition: For an \\(m\\times n\\) matrix \\(\\mathbf A=[a_1, a_2, \\dots, a_n]\\) \\[ \\begin{gather} \\text{Col }\\mathbf A= \\text{Span}\\set{a_1, a_2, \\dots, a_n} \\\\\\\\ a_i \\in \u211d^{m} \\end{gather} \\] or equivalently, \\[ \\begin{gather} \\text{Col }\\mathbf A=\\set{\\mathbf {Ax}|\\mathbf x \\in \u211d^{n}} \\end{gather} \\] Properties \u00b6 \\(\\text{Col }\\mathbf A\\) is a subspace of \\(\u211d^{m}\\) range of linear transform \\(\\mathbf {Ax}\\) \\(\\text{rank }\\mathbf A = \\text{dim}(\\text{Col } \\mathbf A)\\) \\(\\text{rank }\\mathbf A+\\text{dim}(\\text{Nul }\\mathbf A)=n\\) this is quite intuitive since when we lost \\(1\\) dimension in \\(\\text{Col }\\mathbf A\\) , we got \\(1\\) dimension in solution space. e.g. \\(3\\) equations in \\(\u211d^{3}\\) can yield an unique solution. However, \\(2\\) equations can only yield a line (i.e. one dimension) at most. Characteristic Equation \u00b6 Eigenvector & Eigenvalue \u00b6 Definition \\[ \\begin{gather} \\mathbf {Ax}= \\lambda \\mathbf x \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {(A-\\lambda I)\\,x}= 0 \\end{gather} \\] \\(n\\times n\\) matrix \\(\\mathbf A\\) , with the eigenvector \\(\\mathbf x\\) and eigenvalue \\(\\lambda\\) . note: \\(\\mathbf 0\\) cannot be an eigenvector (since it is trivial and by definition) 0 can be an eigenvalue. Orthogonality \u00b6 Orthogonal Complements \u00b6 A vector \\(\\mathbf x\\) is in \\(\\mathbf {W^{\\bot}}\\) iff \\(\\mathbf x\\) is orthogonal to every vector in \\(\\mathbf W\\) \\((\\text{Row }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf A\\) \\((\\text{Col }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf {A^{T}}\\) Orthogonal Projection \u00b6 Given a vector \\(\\mathbf y\\) and a subspace \\(\\mathbf W\\) in \\(\u211d^{n}\\) \\[ \\begin{align} \\mathbf y &= c_1 \\mathbf {u_1} + c_2 \\mathbf {u_2} + \\dots + c_n \\mathbf {u_n} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_n} \\\\ \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{align} \\] Let's say \\(\\mathbf W = \\text{Span }\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) ( \\(p \\le n\\) ) then the orthogonal projection of \\(\\mathbf y\\) onto \\(\\mathbf W\\) , \\(\\hat {\\mathbf y}\\) (aka \\(\\text{proj}_{\\mathbf W}\\mathbf y\\) ) \\[ \\begin{gather} \\hat {\\mathbf y} = \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\end{gather} \\] assume \\(\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) is an orthonormal basic. we can thus further write \\[ \\begin{align} \\hat {\\mathbf y} &= \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\\\\\\\ &= (\\mathbf {y \\cdot u_1})\\mathbf {u_1} + (\\mathbf {y \\cdot u_2})\\mathbf {u_2} + (\\mathbf {y \\cdot u_p})\\mathbf {u_p} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {y \\cdot u_1} \\\\ \\mathbf {y \\cdot u_2} \\\\ \\vdots \\\\ \\mathbf {y \\cdot u_p} \\\\ \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {u_1^{T}} \\\\ \\mathbf {u_2^{T}} \\\\ \\vdots \\\\ \\mathbf {u_p^{T}} \\\\ \\end{bmatrix} \\mathbf y \\\\\\\\ &= \\mathbf {UU^{T}y} \\end{align} \\] Thus, we can conclude that for an \\(n \\times p\\) matrix \\(\\mathbf U\\) which consists of \\(\\mathbf W\\) which is \\(p\\) -dimension subspace of \\(\u211d^{n}\\) , there is \\(\\mathbf {UU^{T}}\\) that project \\(\\mathbf y \\in \u211d^{n}\\) onto \\(\\mathbf W\\) . \\[ \\begin{gather} \\mathbf {UU^{T}y}=\\text{proj}_{\\text{W}}\\,\\mathbf y \\end{gather} \\] Diagonalization \u00b6 Diagonal Matrix \u00b6 \\(\\mathbf D\\) is said to be a diagonal matrix if \\(\\mathbf D\\) has the form \\[ \\begin{gather} \\mathbf D= \\begin{bmatrix} a & 0 & 0 & \\cdots \\\\ 0 & b & 0 & \\cdots \\\\ 0 & 0 & c & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\] thus \\(\\mathbf D\\) has the property \\[ \\begin{gather} \\mathbf D^{n}= \\begin{bmatrix} a^{n} & 0 & 0 & \\cdots \\\\ 0 & b^{n} & 0 & \\cdots \\\\ 0 & 0 & c^{n} & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\] Diagonalization \u00b6 Square matrix \\(\\mathbf A\\) is said to be diagonalizable if \\(\\mathbf {A=PDP^{-1}}\\) in which \\(\\mathbf {D}\\) is diagonal matrix \\(\\mathbf P\\) is invertible matrix. The Diagonalization Theorem \u00b6 An \\(n \\times n\\) matrix \\(\\mathbf A\\) is diagonalizable iff \\(\\mathbf A\\) has \\(n\\) eigenvectors and eigenvalues(may be multiple roots). Easy Proof Let's say an invertiable matrix \\(P_{n\\times n}\\) and diagnol matrix \\(D\\) \\[ \\begin{gather} \\mathbf P = [\\mathbf {v_1, v_2, \\dots , v_n}] \\qquad D = \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} \\\\\\\\ \\mathbf {AP}=\\mathbf{[Av_1, Av_2, \\dots]} \\\\\\\\ \\mathbf {PD}=[\\mathbf {v_1, v_2, \\dots , v_n}] \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} =[\\mathbf {\\lambda_1v_1,\\,\\lambda_2v_2,\\,\\dots}] \\end{gather} \\] by the definition of characteristic equation \\[ \\begin{gather} \\mathbf {AP=PD} \\\\\\\\ \\implies \\mathbf {A=PDP^{-1}} \\end{gather} \\] Orthogonally Diagonalizable \u00b6 Definition A matrix is said to be orthogonally diagonalizable if \\[ \\begin{gather} \\mathbf {A=PDP^{T}=PDP^{-1}} \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {A^{T}=P^{TT}D^{T}P^{T}=PDP^{T}=A} \\end{gather} \\] And iff \\(\\mathbf A\\) is a symmetric matrix. SVD \u00b6 S ingular V alue D iagonalization Singular Values \u00b6 Let \\(\\mathbf A\\) be an \\(m\\times n\\) matrix. Then \\[ \\begin{gather} ({\\mathbf {A^{T}A}})^{\\mathbf T}=\\mathbf {A^{T}A^{TT}}={\\mathbf {A^{T}A}} \\end{gather} \\] Thus, \\(\\mathbf {A^{T}A}\\) is symmetric and orthogonally diagonalizable. Then, Let say \\(\\{\\mathbf {v_1, v_2, \\dots, v_n}\\}\\) be an orthonormal basic for \\(\\mathbf \u211d^{n}\\) consisting of eigenvectors of \\(\\mathbf {A^T A}\\) And also, say \\(\\lambda_1, \\lambda_2, \\dots\\) be the eigenvalues of \\(\\mathbf {A^{T}A}\\) (make \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\) ) The singular values \\(\\sigma_i\\) of \\(\\mathbf A\\) are \\[ \\begin{gather} \\sigma_i = \\sqrt{\\lambda_i} \\end{gather} \\] Thinking collapse: none Singular values are the \"eigenvalues\" of non-square matrix \\(\\mathbf A\\) . Properties \u00b6 \\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_n}\\}\\) is an orthogonal set. for any \\(i\\neq j\\) , \\(\\mathbf {(Av_i)^{T}(Av_j)=v_i A^{T}Av_j=v_i\\lambda_j v_j}=0\\) \\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_r}\\}\\) is an orthogonal basic of \\(\\text{Col }\\mathbf A\\) . \\(\\iff \\text{rank }\\mathbf A = \\text{dim }\\text{Col }\\mathbf A=r\\) say \\(\\mathbf {y = Ax}\\) in \\(\\text{Col }\\mathbf A\\) , then \\[ \\begin{gather} \\begin{aligned} \\mathbf x &= c_1 \\mathbf v_1 + c_2 \\mathbf v_2 + \\dots + c_r \\mathbf v_r + \\dots + c_n \\mathbf v_n \\\\\\\\ &= c_1' \\mathbf v_1 + c_2' \\mathbf v_2 + \\dots + c_r' \\mathbf v_r \\end{aligned} \\\\\\\\ \\Downarrow \\\\\\\\ \\begin{aligned} \\mathbf y &=\\mathbf {Ax} \\\\\\\\ &= c_1' \\mathbf {Av_1} + c_2' \\mathbf {Av_2} + \\dots + c_r' \\mathbf {Av_r} \\end{aligned} \\end{gather} \\] Singular Value Decomposition \u00b6 \\(m\\times n\\) \"diagonal\" matrix \\[ \\begin{gather} \\mathbf \\Sigma_{m\\times n}= \\begin{bmatrix} \\mathbf D_{r\\times r} & \\mathbf 0_{r\\times (n-r)} \\\\ \\mathbf 0_{(m-r)\\times r} & \\mathbf 0_{(m-r)\\times (n-r)} \\\\ \\end{bmatrix} \\end{gather} \\] e.g. \\(\\mathbf A_{2\\times 3}\\) with \\(r=2\\) \\[ \\begin{gather} \\mathbf \\Sigma= \\begin{bmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ \\end{bmatrix} \\end{gather} \\] singular value decomposition \\[ \\begin{gather} \\mathbf A_{m\\times n}= \\mathbf U_{m\\times m} \\mathbf \\Sigma_{m\\times n} {\\mathbf V_{n\\times n}}^{\\mathbf T} \\end{gather} \\] Left singular vectors of \\(A\\,\\,\\) : The columns of \\(\\mathbf U\\) Right singular vectors of \\(A\\) : The columns of \\(\\mathbf V\\) Easy Proof First following the property 2. of singular value, we can obtain an orthonormal basic \\(\\{\\mathbf {u_1, u_2, \\dots, u_r}\\}\\) by \\[ \\begin{gather} \\mathbf {u_i} = \\mathbf {\\frac{Av_i}{||Av_i||}} = \\frac{\\mathbf {Av_i}}{\\sigma_i} \\end{gather} \\] then \\[ \\begin{gather} \\begin{aligned} \\mathbf {U\\Sigma} &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_m} \\end{bmatrix} \\begin{bmatrix} \\sigma_1 & 0 & \\cdots \\\\ 0 & \\sigma_2 & \\cdots \\\\ & & \\ddots \\end{bmatrix}_{m\\times n} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\\\\\\\\\\\ \\mathbf {AV} &= \\begin{bmatrix} \\mathbf {Av_1} & \\mathbf {Av_2} & \\cdots & \\mathbf {Av_n} \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\end{aligned} \\\\\\\\\\\\ \\implies \\mathbf {U\\Sigma=AV} \\implies \\mathbf {U\\Sigma V^{T}=A} \\end{gather} \\] Least-Squares Solution \u00b6 Target for \\(m \\times n\\) matrix \\(\\mathbf A\\) \\[ \\begin{gather} \\mathbf A= \\begin{bmatrix} \\mathbf {a_1} & \\mathbf {a_2} & \\cdots &\\mathbf {a_n} \\end{bmatrix} \\\\\\\\ \\mathbf {Ax=b} \\end{gather} \\] say \\(\\mathbf b\\) is a linear combination of columns of \\(\\mathbf A\\) for solution \\(\\mathbf x\\) . Most of times we cannot find the perfect solution. Thus, we try to find the Least-Squares solution \\(\\mathbf {\\hat x}\\) . Find \\[ \\begin{gather} \\mathbf {\\hat x} = \\mathop{\\arg\\min}_{\\mathbf {\\hat x}}{\\,||\\mathbf {b-A\\hat x}||}^{2} \\end{gather} \\] First, let define \\[ \\begin{gather} \\mathbf {p = A\\hat{x} \\in \\text{Col }A} \\end{gather} \\] We know that the least-square root happens at \\((\\mathbf {b-p})\\) orthogonal to \\(\\mathbf p\\) . Thus say the projection matrix \\(\\mathbf P\\) s.t. \\[\\mathbf {P b=p}\\] Back to the condition \\(\\mathbf {(b-p)\\cdot p}=0\\) . That is \\[ \\begin{align} \\mathbf p &\\in \\text{Col }\\mathbf {A} \\\\\\\\ \\mathbf {(b-p)} &\\in (\\text{Col }\\mathbf {A})^{\\bot} = \\text{Nul }\\mathbf {A^{T}} \\tag{a} \\end{align} \\] from the definition of null space and equation \\(a\\) , we have \\[ \\begin{gather} \\mathbf {A^{T}(b-p)=0} \\\\\\\\ \\implies \\mathbf {A^{T}(b-A\\hat x)=0} \\\\\\\\ \\implies \\mathbf {A^{T}A\\hat x=A^{T}\\,b} \\\\\\\\ \\implies \\mathbf {\\hat x=(A^{T}A)^{-1}A^{T}\\,b} \\end{gather} \\] Besides, we now further have \\[ \\begin{gather} \\mathbf {p=A\\hat x=A(A^{T}A)^{-1}A^{T}\\,b} \\tag{b} \\\\\\\\ \\mathbf {P = A(A^{T}A)^{-1}A^{T}} \\end{gather} \\] Pseudoinverse Aspect \u00b6 \\[ \\begin{gather} \\mathbf {A\\hat x=b} \\\\\\\\ \\implies \\mathbf {\\hat x=A^{\\dagger}\\,b} \\\\\\\\ \\implies \\mathbf {p=A\\hat x=AA^{\\dagger}\\,b} \\tag{c} \\end{gather} \\] compare equation \\(b\\) and \\(c\\) \\[ \\begin{align} \\mathbf {p}&=\\mathbf {A\\hat x=AA^{\\dagger}\\,b} \\\\\\\\ &= (\\mathbf {U_r D V_r^{T}})(\\mathbf {V_r D^{-1} U_r^{T}})\\,\\mathbf b \\\\\\\\ &= \\mathbf {U_r U_r^{T}}\\,\\mathbf {b} \\end{align} \\] by theorem of Orthogonal Projection , we know that \\[ \\begin{gather} \\mathbf {P} = \\mathbf {AA^{\\dagger}} = \\mathbf {U_r U_r^{T}} \\end{gather} \\]","title":"Linear Algebra"},{"location":"Linear%20Algebra/Linear_Algebra/#vector+spaces","text":"","title":"Vector Spaces"},{"location":"Linear%20Algebra/Linear_Algebra/#subspace","text":"Definition: skip due to complication That's say a vector space \\(\\mathbf V = \\text{Span}\\set \\mathbf {v_1, v_2, v_3}\\) , then the subspace \\(\\mathbf H=\\text{Span}\\set\\mathbf{v_1, v_2},\\quad\\text{Span}\\set\\mathbf{v_2, v_3}, \\dots\\) Thus it's intuitive that subspace \\(\\mathbf H\\) have the property: \\(\\set\\mathbf 0 \\in \\mathbf H\\)","title":"Subspace"},{"location":"Linear%20Algebra/Linear_Algebra/#null+space","text":"Definition for homogeneous equation \\[ \\begin{gather} \\mathbf {Ax}= \\mathbf 0 \\\\\\\\ \\text{Nul } \\mathbf A = \\set {\\mathbf x | \\mathbf x \\in \\mathbf R^{n} \\,\\cap\\, \\mathbf {Ax=0}} \\end{gather} \\] example \\[ \\begin{gather} A= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\qquad u= \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} \\\\\\\\ \\mathbf {Au}= \\begin{bmatrix} 1 & -3 & -2 \\\\ -5 & 9 & -1 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 3 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\end{gather} \\] Thus \\(\\mathbf u\\) is in \\(\\text{Nul }\\mathbf A\\) .","title":"Null Space"},{"location":"Linear%20Algebra/Linear_Algebra/#properties","text":"\\(\\text{Nul }\\mathbf A\\) is namely the solution space of \\(\\mathbf x\\) for the matrix \\(\\mathbf A_{m\\times n}\\) , \\(\\text{Nul }\\mathbf A\\) is a subspace of \\(\u211d^{n}\\) since \\(\\mathbf {Ax=0}\\) , thus \\(\\mathbf x\\) can be written as \\[ \\begin{gather} \\mathbf x= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\end{gather} \\] so this property is intuitive.","title":"Properties"},{"location":"Linear%20Algebra/Linear_Algebra/#column+space","text":"Definition: For an \\(m\\times n\\) matrix \\(\\mathbf A=[a_1, a_2, \\dots, a_n]\\) \\[ \\begin{gather} \\text{Col }\\mathbf A= \\text{Span}\\set{a_1, a_2, \\dots, a_n} \\\\\\\\ a_i \\in \u211d^{m} \\end{gather} \\] or equivalently, \\[ \\begin{gather} \\text{Col }\\mathbf A=\\set{\\mathbf {Ax}|\\mathbf x \\in \u211d^{n}} \\end{gather} \\]","title":"Column Space"},{"location":"Linear%20Algebra/Linear_Algebra/#properties_1","text":"\\(\\text{Col }\\mathbf A\\) is a subspace of \\(\u211d^{m}\\) range of linear transform \\(\\mathbf {Ax}\\) \\(\\text{rank }\\mathbf A = \\text{dim}(\\text{Col } \\mathbf A)\\) \\(\\text{rank }\\mathbf A+\\text{dim}(\\text{Nul }\\mathbf A)=n\\) this is quite intuitive since when we lost \\(1\\) dimension in \\(\\text{Col }\\mathbf A\\) , we got \\(1\\) dimension in solution space. e.g. \\(3\\) equations in \\(\u211d^{3}\\) can yield an unique solution. However, \\(2\\) equations can only yield a line (i.e. one dimension) at most.","title":"Properties"},{"location":"Linear%20Algebra/Linear_Algebra/#characteristic+equation","text":"","title":"Characteristic Equation"},{"location":"Linear%20Algebra/Linear_Algebra/#eigenvector++eigenvalue","text":"Definition \\[ \\begin{gather} \\mathbf {Ax}= \\lambda \\mathbf x \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {(A-\\lambda I)\\,x}= 0 \\end{gather} \\] \\(n\\times n\\) matrix \\(\\mathbf A\\) , with the eigenvector \\(\\mathbf x\\) and eigenvalue \\(\\lambda\\) . note: \\(\\mathbf 0\\) cannot be an eigenvector (since it is trivial and by definition) 0 can be an eigenvalue.","title":"Eigenvector &amp; Eigenvalue"},{"location":"Linear%20Algebra/Linear_Algebra/#orthogonality","text":"","title":"Orthogonality"},{"location":"Linear%20Algebra/Linear_Algebra/#orthogonal+complements","text":"A vector \\(\\mathbf x\\) is in \\(\\mathbf {W^{\\bot}}\\) iff \\(\\mathbf x\\) is orthogonal to every vector in \\(\\mathbf W\\) \\((\\text{Row }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf A\\) \\((\\text{Col }\\mathbf {A})^{\\bot}=\\text{Nul }\\mathbf {A^{T}}\\)","title":"Orthogonal Complements"},{"location":"Linear%20Algebra/Linear_Algebra/#orthogonal+projection","text":"Given a vector \\(\\mathbf y\\) and a subspace \\(\\mathbf W\\) in \\(\u211d^{n}\\) \\[ \\begin{align} \\mathbf y &= c_1 \\mathbf {u_1} + c_2 \\mathbf {u_2} + \\dots + c_n \\mathbf {u_n} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_n} \\\\ \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \\end{align} \\] Let's say \\(\\mathbf W = \\text{Span }\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) ( \\(p \\le n\\) ) then the orthogonal projection of \\(\\mathbf y\\) onto \\(\\mathbf W\\) , \\(\\hat {\\mathbf y}\\) (aka \\(\\text{proj}_{\\mathbf W}\\mathbf y\\) ) \\[ \\begin{gather} \\hat {\\mathbf y} = \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\end{gather} \\] assume \\(\\{\\mathbf u_1, \\mathbf u_2, \\dots, \\mathbf u_p\\}\\) is an orthonormal basic. we can thus further write \\[ \\begin{align} \\hat {\\mathbf y} &= \\frac{\\mathbf {y \\cdot u_1}}{||\\mathbf {u_1}||^{2}}\\mathbf u_1 + \\frac{\\mathbf {y \\cdot u_2}}{||\\mathbf {u_2}||^{2}}\\mathbf u_2 + \\dots+ \\frac{\\mathbf {y \\cdot u_p}}{||\\mathbf {u_p}||^{2}}\\mathbf {u_p} \\\\\\\\ &= (\\mathbf {y \\cdot u_1})\\mathbf {u_1} + (\\mathbf {y \\cdot u_2})\\mathbf {u_2} + (\\mathbf {y \\cdot u_p})\\mathbf {u_p} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {y \\cdot u_1} \\\\ \\mathbf {y \\cdot u_2} \\\\ \\vdots \\\\ \\mathbf {y \\cdot u_p} \\\\ \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_p} \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf {u_1^{T}} \\\\ \\mathbf {u_2^{T}} \\\\ \\vdots \\\\ \\mathbf {u_p^{T}} \\\\ \\end{bmatrix} \\mathbf y \\\\\\\\ &= \\mathbf {UU^{T}y} \\end{align} \\] Thus, we can conclude that for an \\(n \\times p\\) matrix \\(\\mathbf U\\) which consists of \\(\\mathbf W\\) which is \\(p\\) -dimension subspace of \\(\u211d^{n}\\) , there is \\(\\mathbf {UU^{T}}\\) that project \\(\\mathbf y \\in \u211d^{n}\\) onto \\(\\mathbf W\\) . \\[ \\begin{gather} \\mathbf {UU^{T}y}=\\text{proj}_{\\text{W}}\\,\\mathbf y \\end{gather} \\]","title":"Orthogonal Projection"},{"location":"Linear%20Algebra/Linear_Algebra/#diagonalization","text":"","title":"Diagonalization"},{"location":"Linear%20Algebra/Linear_Algebra/#diagonal+matrix","text":"\\(\\mathbf D\\) is said to be a diagonal matrix if \\(\\mathbf D\\) has the form \\[ \\begin{gather} \\mathbf D= \\begin{bmatrix} a & 0 & 0 & \\cdots \\\\ 0 & b & 0 & \\cdots \\\\ 0 & 0 & c & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\] thus \\(\\mathbf D\\) has the property \\[ \\begin{gather} \\mathbf D^{n}= \\begin{bmatrix} a^{n} & 0 & 0 & \\cdots \\\\ 0 & b^{n} & 0 & \\cdots \\\\ 0 & 0 & c^{n} & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} \\end{gather} \\]","title":"Diagonal Matrix"},{"location":"Linear%20Algebra/Linear_Algebra/#diagonalization_1","text":"Square matrix \\(\\mathbf A\\) is said to be diagonalizable if \\(\\mathbf {A=PDP^{-1}}\\) in which \\(\\mathbf {D}\\) is diagonal matrix \\(\\mathbf P\\) is invertible matrix.","title":"Diagonalization"},{"location":"Linear%20Algebra/Linear_Algebra/#the+diagonalization+theorem","text":"An \\(n \\times n\\) matrix \\(\\mathbf A\\) is diagonalizable iff \\(\\mathbf A\\) has \\(n\\) eigenvectors and eigenvalues(may be multiple roots). Easy Proof Let's say an invertiable matrix \\(P_{n\\times n}\\) and diagnol matrix \\(D\\) \\[ \\begin{gather} \\mathbf P = [\\mathbf {v_1, v_2, \\dots , v_n}] \\qquad D = \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} \\\\\\\\ \\mathbf {AP}=\\mathbf{[Av_1, Av_2, \\dots]} \\\\\\\\ \\mathbf {PD}=[\\mathbf {v_1, v_2, \\dots , v_n}] \\begin{bmatrix} \\lambda_1 & 0 & \\\\ 0 & \\lambda_2 & \\\\ & & \\ddots \\\\ & & &\\lambda_n \\end{bmatrix} =[\\mathbf {\\lambda_1v_1,\\,\\lambda_2v_2,\\,\\dots}] \\end{gather} \\] by the definition of characteristic equation \\[ \\begin{gather} \\mathbf {AP=PD} \\\\\\\\ \\implies \\mathbf {A=PDP^{-1}} \\end{gather} \\]","title":"The Diagonalization Theorem"},{"location":"Linear%20Algebra/Linear_Algebra/#orthogonally+diagonalizable","text":"Definition A matrix is said to be orthogonally diagonalizable if \\[ \\begin{gather} \\mathbf {A=PDP^{T}=PDP^{-1}} \\\\\\\\ \\Updownarrow \\\\\\\\ \\mathbf {A^{T}=P^{TT}D^{T}P^{T}=PDP^{T}=A} \\end{gather} \\] And iff \\(\\mathbf A\\) is a symmetric matrix.","title":"Orthogonally Diagonalizable"},{"location":"Linear%20Algebra/Linear_Algebra/#svd","text":"S ingular V alue D iagonalization","title":"SVD"},{"location":"Linear%20Algebra/Linear_Algebra/#singular+values","text":"Let \\(\\mathbf A\\) be an \\(m\\times n\\) matrix. Then \\[ \\begin{gather} ({\\mathbf {A^{T}A}})^{\\mathbf T}=\\mathbf {A^{T}A^{TT}}={\\mathbf {A^{T}A}} \\end{gather} \\] Thus, \\(\\mathbf {A^{T}A}\\) is symmetric and orthogonally diagonalizable. Then, Let say \\(\\{\\mathbf {v_1, v_2, \\dots, v_n}\\}\\) be an orthonormal basic for \\(\\mathbf \u211d^{n}\\) consisting of eigenvectors of \\(\\mathbf {A^T A}\\) And also, say \\(\\lambda_1, \\lambda_2, \\dots\\) be the eigenvalues of \\(\\mathbf {A^{T}A}\\) (make \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\) ) The singular values \\(\\sigma_i\\) of \\(\\mathbf A\\) are \\[ \\begin{gather} \\sigma_i = \\sqrt{\\lambda_i} \\end{gather} \\] Thinking collapse: none Singular values are the \"eigenvalues\" of non-square matrix \\(\\mathbf A\\) .","title":"Singular Values"},{"location":"Linear%20Algebra/Linear_Algebra/#properties_2","text":"\\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_n}\\}\\) is an orthogonal set. for any \\(i\\neq j\\) , \\(\\mathbf {(Av_i)^{T}(Av_j)=v_i A^{T}Av_j=v_i\\lambda_j v_j}=0\\) \\(\\{\\mathbf {Av_1, Av_2, \\dots, Av_r}\\}\\) is an orthogonal basic of \\(\\text{Col }\\mathbf A\\) . \\(\\iff \\text{rank }\\mathbf A = \\text{dim }\\text{Col }\\mathbf A=r\\) say \\(\\mathbf {y = Ax}\\) in \\(\\text{Col }\\mathbf A\\) , then \\[ \\begin{gather} \\begin{aligned} \\mathbf x &= c_1 \\mathbf v_1 + c_2 \\mathbf v_2 + \\dots + c_r \\mathbf v_r + \\dots + c_n \\mathbf v_n \\\\\\\\ &= c_1' \\mathbf v_1 + c_2' \\mathbf v_2 + \\dots + c_r' \\mathbf v_r \\end{aligned} \\\\\\\\ \\Downarrow \\\\\\\\ \\begin{aligned} \\mathbf y &=\\mathbf {Ax} \\\\\\\\ &= c_1' \\mathbf {Av_1} + c_2' \\mathbf {Av_2} + \\dots + c_r' \\mathbf {Av_r} \\end{aligned} \\end{gather} \\]","title":"Properties"},{"location":"Linear%20Algebra/Linear_Algebra/#singular+value+decomposition","text":"\\(m\\times n\\) \"diagonal\" matrix \\[ \\begin{gather} \\mathbf \\Sigma_{m\\times n}= \\begin{bmatrix} \\mathbf D_{r\\times r} & \\mathbf 0_{r\\times (n-r)} \\\\ \\mathbf 0_{(m-r)\\times r} & \\mathbf 0_{(m-r)\\times (n-r)} \\\\ \\end{bmatrix} \\end{gather} \\] e.g. \\(\\mathbf A_{2\\times 3}\\) with \\(r=2\\) \\[ \\begin{gather} \\mathbf \\Sigma= \\begin{bmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\\\ \\end{bmatrix} \\end{gather} \\] singular value decomposition \\[ \\begin{gather} \\mathbf A_{m\\times n}= \\mathbf U_{m\\times m} \\mathbf \\Sigma_{m\\times n} {\\mathbf V_{n\\times n}}^{\\mathbf T} \\end{gather} \\] Left singular vectors of \\(A\\,\\,\\) : The columns of \\(\\mathbf U\\) Right singular vectors of \\(A\\) : The columns of \\(\\mathbf V\\) Easy Proof First following the property 2. of singular value, we can obtain an orthonormal basic \\(\\{\\mathbf {u_1, u_2, \\dots, u_r}\\}\\) by \\[ \\begin{gather} \\mathbf {u_i} = \\mathbf {\\frac{Av_i}{||Av_i||}} = \\frac{\\mathbf {Av_i}}{\\sigma_i} \\end{gather} \\] then \\[ \\begin{gather} \\begin{aligned} \\mathbf {U\\Sigma} &= \\begin{bmatrix} \\mathbf {u_1} & \\mathbf {u_2} & \\cdots & \\mathbf {u_m} \\end{bmatrix} \\begin{bmatrix} \\sigma_1 & 0 & \\cdots \\\\ 0 & \\sigma_2 & \\cdots \\\\ & & \\ddots \\end{bmatrix}_{m\\times n} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\\\\\\\\\\\ \\mathbf {AV} &= \\begin{bmatrix} \\mathbf {Av_1} & \\mathbf {Av_2} & \\cdots & \\mathbf {Av_n} \\end{bmatrix} \\\\\\\\ &= \\begin{bmatrix} \\sigma_1 \\mathbf u_1 & \\sigma_2 \\mathbf u_2 & \\cdots & \\sigma_r \\mathbf u_r & \\mathbf 0 & \\cdots & \\mathbf 0 \\end{bmatrix} \\end{aligned} \\\\\\\\\\\\ \\implies \\mathbf {U\\Sigma=AV} \\implies \\mathbf {U\\Sigma V^{T}=A} \\end{gather} \\]","title":"Singular Value Decomposition"},{"location":"Linear%20Algebra/Linear_Algebra/#least-squares+solution","text":"Target for \\(m \\times n\\) matrix \\(\\mathbf A\\) \\[ \\begin{gather} \\mathbf A= \\begin{bmatrix} \\mathbf {a_1} & \\mathbf {a_2} & \\cdots &\\mathbf {a_n} \\end{bmatrix} \\\\\\\\ \\mathbf {Ax=b} \\end{gather} \\] say \\(\\mathbf b\\) is a linear combination of columns of \\(\\mathbf A\\) for solution \\(\\mathbf x\\) . Most of times we cannot find the perfect solution. Thus, we try to find the Least-Squares solution \\(\\mathbf {\\hat x}\\) . Find \\[ \\begin{gather} \\mathbf {\\hat x} = \\mathop{\\arg\\min}_{\\mathbf {\\hat x}}{\\,||\\mathbf {b-A\\hat x}||}^{2} \\end{gather} \\] First, let define \\[ \\begin{gather} \\mathbf {p = A\\hat{x} \\in \\text{Col }A} \\end{gather} \\] We know that the least-square root happens at \\((\\mathbf {b-p})\\) orthogonal to \\(\\mathbf p\\) . Thus say the projection matrix \\(\\mathbf P\\) s.t. \\[\\mathbf {P b=p}\\] Back to the condition \\(\\mathbf {(b-p)\\cdot p}=0\\) . That is \\[ \\begin{align} \\mathbf p &\\in \\text{Col }\\mathbf {A} \\\\\\\\ \\mathbf {(b-p)} &\\in (\\text{Col }\\mathbf {A})^{\\bot} = \\text{Nul }\\mathbf {A^{T}} \\tag{a} \\end{align} \\] from the definition of null space and equation \\(a\\) , we have \\[ \\begin{gather} \\mathbf {A^{T}(b-p)=0} \\\\\\\\ \\implies \\mathbf {A^{T}(b-A\\hat x)=0} \\\\\\\\ \\implies \\mathbf {A^{T}A\\hat x=A^{T}\\,b} \\\\\\\\ \\implies \\mathbf {\\hat x=(A^{T}A)^{-1}A^{T}\\,b} \\end{gather} \\] Besides, we now further have \\[ \\begin{gather} \\mathbf {p=A\\hat x=A(A^{T}A)^{-1}A^{T}\\,b} \\tag{b} \\\\\\\\ \\mathbf {P = A(A^{T}A)^{-1}A^{T}} \\end{gather} \\]","title":"Least-Squares Solution"},{"location":"Linear%20Algebra/Linear_Algebra/#pseudoinverse+aspect","text":"\\[ \\begin{gather} \\mathbf {A\\hat x=b} \\\\\\\\ \\implies \\mathbf {\\hat x=A^{\\dagger}\\,b} \\\\\\\\ \\implies \\mathbf {p=A\\hat x=AA^{\\dagger}\\,b} \\tag{c} \\end{gather} \\] compare equation \\(b\\) and \\(c\\) \\[ \\begin{align} \\mathbf {p}&=\\mathbf {A\\hat x=AA^{\\dagger}\\,b} \\\\\\\\ &= (\\mathbf {U_r D V_r^{T}})(\\mathbf {V_r D^{-1} U_r^{T}})\\,\\mathbf b \\\\\\\\ &= \\mathbf {U_r U_r^{T}}\\,\\mathbf {b} \\end{align} \\] by theorem of Orthogonal Projection , we know that \\[ \\begin{gather} \\mathbf {P} = \\mathbf {AA^{\\dagger}} = \\mathbf {U_r U_r^{T}} \\end{gather} \\]","title":"Pseudoinverse Aspect"},{"location":"Machine%20Learning/MPPP/","text":"Music Popularity Prediction Targets \u00b6 target \u2014 Predict Popularity with acoustic (low-level) feature only(?. Popularity \u00b6 Popularity can be described in various perspectives. 1 social factors sometimes play an important role in determining whether a song would be popular. ( Experimental study of inequality and unpredictability in an artificial cultural market ) either use stream or rank as criterion. currently use stream Data Sources \u00b6 track's statistics collected from Spotify: kworb.net audio source: Spotfiy API preview (only 30s) (seems enough) MIDI Methodology \u00b6 Train with low-level features only(? > Generic low-level features of songs, like the mel-spetrogram used in [4] and also throughout this work, may suffer from the \u201csemantic gap\u201d [15] and cannot lead to an accurate prediction model for a high-level concept such as hotness. 2 Mixed Region \u00b6 Tracks in single region are not sufficient for training. Thus, train with multi-region tracks may be more resonable. However, preference and population differ from regions. Region information have to be included. region cls_token region embeding Model \u00b6 Transformer targets predict stream-time curve can yield any metrices described in 1 MP2 (Music Propularity Prediction) \u00b6 predict sumation and debut as an regression problem fine-tuned MP2 model (Encoder) can be used in MP3 model. Encoder ( AST ) loss: MSE MP3 (Music Propularity Period Prediction) \u00b6 with decoder Predict stream-time curve Potential Augmentation \u00b6 use different preview clip of same tracks typically, there are more than one track for a song on Spotify (since they published by different album), and they may not have same preview clip. Specaugment Rolling Random Gain pitch timbre Q 2 A \u00b6 seems no comparsion (people use different datasets) Q 2 findout \u00b6 ways to treat null values in streams same song different version (live version, studio version, covered version...) different TA (country, generation, ...) significant difference instrumental? (without singer) Spotify youtube popularity relation? quality of audio Misc. \u00b6 Positional Encoding (sum or concat?) https://github.com/tensorflow/tensor2tensor/issues/1591 Observation \u00b6 Xmax-related songs get and only get popular near Xmas All I Want for Christmas Is You Last Christmas Underneath the Tree KKBox Researches \u00b6 Revisiting the problem of audio-based hit song prediction using convolutional neural networks \u00b6 Hit song prediction for pop music by siamese CNN with ranking loss \u00b6 A_Multimodal_End-to-End_Deep_Learning_Architecture_for_Music_Popularity_Prediction \u00b6 audio and lyrics Predicting Music Popularity Using Music Charts (Sep. 2019) \u00b6 target: whether a song is featured in Spotify's \"Top 50\" charts. Strategies \u00b6 uses social network data. predict the success of a song by acoustic information of previous successful songs. use past data from charts to predict whether a song will be featured in the same chart in the future. Recycle Bin \u00b6 Track Popularity Dataset (seems unuseful) \u00b6 Musical track popularity mining dataset: Extension & experimentation,\u2019\u2019 N http://mir.ilsp.gr/track_popularity.html Refs \u00b6 Music popularity: Metrics, characteristics, and audio-based prediction,\u201d IE \u21a9 \u21a9 HIT SONG PREDICTION FOR POP MUSIC BY SIAMESE CNN WITH RANKING LOSS Lang-Chi Yu\u2217 , Yi-Hsuan Yang\u2217 , Yun-Ning Hung\u2217 , Yi-An Chen\u2020 \u21a9","title":"MPPP"},{"location":"Machine%20Learning/MPPP/#targets","text":"target \u2014 Predict Popularity with acoustic (low-level) feature only(?.","title":"Targets"},{"location":"Machine%20Learning/MPPP/#popularity","text":"Popularity can be described in various perspectives. 1 social factors sometimes play an important role in determining whether a song would be popular. ( Experimental study of inequality and unpredictability in an artificial cultural market ) either use stream or rank as criterion. currently use stream","title":"Popularity"},{"location":"Machine%20Learning/MPPP/#data+sources","text":"track's statistics collected from Spotify: kworb.net audio source: Spotfiy API preview (only 30s) (seems enough) MIDI","title":"Data Sources"},{"location":"Machine%20Learning/MPPP/#methodology","text":"Train with low-level features only(? > Generic low-level features of songs, like the mel-spetrogram used in [4] and also throughout this work, may suffer from the \u201csemantic gap\u201d [15] and cannot lead to an accurate prediction model for a high-level concept such as hotness. 2","title":"Methodology"},{"location":"Machine%20Learning/MPPP/#mixed+region","text":"Tracks in single region are not sufficient for training. Thus, train with multi-region tracks may be more resonable. However, preference and population differ from regions. Region information have to be included. region cls_token region embeding","title":"Mixed Region"},{"location":"Machine%20Learning/MPPP/#model","text":"Transformer targets predict stream-time curve can yield any metrices described in 1","title":"Model"},{"location":"Machine%20Learning/MPPP/#mp2+music+propularity+prediction","text":"predict sumation and debut as an regression problem fine-tuned MP2 model (Encoder) can be used in MP3 model. Encoder ( AST ) loss: MSE","title":"MP2 (Music Propularity Prediction)"},{"location":"Machine%20Learning/MPPP/#mp3+music+propularity+period+prediction","text":"with decoder Predict stream-time curve","title":"MP3 (Music Propularity Period Prediction)"},{"location":"Machine%20Learning/MPPP/#potential+augmentation","text":"use different preview clip of same tracks typically, there are more than one track for a song on Spotify (since they published by different album), and they may not have same preview clip. Specaugment Rolling Random Gain pitch timbre","title":"Potential Augmentation"},{"location":"Machine%20Learning/MPPP/#q+2+a","text":"seems no comparsion (people use different datasets)","title":"Q 2 A"},{"location":"Machine%20Learning/MPPP/#q+2+findout","text":"ways to treat null values in streams same song different version (live version, studio version, covered version...) different TA (country, generation, ...) significant difference instrumental? (without singer) Spotify youtube popularity relation? quality of audio","title":"Q 2 findout"},{"location":"Machine%20Learning/MPPP/#misc","text":"Positional Encoding (sum or concat?) https://github.com/tensorflow/tensor2tensor/issues/1591","title":"Misc."},{"location":"Machine%20Learning/MPPP/#observation","text":"Xmax-related songs get and only get popular near Xmas All I Want for Christmas Is You Last Christmas Underneath the Tree","title":"Observation"},{"location":"Machine%20Learning/MPPP/#kkbox+researches","text":"","title":"KKBox Researches"},{"location":"Machine%20Learning/MPPP/#revisiting+the+problem+of+audio-based+hit+song+prediction+using+convolutional+neural+networks","text":"","title":"Revisiting the problem of audio-based hit song prediction using convolutional neural networks"},{"location":"Machine%20Learning/MPPP/#hit+song+prediction+for+pop+music+by+siamese+cnn+with+ranking+loss","text":"","title":"Hit song prediction for pop music by siamese CNN with ranking loss"},{"location":"Machine%20Learning/MPPP/#a_multimodal_end-to-end_deep_learning_architecture_for_music_popularity_prediction","text":"audio and lyrics","title":"A_Multimodal_End-to-End_Deep_Learning_Architecture_for_Music_Popularity_Prediction"},{"location":"Machine%20Learning/MPPP/#predicting+music+popularity+using+music+charts+sep+2019","text":"target: whether a song is featured in Spotify's \"Top 50\" charts.","title":"Predicting Music Popularity Using Music Charts (Sep. 2019)"},{"location":"Machine%20Learning/MPPP/#strategies","text":"uses social network data. predict the success of a song by acoustic information of previous successful songs. use past data from charts to predict whether a song will be featured in the same chart in the future.","title":"Strategies"},{"location":"Machine%20Learning/MPPP/#recycle+bin","text":"","title":"Recycle Bin"},{"location":"Machine%20Learning/MPPP/#track+popularity+dataset+seems+unuseful","text":"Musical track popularity mining dataset: Extension & experimentation,\u2019\u2019 N http://mir.ilsp.gr/track_popularity.html","title":"Track Popularity Dataset (seems unuseful)"},{"location":"Machine%20Learning/MPPP/#refs","text":"Music popularity: Metrics, characteristics, and audio-based prediction,\u201d IE \u21a9 \u21a9 HIT SONG PREDICTION FOR POP MUSIC BY SIAMESE CNN WITH RANKING LOSS Lang-Chi Yu\u2217 , Yi-Hsuan Yang\u2217 , Yun-Ning Hung\u2217 , Yi-An Chen\u2020 \u21a9","title":"Refs"},{"location":"Machine%20Learning/Machine%20Learning/","text":"To Read \u00b6 seq2seq for object detection some useful site \u00b6 Feature Engineering On the Relationship between Self-Attention and Convolutional Layers Restructuring of deep neural network acoustic models with singular value decomposition. Feature Disentangle arxiv.org/abs/1904.05742 arxiv.org/abs/1804.02812 arxiv.org/abs/1905.05879 Back Propagation \u00b6 let say Error Function \\(E\\) \\[ \\begin{align} E &= \\frac{1}{2}(y-d)^{2} \\\\\\\\ &= \\frac{1}{2}(\\sigma(v)-d)^{2} \\\\\\\\ &= \\frac{1}{2}\\bigg(\\sigma\\left(w^{T}y^{h}\\right)-d\\bigg)^{2} \\end{align} \\] in which - \\(y\\) : the output value - \\(d\\) : ground-truth value - \\(w^{T}\\) : \\([w_1\\, w_2\\, \\dots\\, w_i]\\) : last weights for the output then \\[ \\begin{array}{} \\displaystyle\\frac{\\partial E}{\\partial w_i} &= & \\bigg(\\sigma\\left(w_i\\,y_i^{h}\\right)-d\\bigg) &{\\bigg(\\sigma'\\left(w_iy_i^{h}\\right)\\bigg)} & y_i^{h} \\\\\\\\ &= & \\left(\\sigma(v)-d\\right) & \\sigma(v)(1-\\sigma(v)) & y_i^{h} \\end{array} \\] MLE \u00b6 https://www.ycc.idv.tw/deep-dl_3.html Maximum Likelihood Estimation \\[ \\begin{align} \\theta_{MLE} &= \\arg\\max_\\theta{\\bigg[p(X|\\theta)\\bigg]} \\\\\\\\ &= \\arg\\max_\\theta{\\bigg[\\ln {p(X|\\theta)\\bigg]}} \\\\\\\\ &= \\arg\\max_\\theta{\\bigg[E_{x\\sim p_{data}}\\big[\\ln {p(x|\\theta)\\big]\\bigg]}} \\end{align} \\] MAP \u00b6 Maximum A Posterior evidence \\(X\\) , parameters \\(\\theta\\) \\(p(X \\,| \\,\\theta)\\) : Likelihood \\(p(\\theta)\\) : Prior Probability, which is independent to \\(D\\) \\(p(\\theta \\,|\\, X)\\) : Posterior Probability By Bayes' law, \\[ \\begin{gather} P(\\theta\\,|\\,X)=\\frac{P(X|\\theta)\\,P(\\theta)}{P(X)} \\end{gather} \\] In optimization task, we care about parameters \\(\\theta\\) , so we can simply ignore \\(P(X)\\) when it come to MAP. \\[ \\begin{align} \\theta_{MAP} &= \\arg\\max_\\theta \\bigg[{P(\\theta|X)}\\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {\\frac{P(X|\\theta)\\,P(\\theta)}{P(X)}} \\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {P(X|\\theta)\\,P(\\theta)} \\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {\\ln P(X|\\theta)+ \\ln P(\\theta)} \\bigg] \\end{align} \\] Uniform Distribution \u21d2 \\(\\ln{p(\\theta \\ | \\ m)} =0\\) (like MLE that assume \\(\\theta\\) is uniform distribution) \\[ \\begin{align} \\theta_{MAP} &= \\arg\\max_\\theta \\bigg[ {\\ln P(X|\\theta)+ \\ln P(\\theta)} \\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {\\ln P(X|\\theta)} \\bigg] \\\\\\\\ &= \\theta_{MLE} \\end{align} \\] Normal Distribution \u21d2 \\(\\ln{p(\\theta \\ | \\ m)} \\propto \\theta^{2}\\) \u2003 (L2 Regulation) Laplace Distribution \u21d2 \\(\\ln{p(\\theta \\ | \\ m)} \\propto \\theta\\) \u2003 (L1 Regulation) Cross Entropy \u00b6 Entropy \u00b6 for an distribution \\(p\\) \\[ \\begin{gather} H(p)=E_{x\\sim p}\\big[-\\ln{p(x)}\\big] \\end{gather} \\] note \\[ \\begin{gather} E_{x\\sim p}[f(x)]= \\int_x{p(x)f(x)\\,dx} \\end{gather} \\] KL Divergence \u00b6 difference of given distribution \\(q\\) to the target distribution \\(p\\) \\((\\text{target's entropy}) - (\\text{cross entropy})\\) \\[ \\begin{align} D_{KL}(p||q)&=-E_{x\\sim p}\\big[\\ln q(x) - \\ln{p(x)}\\big] \\\\\\\\ &= H(p, q) - H(p) \\end{align} \\] Cross Entropy \u00b6 \\[ \\begin{align} H(p, q) = E_{x\\sim p}\\big[-\\ln q(x)\\big] \\end{align} \\] VAE \u00b6 Gaussian Mixture Model %% %% Here we define a normal distribution \\(p(z)\\) (There is no need to be normal.), then we can express the complex distribution \\(p(x)\\) as \\[ \\begin{gather} p(x) = \\int_z{p(z)\\,p(x|z)\\,dz} \\end{gather} \\] However, \\(p(x)\\) is intraceable then , try maximize the likelihood, \\[ \\begin{align} \\theta_{MLH} &= \\arg \\max_{\\theta} \\bigg[\\ln{p(x)}\\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ \\int_z{\\left(\\ln{p(z)}+\\ln{p(x|z)}\\right)\\,dz} \\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ \\int_z{\\ln{p(z)}\\,dz} + \\int_z{\\ln{p(x|z)}\\,dz} \\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ 1 + \\int_z{\\ln{p(x|z)}\\,dz} \\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ \\int_z{\\frac{}{f}\\,dz} \\bigg] \\end{align} \\] Suphx \u00b6 Building a Computer Mahjong Player via Deep Convolutional Neural Networks Saliency Map SmoothGrad (https://arxiv.org/abs/1706.30825 ) Integrated gradient (IG) ML Explanation \u00b6 Local Explanation( \\(\\sqrt{}\\) ) Why do u think this image is a cat Global Exp. What does a \"cat\" look like LIME(Local Interpretable Model-Agnostic Explanations) AlphaGo \u00b6 - tabula rasa \u00b6 Self Attention \u00b6 \\[ \\begin{align} Q&= \\begin{bmatrix} q1 & q2 & \\cdots \\end{bmatrix} \\\\\\\\ I&= \\begin{bmatrix} a1 & a2 & \\cdots \\end{bmatrix} \\qquad(\\text{input}) \\\\\\\\ Q&=W^{q}\\,I \\\\\\\\ K&=W^{k}\\,I \\\\\\\\ V&=W^{v}\\,I \\end{align} \\] \\(\\alpha_{1, 2}\\) is attention score of \\(q_1\\) with \\(k_2\\) , (by dot product ) i.e. \\[ \\begin{gather} \\mathbf{\\alpha_1}= \\begin{bmatrix} \\alpha_{1, 1} \\\\ \\alpha_{1, 2} \\\\ \\alpha_{1, 3} \\\\ \\vdots \\end{bmatrix} \\qquad \\mathbf{K}= \\left[ \\begin{matrix} k_{1} & k_{2} & k_{3} & \\cdots \\end{matrix} \\right] \\\\\\\\ \\alpha_1 = K^{T}\\, q_1 \\end{gather} \\] Let expand \\(\\alpha_i\\) to \\([\\alpha_1, \\alpha_2, \\dots]\\) \\[ \\begin{gather} \\begin{bmatrix} \\alpha_1 & \\alpha_2 & \\cdots \\end{bmatrix} =\\mathbf{K^{T}} \\, \\begin{bmatrix} q_1 & q_2 & \\cdots \\end{bmatrix} \\iff \\mathbf{A} =\\mathbf{K^{T}} \\, \\mathbf{Q} \\end{gather} \\] as we have the score of attention, it is intuitive to think the output (e.g. \\(b_1\\) ) is \\[ \\begin{gather} b_1 \\propto \\begin{bmatrix} a_1 & a_2 & a_3 & \\cdots \\end{bmatrix} \\begin{bmatrix} \\alpha_{1, 1} \\\\ \\alpha_{1, 2} \\\\ \\alpha_{1, 3} \\\\ \\alpha_{1, 4} \\\\ \\end{bmatrix} \\iff \\mathbf O \\propto \\mathbf I \\, \\mathbf A \\end{gather} \\] Thus we use \\(W^{v}\\) to extract the feature of input \\(I\\) . \\[ \\begin{gather} O=VA=(W^{v}\\,I)\\,A \\end{gather} \\]","title":"Machine Learning"},{"location":"Machine%20Learning/Machine%20Learning/#to+read","text":"seq2seq for object detection","title":"To Read"},{"location":"Machine%20Learning/Machine%20Learning/#some+useful+site","text":"Feature Engineering On the Relationship between Self-Attention and Convolutional Layers Restructuring of deep neural network acoustic models with singular value decomposition. Feature Disentangle arxiv.org/abs/1904.05742 arxiv.org/abs/1804.02812 arxiv.org/abs/1905.05879","title":"some useful site"},{"location":"Machine%20Learning/Machine%20Learning/#back+propagation","text":"let say Error Function \\(E\\) \\[ \\begin{align} E &= \\frac{1}{2}(y-d)^{2} \\\\\\\\ &= \\frac{1}{2}(\\sigma(v)-d)^{2} \\\\\\\\ &= \\frac{1}{2}\\bigg(\\sigma\\left(w^{T}y^{h}\\right)-d\\bigg)^{2} \\end{align} \\] in which - \\(y\\) : the output value - \\(d\\) : ground-truth value - \\(w^{T}\\) : \\([w_1\\, w_2\\, \\dots\\, w_i]\\) : last weights for the output then \\[ \\begin{array}{} \\displaystyle\\frac{\\partial E}{\\partial w_i} &= & \\bigg(\\sigma\\left(w_i\\,y_i^{h}\\right)-d\\bigg) &{\\bigg(\\sigma'\\left(w_iy_i^{h}\\right)\\bigg)} & y_i^{h} \\\\\\\\ &= & \\left(\\sigma(v)-d\\right) & \\sigma(v)(1-\\sigma(v)) & y_i^{h} \\end{array} \\]","title":"Back Propagation"},{"location":"Machine%20Learning/Machine%20Learning/#mle","text":"https://www.ycc.idv.tw/deep-dl_3.html Maximum Likelihood Estimation \\[ \\begin{align} \\theta_{MLE} &= \\arg\\max_\\theta{\\bigg[p(X|\\theta)\\bigg]} \\\\\\\\ &= \\arg\\max_\\theta{\\bigg[\\ln {p(X|\\theta)\\bigg]}} \\\\\\\\ &= \\arg\\max_\\theta{\\bigg[E_{x\\sim p_{data}}\\big[\\ln {p(x|\\theta)\\big]\\bigg]}} \\end{align} \\]","title":"MLE"},{"location":"Machine%20Learning/Machine%20Learning/#map","text":"Maximum A Posterior evidence \\(X\\) , parameters \\(\\theta\\) \\(p(X \\,| \\,\\theta)\\) : Likelihood \\(p(\\theta)\\) : Prior Probability, which is independent to \\(D\\) \\(p(\\theta \\,|\\, X)\\) : Posterior Probability By Bayes' law, \\[ \\begin{gather} P(\\theta\\,|\\,X)=\\frac{P(X|\\theta)\\,P(\\theta)}{P(X)} \\end{gather} \\] In optimization task, we care about parameters \\(\\theta\\) , so we can simply ignore \\(P(X)\\) when it come to MAP. \\[ \\begin{align} \\theta_{MAP} &= \\arg\\max_\\theta \\bigg[{P(\\theta|X)}\\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {\\frac{P(X|\\theta)\\,P(\\theta)}{P(X)}} \\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {P(X|\\theta)\\,P(\\theta)} \\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {\\ln P(X|\\theta)+ \\ln P(\\theta)} \\bigg] \\end{align} \\] Uniform Distribution \u21d2 \\(\\ln{p(\\theta \\ | \\ m)} =0\\) (like MLE that assume \\(\\theta\\) is uniform distribution) \\[ \\begin{align} \\theta_{MAP} &= \\arg\\max_\\theta \\bigg[ {\\ln P(X|\\theta)+ \\ln P(\\theta)} \\bigg] \\\\\\\\ &= \\arg\\max_\\theta \\bigg[ {\\ln P(X|\\theta)} \\bigg] \\\\\\\\ &= \\theta_{MLE} \\end{align} \\] Normal Distribution \u21d2 \\(\\ln{p(\\theta \\ | \\ m)} \\propto \\theta^{2}\\) \u2003 (L2 Regulation) Laplace Distribution \u21d2 \\(\\ln{p(\\theta \\ | \\ m)} \\propto \\theta\\) \u2003 (L1 Regulation)","title":"MAP"},{"location":"Machine%20Learning/Machine%20Learning/#cross+entropy","text":"","title":"Cross Entropy"},{"location":"Machine%20Learning/Machine%20Learning/#entropy","text":"for an distribution \\(p\\) \\[ \\begin{gather} H(p)=E_{x\\sim p}\\big[-\\ln{p(x)}\\big] \\end{gather} \\] note \\[ \\begin{gather} E_{x\\sim p}[f(x)]= \\int_x{p(x)f(x)\\,dx} \\end{gather} \\]","title":"Entropy"},{"location":"Machine%20Learning/Machine%20Learning/#kl+divergence","text":"difference of given distribution \\(q\\) to the target distribution \\(p\\) \\((\\text{target's entropy}) - (\\text{cross entropy})\\) \\[ \\begin{align} D_{KL}(p||q)&=-E_{x\\sim p}\\big[\\ln q(x) - \\ln{p(x)}\\big] \\\\\\\\ &= H(p, q) - H(p) \\end{align} \\]","title":"KL Divergence"},{"location":"Machine%20Learning/Machine%20Learning/#cross+entropy_1","text":"\\[ \\begin{align} H(p, q) = E_{x\\sim p}\\big[-\\ln q(x)\\big] \\end{align} \\]","title":"Cross Entropy"},{"location":"Machine%20Learning/Machine%20Learning/#vae","text":"Gaussian Mixture Model %% %% Here we define a normal distribution \\(p(z)\\) (There is no need to be normal.), then we can express the complex distribution \\(p(x)\\) as \\[ \\begin{gather} p(x) = \\int_z{p(z)\\,p(x|z)\\,dz} \\end{gather} \\] However, \\(p(x)\\) is intraceable then , try maximize the likelihood, \\[ \\begin{align} \\theta_{MLH} &= \\arg \\max_{\\theta} \\bigg[\\ln{p(x)}\\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ \\int_z{\\left(\\ln{p(z)}+\\ln{p(x|z)}\\right)\\,dz} \\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ \\int_z{\\ln{p(z)}\\,dz} + \\int_z{\\ln{p(x|z)}\\,dz} \\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ 1 + \\int_z{\\ln{p(x|z)}\\,dz} \\bigg] \\\\\\\\ &= \\arg \\max_{\\theta} \\bigg[ \\int_z{\\frac{}{f}\\,dz} \\bigg] \\end{align} \\]","title":"VAE"},{"location":"Machine%20Learning/Machine%20Learning/#suphx","text":"Building a Computer Mahjong Player via Deep Convolutional Neural Networks Saliency Map SmoothGrad (https://arxiv.org/abs/1706.30825 ) Integrated gradient (IG)","title":"Suphx"},{"location":"Machine%20Learning/Machine%20Learning/#ml+explanation","text":"Local Explanation( \\(\\sqrt{}\\) ) Why do u think this image is a cat Global Exp. What does a \"cat\" look like LIME(Local Interpretable Model-Agnostic Explanations)","title":"ML Explanation"},{"location":"Machine%20Learning/Machine%20Learning/#alphago","text":"","title":"AlphaGo"},{"location":"Machine%20Learning/Machine%20Learning/#-+tabula+rasa","text":"","title":"- tabula rasa"},{"location":"Machine%20Learning/Machine%20Learning/#self+attention","text":"\\[ \\begin{align} Q&= \\begin{bmatrix} q1 & q2 & \\cdots \\end{bmatrix} \\\\\\\\ I&= \\begin{bmatrix} a1 & a2 & \\cdots \\end{bmatrix} \\qquad(\\text{input}) \\\\\\\\ Q&=W^{q}\\,I \\\\\\\\ K&=W^{k}\\,I \\\\\\\\ V&=W^{v}\\,I \\end{align} \\] \\(\\alpha_{1, 2}\\) is attention score of \\(q_1\\) with \\(k_2\\) , (by dot product ) i.e. \\[ \\begin{gather} \\mathbf{\\alpha_1}= \\begin{bmatrix} \\alpha_{1, 1} \\\\ \\alpha_{1, 2} \\\\ \\alpha_{1, 3} \\\\ \\vdots \\end{bmatrix} \\qquad \\mathbf{K}= \\left[ \\begin{matrix} k_{1} & k_{2} & k_{3} & \\cdots \\end{matrix} \\right] \\\\\\\\ \\alpha_1 = K^{T}\\, q_1 \\end{gather} \\] Let expand \\(\\alpha_i\\) to \\([\\alpha_1, \\alpha_2, \\dots]\\) \\[ \\begin{gather} \\begin{bmatrix} \\alpha_1 & \\alpha_2 & \\cdots \\end{bmatrix} =\\mathbf{K^{T}} \\, \\begin{bmatrix} q_1 & q_2 & \\cdots \\end{bmatrix} \\iff \\mathbf{A} =\\mathbf{K^{T}} \\, \\mathbf{Q} \\end{gather} \\] as we have the score of attention, it is intuitive to think the output (e.g. \\(b_1\\) ) is \\[ \\begin{gather} b_1 \\propto \\begin{bmatrix} a_1 & a_2 & a_3 & \\cdots \\end{bmatrix} \\begin{bmatrix} \\alpha_{1, 1} \\\\ \\alpha_{1, 2} \\\\ \\alpha_{1, 3} \\\\ \\alpha_{1, 4} \\\\ \\end{bmatrix} \\iff \\mathbf O \\propto \\mathbf I \\, \\mathbf A \\end{gather} \\] Thus we use \\(W^{v}\\) to extract the feature of input \\(I\\) . \\[ \\begin{gather} O=VA=(W^{v}\\,I)\\,A \\end{gather} \\]","title":"Self Attention"},{"location":"Machine%20Learning/PCA/","text":"Lagrange Multiplier \u00b6 Statement \u2014 maximize \\(f(x, y)\\) which is subject to \\(g(x, y) = 0\\) with the concept showed in the image, we can say that \\[ \\begin{gather} \\nabla f \\parallel \\nabla g \\\\\\\\ \\implies \\nabla f = \\lambda \\nabla g \\end{gather} \\] in which \\(\\lambda\\) is known as Lagrange multiplier .","title":"PCA"},{"location":"Machine%20Learning/PCA/#lagrange+multiplier","text":"Statement \u2014 maximize \\(f(x, y)\\) which is subject to \\(g(x, y) = 0\\) with the concept showed in the image, we can say that \\[ \\begin{gather} \\nabla f \\parallel \\nabla g \\\\\\\\ \\implies \\nabla f = \\lambda \\nabla g \\end{gather} \\] in which \\(\\lambda\\) is known as Lagrange multiplier .","title":"Lagrange Multiplier"},{"location":"Machine%20Learning/cheat/","text":"conda \u00b6 create env \u00b6 conda create --name myenv python = 3 .8 copy env \u00b6 conda create --name <new_env> --clone <env_to_clone> pip \u00b6 pip freeze bug \u00b6 pip list --format = freeze > requirements.txt pytorch \u00b6 Install \u00b6 https://pytorch.org/get-started/locally/ conda install pytorch torchvision torchaudio cudatoolkit = 11 .3 -c pytorch pylint \u00b6 generate .pylintrc \u00b6 pylint --generate-rcfile > .pylintrc","title":"Cheat"},{"location":"Machine%20Learning/cheat/#conda","text":"","title":"conda"},{"location":"Machine%20Learning/cheat/#create+env","text":"conda create --name myenv python = 3 .8","title":"create env"},{"location":"Machine%20Learning/cheat/#copy+env","text":"conda create --name <new_env> --clone <env_to_clone>","title":"copy env"},{"location":"Machine%20Learning/cheat/#pip","text":"","title":"pip"},{"location":"Machine%20Learning/cheat/#pip+freeze+bug","text":"pip list --format = freeze > requirements.txt","title":"pip freeze bug"},{"location":"Machine%20Learning/cheat/#pytorch","text":"","title":"pytorch"},{"location":"Machine%20Learning/cheat/#install","text":"https://pytorch.org/get-started/locally/ conda install pytorch torchvision torchaudio cudatoolkit = 11 .3 -c pytorch","title":"Install"},{"location":"Machine%20Learning/cheat/#pylint","text":"","title":"pylint"},{"location":"Machine%20Learning/cheat/#generate+pylintrc","text":"pylint --generate-rcfile > .pylintrc","title":"generate .pylintrc"},{"location":"Machine%20Learning/consistency/","text":"static code analyze","title":"Consistency"},{"location":"Machine%20Learning/pylint/","text":"Workflow \u00b6 We use github flow in the future mandarin tutorial To be brief, the only thing you have to know is @at github: fork the repo git clone <your repo url> cd <repo-dir> ## after y do something git add . # stage all change git commit -m \"<Tell what y have done>\" # commit git push # push it to remote (github) @at github: submit pull request Getting Start with Python Locally \u00b6 Virtual Environment (and for package management) \u00b6 Without virtual environment, you are very likely to facing dependency error. There are Anaconda and venv available. Please, choose one to use. My recommendation is anaconda, but this is just because I am only familiar with it. There are plenty of tutorials on internet with traditional Chinese. Thus, go through then if u need them. IDE \u00b6 VSCode is highly recommended, please install those helpful extension for python development. e.g. Pylance , autoDocstring Typed Python \u00b6 tutorial Adding type hint to your code is important for both u and your teammate. for example def foo ( arg1 : int , arg2 : str ) -> Tuple [ int , str ]: do_some_thing ... return arg1 , arg2 please implement all type hint in your code. However, in some case you can just ignore it. class Foo : @classmethod def a_classmethod ( cls , arg : int ): # cls is implict with type Foo, no need to specify pass def member_func ( self , arg : int ): # self is implict with type Foo, no need to specify do some thing ... return def foo ( * args , ** kwargs ): \"\"\"args and kwargs is implicitly the type 'tuple' and 'dict' respectively.\"\"\" pass Type hint is not guaranteed to check by interpreter at runtime. However, it is very important to programmer, since we can therefore get help by Pylance For example class Foo : name : str \"\"\"tell pylance there is an attribute 'name' with type 'str'\"\"\" def __init__ ( self , name : str ): self . name = name return def say_hello ( self ) -> None : print ( f \"hello { self . name } \" ) foo = Foo ( \"world\" ) print ( foo . name ) # pylance mark the 'name' as the color as 'variable' >>> 'world' print ( foo . naem ) # pylance do not mark the word 'naem' which tell you something may wrong. >>> attribute error print ( foo . name . upper ()) # pylance: Oh I know there is a method for the type 'str' called 'upper', hover your mouse on 'upper' to get the docs for upper if u need it. >>> 'WORLD' Sometime, there may be some function that do not explicitly tell the return type. However, you can still tell pylance what the type is. ```python output: Tensor = model(inputs) output is a Tensor no matter how the model(inputs) actually return \u00b6 for the function returning tuple, there is two way to do explicit type hint. ```python # wrong syntax a: str, b: int = func_return_str_and_int() # correct from typing import Tuple tem: Tuple[str, int] = func_return_str_and_int() a, b = tem # correct and recommanded a, b = func_return_str_and_int() a: str # do not thing but tell a is a string b: int # do not thing but tell b is an int \u5927\u539f\u5247\u662f\u8b93 pylance \u8a8d\u5f97\u6240\u6709 literal \uff0c\u4e00\u822c\u800c\u8a00 (variable, attribute, ...), function, (class and module) \u6703\u5206\u5225\u662f\u4e00\u500b\u984f\u8272\u3002\u5982\u679c\u6709\u4efb\u4f55\u7684\u300c\u767d\u5b57\u300d\uff08actually depend on theme\uff09\u90a3\u5c31\u662f\u4ee3\u8868\u6709\u6771\u897f\u9700\u8981\u52a0\u4e0a type hint \uff0c\u6216\u662f\u4f60\u5beb\u932f\u4e86\u3002 Best Practice \u00b6 Reusing Structure \u00b6 bad example def deal_with_history ( history ): loss = history [ \"train_loss\" ] # pylance: what is 'loss'? accuracy = history [ \"train_accuracy\" ] # get attribute error this line pass history = { \"train_loss\" : 1.01 , \"train_acc\" : 0.99 } deal_with_history ( history ) You would not get or find error until runtime. Better Practice by Namespace from argparse import Namespace class History ( Namespace ): train_loss : float train_acc : float def deal_with_history ( history : History ): loss = history . train_loss # pylance: yes there is an attribute 'train_loss' for history with type 'float' accuracy = history . train_accuracy # pylance: what is 'train_accuracy' ? pass history = History ( train_loss = 1.01 , train_acc = 0.99 ) deal_with_history ( history ) As you can see, the main advantage of using Typed Object (In this case, Namespace) is you can find error right after you type history.train_accuracy , rather than get an error until runtime. Another typed structure for tuple is namedtuple , for dict is TypedDict . However, Namespace and namedtuple is more recommended. The reason why TypedDict is not so recommended is TypedDict is work the same as the normal dict except for type hint. And as we know that we can only get attribute of value by string from dict, and string sometime suppress warning to typo. e.g. try : from typing import TypedDict # notice that this is for python 3.8 and after except ImportError : from typing_extensions import TypedDict # use this for python < 3.8 class Image ( TypeDict ): color : str size : Tuple [ int , int ] img = Image ( color = \"blue\" , size = [ 100 , 100 ]) print ( img [ \"size\" ]) # pylance: well... yes print ( img [ \"colour\" ]) # pylance: well... yes (but get runtime error because no attribute called 'colour') Assertion \u00b6 see official document An example: def do_something ( model : nn . Module , mode : Literal [ \"train\" , \"eval\" ]): assert mode in [ \"train\" , \"eval\" ], f \"expect mode is 'train' or 'eval', got { mode } \" Adding assert in your code appropriately tell other what exception u have concerned, and help others debug by a comprehensive message, instead of go through all your code. I wrote this example above with mode: Literal . However, it may be a better idea to use ENUM. (google python ENUM) from enum import Enum , auto class Mode ( Enum ): TRAIN = auto () EVAL = auto () VALID = EVAL # alias for EVAL INFERENCE = auto () mode1 = Mode . TRAIN mode2 = Mode . EVAL print ( mode1 is Mode . TRAIN ) >>> True print ( mode1 is mode2 ) >>> False print ( mode2 is Mode . VALID ) >>> True def do_something ( model : nn . Module , mode : Mode ): assert isinstance ( mode , Mode ), f \"expect mode is an instance of Mode, got { type ( mode ) } however\" pass Pylint \u00b6 Pylint is lint for python. (Lint is a rule checker for code consistency.) tutorial Pylint is basic depend on the style guide PEP8 Lint in terminal \u00b6 in terminal, type pylint <module_name or py_file.py> pylint well calculate the score of your code. like Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00) If you score is not 10/10, pylint would tell you why and where(in which file which line) make u lose score for example ************* Module do do.py:71:0: C0301: Line too long (113/100) (line-too-long) do.py:18:8: W0612: Unused variable 'cat' (unused-variable) do.py:25:8: W0612: Unused variable 'cat' (unused-variable) do.py:33:8: W0612: Unused variable 'cat' (unused-variable) do.py:50:0: C0413: Import \"from nfnet.config import NfnetConfig\" should be placed at the top of the module (wrong-import-position) do.py:51:0: C0413: Import \"from nfnet.nfnet_model_utils import NfnetModelUtils\" should be placed at the top of the module (wrong-import-position) do.py:69:8: W0612: Unused variable 'cat' (unused-variable) do.py:5:0: W0611: Unused FatLeNet5 imported from imgclf.models (unused-import) do.py:5:0: W0611: Unused FakeVGG16 imported from imgclf.models (unused-import) do.py:7:0: W0611: Unused Hw2Config imported from hw2.config (unused-import) do.py:8:0: W0611: Unused AlexNet imported from imgclf.models.models (unused-import) do.py:8:0: C0412: Imports from package imgclf are not grouped (ungrouped-imports) ------------------------------------------------------------------ Your code has been rated at 8.03/10 (previous run: 8.03/10, +0.00) We accept only 10/10 score for every commit, and you may not want to solve plenty of issue at once. Thus, considering enable pylint in vscode. Enable Pylint in VSCode \u00b6 control + shift + p ( cmd + shift + p for Mac) calling command palette type python: lint in the input box choose Python: Select Linter and choose pylint redo 1. and 2. choose Python: Enable/Disable Linting and choose enable Handling Pylint Error/Warning/... \u00b6 If you get a message and you don't know how to solve it. Find document for messages here https://pylint.pycqa.org/en/latest/messages/messages_list.html or just google (or ask me or other teammates). Common Mistake \u00b6 Naming Style \u00b6 It is important to keep ur naming style consistent, and here is our naming style for different classes. snake_case variable module method PascalCase class name Type name UPERR_CASE constant (In Python constant is not real constant (in-redefinable) however, it is a good practice to name a variable with UPPER_CASE to tell other that variable should be treated as a constant) Docs \u00b6 Please add doc-string to every methods/function/module/class unless the methods/function/module/class is just a simple wrapping or it is self-explaining enough. In Python, we normally use google type docstirngs (you can handle this easily by the extension autoDocstring ) line-to-long \u00b6 Line limit is set to 100, please refactor the long line. For long if conditions, use if ( very_long_variable_name is not None and very_long_variable_name . field > 0 or very_long_variable_name . is_debug ): For long string, use implicit string concatenating syntax a_long_string = ( \"this is a long long sting\" \"this is the second line\" \"notice that this is all in the same line\" ) for others, use back splash \\ def abs ( somehow_this_is_a_long_variable : int ): res = somehow_this_is_a_long_variable if \\ somehow_this_is_a_long_variable > 0 \\ else - 1 * somehow_this_is_a_long_variable return res inconsistent-quotes \u00b6 using double quotes \"\" for string, notice that the single quote is still acceptable in the double-quote string. E.g. a_str = \"this is an inside 'single' quote\" # accepted b_str = 'string with outer single quote' # not accetped Import outside toplevel \u00b6 The import syntax should be on the top of file. Thus solving path issue by sys.append(... is not acceptable in modules. wrong-import-order \u00b6 Import should respect the order standard imports first, then third-party libraries, then local imports # ok import os # standard module import torch # third party module import .my_module # local module # wrong import os import .my_module import torch beside, always use from ___ import ___ if you can # wrong import torch.nn as nn # ok from torch import nn","title":"Pylint"},{"location":"Machine%20Learning/pylint/#workflow","text":"We use github flow in the future mandarin tutorial To be brief, the only thing you have to know is @at github: fork the repo git clone <your repo url> cd <repo-dir> ## after y do something git add . # stage all change git commit -m \"<Tell what y have done>\" # commit git push # push it to remote (github) @at github: submit pull request","title":"Workflow"},{"location":"Machine%20Learning/pylint/#getting+start+with+python+locally","text":"","title":"Getting Start with Python Locally"},{"location":"Machine%20Learning/pylint/#virtual+environment+and+for+package+management","text":"Without virtual environment, you are very likely to facing dependency error. There are Anaconda and venv available. Please, choose one to use. My recommendation is anaconda, but this is just because I am only familiar with it. There are plenty of tutorials on internet with traditional Chinese. Thus, go through then if u need them.","title":"Virtual Environment (and for package management)"},{"location":"Machine%20Learning/pylint/#ide","text":"VSCode is highly recommended, please install those helpful extension for python development. e.g. Pylance , autoDocstring","title":"IDE"},{"location":"Machine%20Learning/pylint/#typed+python","text":"tutorial Adding type hint to your code is important for both u and your teammate. for example def foo ( arg1 : int , arg2 : str ) -> Tuple [ int , str ]: do_some_thing ... return arg1 , arg2 please implement all type hint in your code. However, in some case you can just ignore it. class Foo : @classmethod def a_classmethod ( cls , arg : int ): # cls is implict with type Foo, no need to specify pass def member_func ( self , arg : int ): # self is implict with type Foo, no need to specify do some thing ... return def foo ( * args , ** kwargs ): \"\"\"args and kwargs is implicitly the type 'tuple' and 'dict' respectively.\"\"\" pass Type hint is not guaranteed to check by interpreter at runtime. However, it is very important to programmer, since we can therefore get help by Pylance For example class Foo : name : str \"\"\"tell pylance there is an attribute 'name' with type 'str'\"\"\" def __init__ ( self , name : str ): self . name = name return def say_hello ( self ) -> None : print ( f \"hello { self . name } \" ) foo = Foo ( \"world\" ) print ( foo . name ) # pylance mark the 'name' as the color as 'variable' >>> 'world' print ( foo . naem ) # pylance do not mark the word 'naem' which tell you something may wrong. >>> attribute error print ( foo . name . upper ()) # pylance: Oh I know there is a method for the type 'str' called 'upper', hover your mouse on 'upper' to get the docs for upper if u need it. >>> 'WORLD' Sometime, there may be some function that do not explicitly tell the return type. However, you can still tell pylance what the type is. ```python output: Tensor = model(inputs)","title":"Typed Python"},{"location":"Machine%20Learning/pylint/#output+is+a+tensor+no+matter+how+the+modelinputs+actually+return","text":"for the function returning tuple, there is two way to do explicit type hint. ```python # wrong syntax a: str, b: int = func_return_str_and_int() # correct from typing import Tuple tem: Tuple[str, int] = func_return_str_and_int() a, b = tem # correct and recommanded a, b = func_return_str_and_int() a: str # do not thing but tell a is a string b: int # do not thing but tell b is an int \u5927\u539f\u5247\u662f\u8b93 pylance \u8a8d\u5f97\u6240\u6709 literal \uff0c\u4e00\u822c\u800c\u8a00 (variable, attribute, ...), function, (class and module) \u6703\u5206\u5225\u662f\u4e00\u500b\u984f\u8272\u3002\u5982\u679c\u6709\u4efb\u4f55\u7684\u300c\u767d\u5b57\u300d\uff08actually depend on theme\uff09\u90a3\u5c31\u662f\u4ee3\u8868\u6709\u6771\u897f\u9700\u8981\u52a0\u4e0a type hint \uff0c\u6216\u662f\u4f60\u5beb\u932f\u4e86\u3002","title":"output is a Tensor no matter how the model(inputs) actually return"},{"location":"Machine%20Learning/pylint/#best+practice","text":"","title":"Best Practice"},{"location":"Machine%20Learning/pylint/#reusing+structure","text":"bad example def deal_with_history ( history ): loss = history [ \"train_loss\" ] # pylance: what is 'loss'? accuracy = history [ \"train_accuracy\" ] # get attribute error this line pass history = { \"train_loss\" : 1.01 , \"train_acc\" : 0.99 } deal_with_history ( history ) You would not get or find error until runtime. Better Practice by Namespace from argparse import Namespace class History ( Namespace ): train_loss : float train_acc : float def deal_with_history ( history : History ): loss = history . train_loss # pylance: yes there is an attribute 'train_loss' for history with type 'float' accuracy = history . train_accuracy # pylance: what is 'train_accuracy' ? pass history = History ( train_loss = 1.01 , train_acc = 0.99 ) deal_with_history ( history ) As you can see, the main advantage of using Typed Object (In this case, Namespace) is you can find error right after you type history.train_accuracy , rather than get an error until runtime. Another typed structure for tuple is namedtuple , for dict is TypedDict . However, Namespace and namedtuple is more recommended. The reason why TypedDict is not so recommended is TypedDict is work the same as the normal dict except for type hint. And as we know that we can only get attribute of value by string from dict, and string sometime suppress warning to typo. e.g. try : from typing import TypedDict # notice that this is for python 3.8 and after except ImportError : from typing_extensions import TypedDict # use this for python < 3.8 class Image ( TypeDict ): color : str size : Tuple [ int , int ] img = Image ( color = \"blue\" , size = [ 100 , 100 ]) print ( img [ \"size\" ]) # pylance: well... yes print ( img [ \"colour\" ]) # pylance: well... yes (but get runtime error because no attribute called 'colour')","title":"Reusing Structure"},{"location":"Machine%20Learning/pylint/#assertion","text":"see official document An example: def do_something ( model : nn . Module , mode : Literal [ \"train\" , \"eval\" ]): assert mode in [ \"train\" , \"eval\" ], f \"expect mode is 'train' or 'eval', got { mode } \" Adding assert in your code appropriately tell other what exception u have concerned, and help others debug by a comprehensive message, instead of go through all your code. I wrote this example above with mode: Literal . However, it may be a better idea to use ENUM. (google python ENUM) from enum import Enum , auto class Mode ( Enum ): TRAIN = auto () EVAL = auto () VALID = EVAL # alias for EVAL INFERENCE = auto () mode1 = Mode . TRAIN mode2 = Mode . EVAL print ( mode1 is Mode . TRAIN ) >>> True print ( mode1 is mode2 ) >>> False print ( mode2 is Mode . VALID ) >>> True def do_something ( model : nn . Module , mode : Mode ): assert isinstance ( mode , Mode ), f \"expect mode is an instance of Mode, got { type ( mode ) } however\" pass","title":"Assertion"},{"location":"Machine%20Learning/pylint/#pylint","text":"Pylint is lint for python. (Lint is a rule checker for code consistency.) tutorial Pylint is basic depend on the style guide PEP8","title":"Pylint"},{"location":"Machine%20Learning/pylint/#lint+in+terminal","text":"in terminal, type pylint <module_name or py_file.py> pylint well calculate the score of your code. like Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00) If you score is not 10/10, pylint would tell you why and where(in which file which line) make u lose score for example ************* Module do do.py:71:0: C0301: Line too long (113/100) (line-too-long) do.py:18:8: W0612: Unused variable 'cat' (unused-variable) do.py:25:8: W0612: Unused variable 'cat' (unused-variable) do.py:33:8: W0612: Unused variable 'cat' (unused-variable) do.py:50:0: C0413: Import \"from nfnet.config import NfnetConfig\" should be placed at the top of the module (wrong-import-position) do.py:51:0: C0413: Import \"from nfnet.nfnet_model_utils import NfnetModelUtils\" should be placed at the top of the module (wrong-import-position) do.py:69:8: W0612: Unused variable 'cat' (unused-variable) do.py:5:0: W0611: Unused FatLeNet5 imported from imgclf.models (unused-import) do.py:5:0: W0611: Unused FakeVGG16 imported from imgclf.models (unused-import) do.py:7:0: W0611: Unused Hw2Config imported from hw2.config (unused-import) do.py:8:0: W0611: Unused AlexNet imported from imgclf.models.models (unused-import) do.py:8:0: C0412: Imports from package imgclf are not grouped (ungrouped-imports) ------------------------------------------------------------------ Your code has been rated at 8.03/10 (previous run: 8.03/10, +0.00) We accept only 10/10 score for every commit, and you may not want to solve plenty of issue at once. Thus, considering enable pylint in vscode.","title":"Lint in terminal"},{"location":"Machine%20Learning/pylint/#enable+pylint+in+vscode","text":"control + shift + p ( cmd + shift + p for Mac) calling command palette type python: lint in the input box choose Python: Select Linter and choose pylint redo 1. and 2. choose Python: Enable/Disable Linting and choose enable","title":"Enable Pylint in VSCode"},{"location":"Machine%20Learning/pylint/#handling+pylint+errorwarning","text":"If you get a message and you don't know how to solve it. Find document for messages here https://pylint.pycqa.org/en/latest/messages/messages_list.html or just google (or ask me or other teammates).","title":"Handling Pylint Error/Warning/..."},{"location":"Machine%20Learning/pylint/#common+mistake","text":"","title":"Common Mistake"},{"location":"Machine%20Learning/pylint/#naming+style","text":"It is important to keep ur naming style consistent, and here is our naming style for different classes. snake_case variable module method PascalCase class name Type name UPERR_CASE constant (In Python constant is not real constant (in-redefinable) however, it is a good practice to name a variable with UPPER_CASE to tell other that variable should be treated as a constant)","title":"Naming Style"},{"location":"Machine%20Learning/pylint/#docs","text":"Please add doc-string to every methods/function/module/class unless the methods/function/module/class is just a simple wrapping or it is self-explaining enough. In Python, we normally use google type docstirngs (you can handle this easily by the extension autoDocstring )","title":"Docs"},{"location":"Machine%20Learning/pylint/#line-to-long","text":"Line limit is set to 100, please refactor the long line. For long if conditions, use if ( very_long_variable_name is not None and very_long_variable_name . field > 0 or very_long_variable_name . is_debug ): For long string, use implicit string concatenating syntax a_long_string = ( \"this is a long long sting\" \"this is the second line\" \"notice that this is all in the same line\" ) for others, use back splash \\ def abs ( somehow_this_is_a_long_variable : int ): res = somehow_this_is_a_long_variable if \\ somehow_this_is_a_long_variable > 0 \\ else - 1 * somehow_this_is_a_long_variable return res","title":"line-to-long"},{"location":"Machine%20Learning/pylint/#inconsistent-quotes","text":"using double quotes \"\" for string, notice that the single quote is still acceptable in the double-quote string. E.g. a_str = \"this is an inside 'single' quote\" # accepted b_str = 'string with outer single quote' # not accetped","title":"inconsistent-quotes"},{"location":"Machine%20Learning/pylint/#import+outside+toplevel","text":"The import syntax should be on the top of file. Thus solving path issue by sys.append(... is not acceptable in modules.","title":"Import outside toplevel"},{"location":"Machine%20Learning/pylint/#wrong-import-order","text":"Import should respect the order standard imports first, then third-party libraries, then local imports # ok import os # standard module import torch # third party module import .my_module # local module # wrong import os import .my_module import torch beside, always use from ___ import ___ if you can # wrong import torch.nn as nn # ok from torch import nn","title":"wrong-import-order"},{"location":"OS/Mid1/","text":"Computer System Structure \u00b6 Hardware OS Application Users OS \u00b6 main proposes resource allocator resource manager control program interrupt driven System Call \u00b6 Purpose interface of the service provided from OS types of system call process control file manipulation device manipulation information maintenance communications protection Interrupt \u00b6 hardware-generated change of flow within the system interrupt handler is called to deal with the cause of interrupt Single-Processor \u00b6 One general-propose CPU - run all of the tasks including user's processes One special-propose processor - lies in most of single-processor system Multiprocessors \u00b6 Shares the computer bus and clock, memory, and peripheral devices. Aka parallel systems tightly-coupled systems Pro Increased throughput - speed up Economy of scale - cost less than N * single-processor systems Increased reliability - failure of one processor won't halt the system, only slow it down Asymmetric Multiprocessing \u00b6 Each processor is assigned a specie task The master processor schedules and allocates work to the slave processor Symmetric Multiprocessing \u00b6 Each processor performs all tasks All processors are peers; no master-slave relationship exists Uniform Memory Access \u00b6 All of RAM takes the same amount of time to access. Non-uniform Memory Access \u00b6 Some parts of memory may takes longer to access than other parts, creating a performance penalty. Multiprocessor System v.s. Clustered System \u00b6 Multiprocessor System shares hardware, while clustered system have their own hardware for each computer. Multiprocessor System could communicate using shared memory, while clustered system using message. Clustered system requires stable and high speed network in order to synchronize Multi-programming System \u00b6 Maximize the use of CPU time so that CPU and I/O can be busy all the time. switch job basically due to waiting for I/O. Multi-tasking System \u00b6 aka time-sharing system a logical extension of multi-programming system switch among jobs so frequently that users can interact with each program while it's running Schedulers \u00b6 Short-term Scheduler \u00b6 aka CPU scheduler allocate one of the ready-to-execute processes to CPU invoked frequently (in ms) Long-term Scheduler \u00b6 aka job scheduler controls the degree of multiprogramming invoked infrequently (in sec, min) Mid-term Scheduler \u00b6 reduce the degree of multiprogramming swapping : remove process form memory, and store on disk, swap in to continue execution Process State \u00b6 new : The process is being created running : Instruction are being executed waiting : The process is waiting for some event to occur ready : The process is waiting to be assigned to processor terminated : The process has finished execution Process Control Block \u00b6 Process state Program counter CPU registers CPU scheduling information Memory-management information Accounting information I/O status information Direct Memory Access \u00b6 Please explain why direct memory access (DMA) is considered an efficient mechanism for performing I/O. DMA is efficient for moving large amounts of data between I/O devices and main memory. It is considered efficient because it removes the CPU from being responsible for transferring data. DMA instructs the device controller to move data between the devices and main memory. DMA \u6280\u8853\u53ef\u4ee5\u8b93\u67d0\u4e9b\u96fb\u8166\u5167\u90e8\u7d44\u4ef6\u76f4\u63a5\u8b80\u5beb\u8a18\u61b6\u9ad4\u800c\u4e0d\u9700\u900f\u904e CPU\uff0c\u5982 \u6b64\u53ef \u4ee5\u964d\u4f4e CPU \u904b\u7b97\u8ca0\u64d4\uff0c\u4f7f\u6574\u9ad4\u7cfb\u7d71\u904b\u7b97\u901f\u5ea6\u52a0\u5feb\u3002 Layered Approach \u00b6 Simplicity of construction Simplifies debugging Micro-kernel System \u00b6 Minimize kernel Implement then as system and user-level programs Communication Models \u00b6 Shared Memory \u00b6 Pro - faster Con - have to ensure no writing to same place simultaneously Message Passing \u00b6 exchange messages via system call. Pros - useful for light data, no conflicts, easier to implement Cons - slower","title":"Mid1"},{"location":"OS/Mid1/#computer+system+structure","text":"Hardware OS Application Users","title":"Computer System Structure"},{"location":"OS/Mid1/#os","text":"main proposes resource allocator resource manager control program interrupt driven","title":"OS"},{"location":"OS/Mid1/#system+call","text":"Purpose interface of the service provided from OS types of system call process control file manipulation device manipulation information maintenance communications protection","title":"System Call"},{"location":"OS/Mid1/#interrupt","text":"hardware-generated change of flow within the system interrupt handler is called to deal with the cause of interrupt","title":"Interrupt"},{"location":"OS/Mid1/#single-processor","text":"One general-propose CPU - run all of the tasks including user's processes One special-propose processor - lies in most of single-processor system","title":"Single-Processor"},{"location":"OS/Mid1/#multiprocessors","text":"Shares the computer bus and clock, memory, and peripheral devices. Aka parallel systems tightly-coupled systems Pro Increased throughput - speed up Economy of scale - cost less than N * single-processor systems Increased reliability - failure of one processor won't halt the system, only slow it down","title":"Multiprocessors"},{"location":"OS/Mid1/#asymmetric+multiprocessing","text":"Each processor is assigned a specie task The master processor schedules and allocates work to the slave processor","title":"Asymmetric Multiprocessing"},{"location":"OS/Mid1/#symmetric+multiprocessing","text":"Each processor performs all tasks All processors are peers; no master-slave relationship exists","title":"Symmetric Multiprocessing"},{"location":"OS/Mid1/#uniform+memory+access","text":"All of RAM takes the same amount of time to access.","title":"Uniform Memory Access"},{"location":"OS/Mid1/#non-uniform+memory+access","text":"Some parts of memory may takes longer to access than other parts, creating a performance penalty.","title":"Non-uniform Memory Access"},{"location":"OS/Mid1/#multiprocessor+system+vs+clustered+system","text":"Multiprocessor System shares hardware, while clustered system have their own hardware for each computer. Multiprocessor System could communicate using shared memory, while clustered system using message. Clustered system requires stable and high speed network in order to synchronize","title":"Multiprocessor System v.s. Clustered System"},{"location":"OS/Mid1/#multi-programming+system","text":"Maximize the use of CPU time so that CPU and I/O can be busy all the time. switch job basically due to waiting for I/O.","title":"Multi-programming System"},{"location":"OS/Mid1/#multi-tasking+system","text":"aka time-sharing system a logical extension of multi-programming system switch among jobs so frequently that users can interact with each program while it's running","title":"Multi-tasking System"},{"location":"OS/Mid1/#schedulers","text":"","title":"Schedulers"},{"location":"OS/Mid1/#short-term+scheduler","text":"aka CPU scheduler allocate one of the ready-to-execute processes to CPU invoked frequently (in ms)","title":"Short-term Scheduler"},{"location":"OS/Mid1/#long-term+scheduler","text":"aka job scheduler controls the degree of multiprogramming invoked infrequently (in sec, min)","title":"Long-term Scheduler"},{"location":"OS/Mid1/#mid-term+scheduler","text":"reduce the degree of multiprogramming swapping : remove process form memory, and store on disk, swap in to continue execution","title":"Mid-term Scheduler"},{"location":"OS/Mid1/#process+state","text":"new : The process is being created running : Instruction are being executed waiting : The process is waiting for some event to occur ready : The process is waiting to be assigned to processor terminated : The process has finished execution","title":"Process State"},{"location":"OS/Mid1/#process+control+block","text":"Process state Program counter CPU registers CPU scheduling information Memory-management information Accounting information I/O status information","title":"Process Control Block"},{"location":"OS/Mid1/#direct+memory+access","text":"Please explain why direct memory access (DMA) is considered an efficient mechanism for performing I/O. DMA is efficient for moving large amounts of data between I/O devices and main memory. It is considered efficient because it removes the CPU from being responsible for transferring data. DMA instructs the device controller to move data between the devices and main memory. DMA \u6280\u8853\u53ef\u4ee5\u8b93\u67d0\u4e9b\u96fb\u8166\u5167\u90e8\u7d44\u4ef6\u76f4\u63a5\u8b80\u5beb\u8a18\u61b6\u9ad4\u800c\u4e0d\u9700\u900f\u904e CPU\uff0c\u5982 \u6b64\u53ef \u4ee5\u964d\u4f4e CPU \u904b\u7b97\u8ca0\u64d4\uff0c\u4f7f\u6574\u9ad4\u7cfb\u7d71\u904b\u7b97\u901f\u5ea6\u52a0\u5feb\u3002","title":"Direct Memory Access"},{"location":"OS/Mid1/#layered+approach","text":"Simplicity of construction Simplifies debugging","title":"Layered Approach"},{"location":"OS/Mid1/#micro-kernel+system","text":"Minimize kernel Implement then as system and user-level programs","title":"Micro-kernel System"},{"location":"OS/Mid1/#communication+models","text":"","title":"Communication Models"},{"location":"OS/Mid1/#shared+memory","text":"Pro - faster Con - have to ensure no writing to same place simultaneously","title":"Shared Memory"},{"location":"OS/Mid1/#message+passing","text":"exchange messages via system call. Pros - useful for light data, no conflicts, easier to implement Cons - slower","title":"Message Passing"},{"location":"OS/Mid2/","text":"Communications in Client-Server Systems \u00b6 Socket \u00b6 low-level form of communication only allow an unstructured stream of bytes to be exchanged between the communicating threads. RPC \u00b6 aka Remote Procedure Calls higher level form of communication than Socket transmit parameters which are marshaled RPC abstracts procedure call between processes on networked systems. issues must be dealt in RPC Differences in data representation on the client and server machines the semantics of a call (messages are acted on exactly once or at most once) Communication between a server and a client Pipes \u00b6 Concurrency \u00b6 two or more tasks can start, run and complete in overlapping time periods for example multitasking on single-core machine Parallelism \u00b6 at least two threads are executing simultaneously for example on a multi-core processor. Lightweight process \u00b6 LWP M:M and Two-level models require communication to maintain the appropriate number of kernel threads allocated to the application an intermediate data structure between the user thread and kernel thread Thread \u00b6 User-level thread Kernel-level thread It is managed by thread library in user space. It is managed by kernel Faster context switching Slower context switching Whole process will be blocked if one of threads calls blocking system call. Whole process will not be blocked if one of threads calls blocking system call. Better utilization on multiprocessors. No advantage on multiprocessors. Scheduling Algorithm \u00b6 FCFS \u00b6 First-Come First-Served must be non-preemptive troublesome in time-sharing systems SJF \u00b6 Shortest-Job_First frequently used in long-term scheduling. RR \u00b6 Round-Robin for time-sharing systems Convoy Effect \u00b6 Short process behind long process All the other processes wait for the one big process to get off the CPU Results in lower CPU and device utilization Affinity \u00b6 Soft affinity possible to migrate between processors Hard affinity run on a specify subset of processors","title":"Mid2"},{"location":"OS/Mid2/#communications+in+client-server+systems","text":"","title":"Communications in Client-Server Systems"},{"location":"OS/Mid2/#socket","text":"low-level form of communication only allow an unstructured stream of bytes to be exchanged between the communicating threads.","title":"Socket"},{"location":"OS/Mid2/#rpc","text":"aka Remote Procedure Calls higher level form of communication than Socket transmit parameters which are marshaled RPC abstracts procedure call between processes on networked systems. issues must be dealt in RPC Differences in data representation on the client and server machines the semantics of a call (messages are acted on exactly once or at most once) Communication between a server and a client","title":"RPC"},{"location":"OS/Mid2/#pipes","text":"","title":"Pipes"},{"location":"OS/Mid2/#concurrency","text":"two or more tasks can start, run and complete in overlapping time periods for example multitasking on single-core machine","title":"Concurrency"},{"location":"OS/Mid2/#parallelism","text":"at least two threads are executing simultaneously for example on a multi-core processor.","title":"Parallelism"},{"location":"OS/Mid2/#lightweight+process","text":"LWP M:M and Two-level models require communication to maintain the appropriate number of kernel threads allocated to the application an intermediate data structure between the user thread and kernel thread","title":"Lightweight process"},{"location":"OS/Mid2/#thread","text":"User-level thread Kernel-level thread It is managed by thread library in user space. It is managed by kernel Faster context switching Slower context switching Whole process will be blocked if one of threads calls blocking system call. Whole process will not be blocked if one of threads calls blocking system call. Better utilization on multiprocessors. No advantage on multiprocessors.","title":"Thread"},{"location":"OS/Mid2/#scheduling+algorithm","text":"","title":"Scheduling Algorithm"},{"location":"OS/Mid2/#fcfs","text":"First-Come First-Served must be non-preemptive troublesome in time-sharing systems","title":"FCFS"},{"location":"OS/Mid2/#sjf","text":"Shortest-Job_First frequently used in long-term scheduling.","title":"SJF"},{"location":"OS/Mid2/#rr","text":"Round-Robin for time-sharing systems","title":"RR"},{"location":"OS/Mid2/#convoy+effect","text":"Short process behind long process All the other processes wait for the one big process to get off the CPU Results in lower CPU and device utilization","title":"Convoy Effect"},{"location":"OS/Mid2/#affinity","text":"Soft affinity possible to migrate between processors Hard affinity run on a specify subset of processors","title":"Affinity"},{"location":"OS/Mid3/","text":"Peterson solution \u00b6 exam_2_ans.pdf Deadlock \u00b6 4 necessary conditions - Mutual exclusion : only 1 process at a time can use a resource - Hold & wait : a process holding some resources and is waiting for another resource. - No preemption : a resource can be only released a process voluntarily. - Circular wait","title":"Mid3"},{"location":"OS/Mid3/#peterson+solution","text":"exam_2_ans.pdf","title":"Peterson solution"},{"location":"OS/Mid3/#deadlock","text":"4 necessary conditions - Mutual exclusion : only 1 process at a time can use a resource - Hold & wait : a process holding some resources and is waiting for another resource. - No preemption : a resource can be only released a process voluntarily. - Circular wait","title":"Deadlock"},{"location":"Probability%20%26%20Statistics/Distributions/","text":"Binomial and Negative Binomial \u00b6 Binomial Distribution \u00b6 distribution of number of success \\(r\\) given number of trails \\(n\\) \\[ \\begin{gather} P(X=r)=\\binom{n}{r}p^{r}(1-p)^{n-r} \\end{gather} \\] Negative Binomial Distribution \u00b6 distribution of number of trails \\(n\\) given number of success \\(r\\) namely, distribution of number of failures \\(k\\) given number of success \\(r\\) \\[ \\begin{gather} P(X=n)=\\binom{n-1}{r-1}p^{r}(1-p)^{n-r} \\end{gather} \\] Geometric Distribution \u00b6 number of trails \\(n\\) until success \\[ \\begin{gather} P(X=n)=p(1-p)^{n-1} \\end{gather} \\] Generation Method \u00b6 CMF \\[ \\begin{gather} F(i) = \\sum\\limits_{n=1}^{i}{p\\,q^{n-1}}=p\\,\\frac{1-q^{i-1}}{1-q}=1-q^{i-1} \\end{gather} \\] let \\[ \\begin{gather} U=G(i) = 1-F(i) = q^{i-1} \\end{gather} \\] thus \\[ \\begin{gather} i-1 = \\log_qU \\\\\\\\ \\implies G^{-1}(U) = \\lfloor {\\log_{q}{U}} \\rfloor + 1 \\end{gather} \\] Poisson Distribution \u00b6 \\(\\text{Pois}(\\lambda)\\) Considering events occur with rate \\(r\\) , with time interval \\(t\\) , there would be average number of events \\(rt\\) per interval. say that \\(\\lambda = rt\\) , expected rate of occurrences Split the time interval \\(t\\) in to \\(N\\) pieces and make the sub-interval \\(t'\\) very small(i.e. \\(N\\) very large). \\(t' = t/N \\text{ where } N \\to \\infty\\quad \\text{then } \\quad r\\,t' \\ll 1\\) if \\(rt' \\ll 1\\) , it approximates to do one Bernoulli trails in each time interval \\(t'\\) . \\(\\text{Pois}(\\lambda) \\equiv B(n=N,\\, p=rt'=\\lambda/N)\\equiv B(n \\to \\infty, p \\to 0)\\) \\(E_{X\\sim Pois}\\big[X\\big] = \\lambda = np = rt\\) \\(X \\sim \\text{Pois}(\\lambda )\\) , indicates \\(X\\) is the number of events occurs in unit time interval (and the event rate is \\(\\lambda\\) ). \\[ \\begin{align} P\\{X=i\\} &= \\binom{n}{i}p^{i}\\,(1-p)^{n-i} \\\\\\\\ &= \\binom{\\cancel{n}}{i} \\frac{\\lambda ^{i}}{\\cancel {n^{i}}}\\,(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(e^{-p})^{n} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}e^{-\\lambda} \\end{align} \\] hint \\[ \\begin{align} \\lim_{x\\to0}e^{x}=e^{0}\\left(1+\\frac{x}{1!}+\\frac{x^{2}}{2!}\\right)\\approx1+x \\end{align} \\] Generation Method \u00b6 Generate several exponential random variable \\(E_i \\sim \\text{Exp}(\\lambda )\\) until \\(E_1 +E_2 + \\cdots + E_i \\ge 1 >E_1 + E_2 + \\cdots + E_{i-1}\\) Exponential Distribution \u00b6 time between events in Poisson process continuous analogue of the geometric distribution . CDF take \\(Y \\sim \\text{Pois}(\\lambda )\\) split unit time interval to \\(1/N\\) , where \\(N\\) is large then \\(p=\\lambda /N\\) \\[ \\begin{gather} P(X \\ge k) = (1-p)^{kN} = e^{-kNp} = e^{-\\lambda k} \\\\\\\\ F(x) = 1-P(X\\ge x) = 1- e^{-\\lambda x} \\end{gather} \\] PDF \\[ \\begin{gather} f(x) = F'(x) = e^{-\\lambda x} \\end{gather} \\] Generation Method \u00b6 Since, \\[ \\begin{gather} F(x) = 1 - e^{-\\lambda x } \\\\\\\\ \\implies F^{-1}(U) = \\frac{\\ln (1-U)}{-\\lambda} \\end{gather} \\] identically, \\[ \\begin{gather} F^{-1}(U) = F^{-1}(1-U) = -\\frac{1}{\\lambda}\\ln U \\end{gather} \\] Beta Distribution \u00b6","title":"Distributions"},{"location":"Probability%20%26%20Statistics/Distributions/#binomial+and+negative+binomial","text":"","title":"Binomial and Negative Binomial"},{"location":"Probability%20%26%20Statistics/Distributions/#binomial+distribution","text":"distribution of number of success \\(r\\) given number of trails \\(n\\) \\[ \\begin{gather} P(X=r)=\\binom{n}{r}p^{r}(1-p)^{n-r} \\end{gather} \\]","title":"Binomial Distribution"},{"location":"Probability%20%26%20Statistics/Distributions/#negative+binomial+distribution","text":"distribution of number of trails \\(n\\) given number of success \\(r\\) namely, distribution of number of failures \\(k\\) given number of success \\(r\\) \\[ \\begin{gather} P(X=n)=\\binom{n-1}{r-1}p^{r}(1-p)^{n-r} \\end{gather} \\]","title":"Negative Binomial Distribution"},{"location":"Probability%20%26%20Statistics/Distributions/#geometric+distribution","text":"number of trails \\(n\\) until success \\[ \\begin{gather} P(X=n)=p(1-p)^{n-1} \\end{gather} \\]","title":"Geometric Distribution"},{"location":"Probability%20%26%20Statistics/Distributions/#generation+method","text":"CMF \\[ \\begin{gather} F(i) = \\sum\\limits_{n=1}^{i}{p\\,q^{n-1}}=p\\,\\frac{1-q^{i-1}}{1-q}=1-q^{i-1} \\end{gather} \\] let \\[ \\begin{gather} U=G(i) = 1-F(i) = q^{i-1} \\end{gather} \\] thus \\[ \\begin{gather} i-1 = \\log_qU \\\\\\\\ \\implies G^{-1}(U) = \\lfloor {\\log_{q}{U}} \\rfloor + 1 \\end{gather} \\]","title":"Generation Method"},{"location":"Probability%20%26%20Statistics/Distributions/#poisson+distribution","text":"\\(\\text{Pois}(\\lambda)\\) Considering events occur with rate \\(r\\) , with time interval \\(t\\) , there would be average number of events \\(rt\\) per interval. say that \\(\\lambda = rt\\) , expected rate of occurrences Split the time interval \\(t\\) in to \\(N\\) pieces and make the sub-interval \\(t'\\) very small(i.e. \\(N\\) very large). \\(t' = t/N \\text{ where } N \\to \\infty\\quad \\text{then } \\quad r\\,t' \\ll 1\\) if \\(rt' \\ll 1\\) , it approximates to do one Bernoulli trails in each time interval \\(t'\\) . \\(\\text{Pois}(\\lambda) \\equiv B(n=N,\\, p=rt'=\\lambda/N)\\equiv B(n \\to \\infty, p \\to 0)\\) \\(E_{X\\sim Pois}\\big[X\\big] = \\lambda = np = rt\\) \\(X \\sim \\text{Pois}(\\lambda )\\) , indicates \\(X\\) is the number of events occurs in unit time interval (and the event rate is \\(\\lambda\\) ). \\[ \\begin{align} P\\{X=i\\} &= \\binom{n}{i}p^{i}\\,(1-p)^{n-i} \\\\\\\\ &= \\binom{\\cancel{n}}{i} \\frac{\\lambda ^{i}}{\\cancel {n^{i}}}\\,(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(e^{-p})^{n} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}e^{-\\lambda} \\end{align} \\] hint \\[ \\begin{align} \\lim_{x\\to0}e^{x}=e^{0}\\left(1+\\frac{x}{1!}+\\frac{x^{2}}{2!}\\right)\\approx1+x \\end{align} \\]","title":"Poisson Distribution"},{"location":"Probability%20%26%20Statistics/Distributions/#generation+method_1","text":"Generate several exponential random variable \\(E_i \\sim \\text{Exp}(\\lambda )\\) until \\(E_1 +E_2 + \\cdots + E_i \\ge 1 >E_1 + E_2 + \\cdots + E_{i-1}\\)","title":"Generation Method"},{"location":"Probability%20%26%20Statistics/Distributions/#exponential+distribution","text":"time between events in Poisson process continuous analogue of the geometric distribution . CDF take \\(Y \\sim \\text{Pois}(\\lambda )\\) split unit time interval to \\(1/N\\) , where \\(N\\) is large then \\(p=\\lambda /N\\) \\[ \\begin{gather} P(X \\ge k) = (1-p)^{kN} = e^{-kNp} = e^{-\\lambda k} \\\\\\\\ F(x) = 1-P(X\\ge x) = 1- e^{-\\lambda x} \\end{gather} \\] PDF \\[ \\begin{gather} f(x) = F'(x) = e^{-\\lambda x} \\end{gather} \\]","title":"Exponential Distribution"},{"location":"Probability%20%26%20Statistics/Distributions/#generation+method_2","text":"Since, \\[ \\begin{gather} F(x) = 1 - e^{-\\lambda x } \\\\\\\\ \\implies F^{-1}(U) = \\frac{\\ln (1-U)}{-\\lambda} \\end{gather} \\] identically, \\[ \\begin{gather} F^{-1}(U) = F^{-1}(1-U) = -\\frac{1}{\\lambda}\\ln U \\end{gather} \\]","title":"Generation Method"},{"location":"Probability%20%26%20Statistics/Distributions/#beta+distribution","text":"","title":"Beta Distribution"},{"location":"Probability%20%26%20Statistics/Mid/","text":"Random Variable \u00b6 The result of experiments (random events) Markov's Inequality \u00b6 for \\(X \\ge 0\\) and \\(a > 0\\) \\[ \\begin{gather} P(X \\ge a) \\le \\frac{E[X]}{a} \\end{gather} \\] Proof \u00b6 Define a lower bound r.v. \\(Y\\) . That is \\[ Y= \\begin{dcases*} a & if $X \\ge a$ \\\\\\\\ 0 & if $0< X < a$ \\end{dcases*} \\] thus, \\[ \\begin{gather} Y \\le X \\end{gather} \\] then, \\[ \\begin{gather} E[X]\\ge E[Y] = aP(X\\ge a) \\\\\\\\ \\implies P(X\\ge a) \\le \\frac{E[X]}{a} \\end{gather} \\] Chebyshev's Inequality \u00b6 for \\(X\\) is r.v. having mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) , then for \\(k > 0\\) , \\[ \\begin{gather} P\\big(|X-\\mu| \\ge k\\sigma\\big) \\le \\frac{1}{k^{2}} \\end{gather} \\] Proof \u00b6 \\[ \\begin{gather} P\\big(|X-\\mu|\\ge k\\sigma\\big)=P\\big((X-\\mu)^{2}\\ge k^{2}\\sigma^{2}\\big) \\end{gather} \\] and by Markov's inequality, we now have, \\[ \\begin{gather} P\\big(|X-\\mu|\\ge k\\sigma\\big)=P\\big((X-\\mu)^{2}\\ge k^{2}\\sigma^{2}\\big) \\le \\frac{E\\big[(X-\\mu)^{2}\\big]}{k^{2}\\sigma^{2}}=\\frac{1}{k^2} \\end{gather} \\] Law of Large Number \u00b6 Weak Law of Large Number \u00b6 Let \\(\\bar{X}=\\frac{1}{n}\\sum\\limits{X_i}\\) , and \\(X_i\\) are i.i.d., and for any \\(\\epsilon > 0\\) \\[ \\begin{gather} P\\bigg(|\\bar X -\\mu| \\ge \\epsilon\\bigg) \\to 0, \\quad \\text{when } n \\to \\infty \\end{gather} \\] Proof \u00b6 First of all, \\[ \\begin{gather} E[\\bar X]=\\mu \\\\\\\\ \\text{Var}(\\bar X) = \\frac{1}{n^{2}}\\sum\\limits_{i} \\text{Var}(X_i) = \\frac{\\sigma^{2}}{n} \\end{gather} \\] since i.i.d., \\(\\text{Cov}(X_i, X_j) = 0\\) then by Chebyshev's inequality, \\[ \\begin{gather} P\\bigg(|\\bar X \\ge \\mu | \\ge k \\frac{\\sigma}{\\sqrt n}\\bigg) \\le \\frac{1}{k^{2}} \\\\\\\\ \\implies P\\bigg(|\\bar X \\ge \\mu | \\ge \\epsilon \\bigg) \\le \\frac{\\sigma^{2}}{n \\epsilon ^{2}} \\end{gather} \\] Central Limit Theorem \u00b6 The sum of samples is also an normal distribution when \\(n\\to \\infty\\) . The sample mean is normal distributed. Let \\(X_1, X_2, \\dots\\) be a sequence of i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) . Then, \\[ \\begin{gather} \\lim_{n\\to \\infty} P\\left(\\frac{\\sum\\limits{X_i}- n\\mu}{\\sigma\\sqrt{n}} < x\\right)=\\Phi(x) \\end{gather} \\] in which \\(\\Phi(x)\\) is cdf of standard normal distribution. Monte Carlo Approach \u00b6 \\[ \\begin{align} \\theta &= \\int_0^{1}{g(x)dx} \\\\\\\\ &= E\\big[g(U)\\big] \\\\\\\\ &= \\lim_{n\\to \\infty}{\\frac{\\sum\\limits^{n} g(U_i)}{n}} \\end{align} \\] Acceptance-Rejection Method \u00b6 required \\[ \\begin{gather} \\frac{f(y)}{c\\,g(y)} \\le 1 \\end{gather} \\] To solve \\(c\\) , find the maximum of \\(\\displaystyle {\\frac{f(y)}{g(y)}}\\) . Then find \\(y^{*}\\) such that \\[ \\begin{gather} y^{*} = \\arg\\max_y \\frac{f(y)}{g(y)} \\\\\\\\ c \\ge \\frac{f(y^{*})}{g(y^{*})} \\end{gather} \\] Normally, we hope \\(c\\) as small as possible (to make the rejection rate lower). Thus we could let \\[ \\begin{gather} \\boxed{ c=\\frac{f(y^{*})}{g(y ^{*})} } \\end{gather} \\] Standard Normal R.V. Example absolute value of Z \\[ \\begin{gather} f(x) = \\frac{\\mathbf 2}{\\sqrt{2\\pi}}e^{-x^{2}/2} \\qquad 0 < x < \\infty \\end{gather} \\] and let say \\(Y\\) be exponential random variable with \\(\\lambda = 1\\) \\[ \\begin{gather} g(x) = e^{- x} \\end{gather} \\] then, \\[ \\begin{gather} \\frac{d}{dx}\\frac{f(x)}{g(x)} = 0 \\\\\\\\ \\implies \\left(-x+1\\right)\\exp\\left(-\\frac{x^{2}}{2}+x\\right) = 0 \\end{gather} \\] thus \\[ \\begin{gather} c =\\frac{f(1)}{g(1)} = \\frac{\\sqrt {2e}}{\\sqrt\\pi} \\end{gather} \\] MM1 queue \u00b6 First of all, with the entering rate \\(\\lambda\\) and exiting rate \\(\\mu\\) , we have the server utilization \\(\\rho\\) \\[ \\begin{gather} \\rho = \\frac{\\lambda}{\\mu} \\end{gather} \\] and the server would be stable iff \\(\\lambda < \\mu\\) so that the \\(\\rho < 1\\) graph LR s0((0)) s1((1)) s2((2)) sn-1((n-1)) sn((n)) sn+1((n+1)) s0 --\"&lambda;\"--> s1 s1 --\"&lambda;\"--> s2 s1 --\"&mu;\"--> s0 s2 --\"&mu;\"--> s1 sn-1 --\"&lambda;\"--> sn sn --\"&lambda;\"--> sn+1 sn --\"&mu;\"--> sn-1 sn+1 --\"&mu;\"--> sn The states transition graph have to follow the rule that the outgoing probability must be equal to ingoing probability. Thus it yields the following equations \\[ \\left\\{ \\begin{gather} \\lambda p_0 =\\mu p_1 \\\\\\\\ (\\lambda +\\mu) p_n = \\lambda p_{n-1} + \\mu p_{n+1} \\end{gather}\\right. \\] in which \\(n \\in \u2115\\) , and then we can get \\(p_2\\) as \\[ \\begin{gather} (\\lambda + \\mu) p_1 = \\lambda p_0 + \\mu p_2 \\\\\\\\ \\implies \\lambda p_1 = \\mu p_2 \\end{gather} \\] Recursively, we would have \\[ \\begin{gather} p_n = \\rho \\,p_{n-1} \\end{gather} \\] and since we known that \\(\\displaystyle {\\sum\\limits_i{p_i}=1}\\) , \\[ \\begin{gather} \\sum\\limits_i{p_i}=p_0\\sum\\limits_i{\\rho^{i}} =p_0\\frac{1}{1-\\rho} =1 \\\\\\\\ \\implies p_0 = 1-\\rho \\\\\\\\ \\implies p_n = (1-\\rho)\\rho^{n} \\end{gather} \\] and thus we know that the stable state probability of number of customers \\(n\\) waiting in queue is a geometric random variable with failure times \\(n\\) . Number of Customers in System \u00b6 \\[ \\begin{align} E[N] &= \\sum\\limits_n{n\\cdot p_n} \\\\\\\\ &= (1-\\rho)\\sum\\limits{n\\rho^{n}} \\\\\\\\ &= (1-\\rho)\\sum\\limits_{i=0}^{\\infty}\\sum\\limits_{j=i}^{\\infty}{\\rho^{j}} \\\\\\\\ &= (1-\\rho)\\frac{\\rho}{(1-\\rho)^{2}} \\\\\\\\ &\\boxed{ = \\frac{\\rho}{1-\\rho} } \\end{align} \\] Average Departure Rate \u00b6 \\[ \\begin{gather} E[N_D] = \\mu\\rho = \\lambda \\end{gather} \\] Waiting Time \u00b6 overall time spent in system \\(W\\) \\[ \\begin{align} E[W] &=\\frac{E[N]}{E[N_D]} \\\\\\\\ &=\\frac{E[N]}{\\lambda} \\\\\\\\ &=\\frac{E[N]/\\mu}{\\rho} \\\\\\\\ &\\boxed{ =\\frac{1}{\\mu(1-\\rho)} } \\end{align} \\] time wait in queue \\(W_q\\) \\[ \\begin{align} E[W_q] &=E[W]-\\frac{1}{\\mu} \\\\\\\\ &=\\frac{1}{\\mu(1-\\rho)}- \\frac{(1-\\rho)}{\\mu(1-\\rho)} \\\\\\\\ &=\\frac{\\rho}{\\mu(1-\\rho)} \\end{align} \\] Sample Stats. \u00b6 Sample Mean \u00b6 denotation \\(\\theta\\) \u2013 popluation mean \\(\\sigma^{2}\\) \u2013 population variance \\(\\bar X\\) \u2013 sample mean \\[ \\begin{align} \\theta &= E[X_i] \\\\\\\\ \\sigma^{2} &= \\text{Var}(X_i) \\\\\\\\ \\bar X &\\equiv \\sum\\limits_{i=1}^{n}{\\frac{X_i}{n}} \\\\\\\\ E[\\bar X] &= E \\left[\\sum\\limits_{i=1}^{n}{\\frac{X_i}{n}}\\right] \\\\\\\\ &= \\frac{1}{n}E \\left[\\sum\\limits_{i=1}^{n}{X_i}\\right] \\\\\\\\ &= \\frac{1}{n}n\\theta = \\theta \\end{align} \\] \\[ \\begin{align} E\\left[\\left(\\bar X - \\theta\\right)^{2}\\right] &= \\text{Var}(\\bar X) \\\\\\\\ &= \\text{Var}\\left(\\sum\\limits_{i=1}^{n}{\\frac{X_i}{n}}\\right) \\\\\\\\ &= \\frac{1}{n^{2}}\\text{Var}\\left(\\sum\\limits_{i=1}^{n}{X_i}\\right) \\\\\\\\ &= \\frac{1}{n^{2}}n\\sigma^{2} =\\frac{\\sigma^{2}}{n} \\end{align} \\] Binomial and Negative Binomial \u00b6 Binomial Distribution \u00b6 distribution of number of success \\(r\\) given number of trails \\(n\\) \\[ \\begin{gather} P(X=r)=\\binom{n}{r}p^{r}(1-p)^{n-r} \\end{gather} \\] Negative Binomial Distribution \u00b6 distribution of number of trails \\(n\\) given number of success \\(r\\) namely, distribution of number of failures \\(k\\) given number of success \\(r\\) \\[ \\begin{gather} P(X=n)=\\binom{n-1}{r-1}p^{r}(1-p)^{n-r} \\end{gather} \\] Geometric Distribution \u00b6 number of trails \\(n\\) until success \\[ \\begin{gather} P(X=n)=p(1-p)^{n-1} \\end{gather} \\] Generation Method \u00b6 CMF \\[ \\begin{gather} F(i) = \\sum\\limits_{n=1}^{i}{p\\,q^{n-1}}=p\\,\\frac{1-q^{i-1}}{1-q}=1-q^{i-1} \\end{gather} \\] let \\[ \\begin{gather} U=G(i) = 1-F(i) = q^{i-1} \\end{gather} \\] thus \\[ \\begin{gather} i-1 = \\log_qU \\\\\\\\ \\implies G^{-1}(U) = \\lfloor {\\log_{q}{U}} \\rfloor + 1 \\end{gather} \\] Poisson Distribution \u00b6 \\(\\text{Pois}(\\lambda)\\) Considering events occur with rate \\(r\\) , with time interval \\(t\\) , there would be average number of events \\(rt\\) per interval. say that \\(\\lambda = rt\\) , expected rate of occurrences Split the time interval \\(t\\) in to \\(N\\) pieces and make the sub-interval \\(t'\\) very small(i.e. \\(N\\) very large). \\(t' = t/N \\text{ where } N \\to \\infty\\quad \\text{then } \\quad r\\,t' \\ll 1\\) if \\(rt' \\ll 1\\) , it approximates to do one Bernoulli trails in each time interval \\(t'\\) . \\(\\text{Pois}(\\lambda) \\equiv B(n=N,\\, p=rt'=\\lambda/N)\\equiv B(n \\to \\infty, p \\to 0)\\) \\(E_{X\\sim Pois}\\big[X\\big] = \\lambda = np = rt\\) \\(X \\sim \\text{Pois}(\\lambda )\\) , indicates \\(X\\) is the number of events occurs in unit time interval (and the event rate is \\(\\lambda\\) ). \\[ \\begin{align} P\\{X=i\\} &= \\binom{n}{i}p^{i}\\,(1-p)^{n-i} \\\\\\\\ &= \\binom{\\cancel{n}}{i} \\frac{\\lambda ^{i}}{\\cancel {n^{i}}}\\,(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(e^{-p})^{n} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}e^{-\\lambda} \\end{align} \\] hint \\[ \\begin{align} \\lim_{x\\to0}e^{x}=e^{0}\\left(1+\\frac{x}{1!}+\\frac{x^{2}}{2!}\\right)\\approx1+x \\end{align} \\] Generation Method \u00b6 Generate several exponential random variable \\(E_i \\sim \\text{Exp}(\\lambda )\\) until \\(E_1 +E_2 + \\cdots + E_i \\ge 1 >E_1 + E_2 + \\cdots + E_{i-1}\\) Exponential Distribution \u00b6 time between events in Poisson process continuous analogue of the geometric distribution . CDF take \\(Y \\sim \\text{Pois}(\\lambda )\\) split unit time interval to \\(1/N\\) , where \\(N\\) is large then \\(p=\\lambda /N\\) \\[ \\begin{gather} P(X \\ge k) = (1-p)^{kN} = e^{-kNp} = e^{-\\lambda k} \\\\\\\\ F(x) = 1-P(X\\ge x) = 1- e^{-\\lambda x} \\end{gather} \\] PDF \\[ \\begin{gather} f(x) = F'(x) = e^{-\\lambda x} \\end{gather} \\] Generation Method \u00b6 Since, \\[ \\begin{gather} F(x) = 1 - e^{-\\lambda x } \\\\\\\\ \\implies F^{-1}(U) = \\frac{\\ln (1-U)}{-\\lambda} \\end{gather} \\] identically, \\[ \\begin{gather} F^{-1}(U) = F^{-1}(1-U) = -\\frac{1}{\\lambda}\\ln U \\end{gather} \\]","title":"Mid"},{"location":"Probability%20%26%20Statistics/Mid/#random+variable","text":"The result of experiments (random events)","title":"Random Variable"},{"location":"Probability%20%26%20Statistics/Mid/#markovs+inequality","text":"for \\(X \\ge 0\\) and \\(a > 0\\) \\[ \\begin{gather} P(X \\ge a) \\le \\frac{E[X]}{a} \\end{gather} \\]","title":"Markov's Inequality"},{"location":"Probability%20%26%20Statistics/Mid/#proof","text":"Define a lower bound r.v. \\(Y\\) . That is \\[ Y= \\begin{dcases*} a & if $X \\ge a$ \\\\\\\\ 0 & if $0< X < a$ \\end{dcases*} \\] thus, \\[ \\begin{gather} Y \\le X \\end{gather} \\] then, \\[ \\begin{gather} E[X]\\ge E[Y] = aP(X\\ge a) \\\\\\\\ \\implies P(X\\ge a) \\le \\frac{E[X]}{a} \\end{gather} \\]","title":"Proof"},{"location":"Probability%20%26%20Statistics/Mid/#chebyshevs+inequality","text":"for \\(X\\) is r.v. having mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) , then for \\(k > 0\\) , \\[ \\begin{gather} P\\big(|X-\\mu| \\ge k\\sigma\\big) \\le \\frac{1}{k^{2}} \\end{gather} \\]","title":"Chebyshev's Inequality"},{"location":"Probability%20%26%20Statistics/Mid/#proof_1","text":"\\[ \\begin{gather} P\\big(|X-\\mu|\\ge k\\sigma\\big)=P\\big((X-\\mu)^{2}\\ge k^{2}\\sigma^{2}\\big) \\end{gather} \\] and by Markov's inequality, we now have, \\[ \\begin{gather} P\\big(|X-\\mu|\\ge k\\sigma\\big)=P\\big((X-\\mu)^{2}\\ge k^{2}\\sigma^{2}\\big) \\le \\frac{E\\big[(X-\\mu)^{2}\\big]}{k^{2}\\sigma^{2}}=\\frac{1}{k^2} \\end{gather} \\]","title":"Proof"},{"location":"Probability%20%26%20Statistics/Mid/#law+of+large+number","text":"","title":"Law of Large Number"},{"location":"Probability%20%26%20Statistics/Mid/#weak+law+of+large+number","text":"Let \\(\\bar{X}=\\frac{1}{n}\\sum\\limits{X_i}\\) , and \\(X_i\\) are i.i.d., and for any \\(\\epsilon > 0\\) \\[ \\begin{gather} P\\bigg(|\\bar X -\\mu| \\ge \\epsilon\\bigg) \\to 0, \\quad \\text{when } n \\to \\infty \\end{gather} \\]","title":"Weak Law of Large Number"},{"location":"Probability%20%26%20Statistics/Mid/#proof_2","text":"First of all, \\[ \\begin{gather} E[\\bar X]=\\mu \\\\\\\\ \\text{Var}(\\bar X) = \\frac{1}{n^{2}}\\sum\\limits_{i} \\text{Var}(X_i) = \\frac{\\sigma^{2}}{n} \\end{gather} \\] since i.i.d., \\(\\text{Cov}(X_i, X_j) = 0\\) then by Chebyshev's inequality, \\[ \\begin{gather} P\\bigg(|\\bar X \\ge \\mu | \\ge k \\frac{\\sigma}{\\sqrt n}\\bigg) \\le \\frac{1}{k^{2}} \\\\\\\\ \\implies P\\bigg(|\\bar X \\ge \\mu | \\ge \\epsilon \\bigg) \\le \\frac{\\sigma^{2}}{n \\epsilon ^{2}} \\end{gather} \\]","title":"Proof"},{"location":"Probability%20%26%20Statistics/Mid/#central+limit+theorem","text":"The sum of samples is also an normal distribution when \\(n\\to \\infty\\) . The sample mean is normal distributed. Let \\(X_1, X_2, \\dots\\) be a sequence of i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) . Then, \\[ \\begin{gather} \\lim_{n\\to \\infty} P\\left(\\frac{\\sum\\limits{X_i}- n\\mu}{\\sigma\\sqrt{n}} < x\\right)=\\Phi(x) \\end{gather} \\] in which \\(\\Phi(x)\\) is cdf of standard normal distribution.","title":"Central Limit Theorem"},{"location":"Probability%20%26%20Statistics/Mid/#monte+carlo+approach","text":"\\[ \\begin{align} \\theta &= \\int_0^{1}{g(x)dx} \\\\\\\\ &= E\\big[g(U)\\big] \\\\\\\\ &= \\lim_{n\\to \\infty}{\\frac{\\sum\\limits^{n} g(U_i)}{n}} \\end{align} \\]","title":"Monte Carlo Approach"},{"location":"Probability%20%26%20Statistics/Mid/#acceptance-rejection+method","text":"required \\[ \\begin{gather} \\frac{f(y)}{c\\,g(y)} \\le 1 \\end{gather} \\] To solve \\(c\\) , find the maximum of \\(\\displaystyle {\\frac{f(y)}{g(y)}}\\) . Then find \\(y^{*}\\) such that \\[ \\begin{gather} y^{*} = \\arg\\max_y \\frac{f(y)}{g(y)} \\\\\\\\ c \\ge \\frac{f(y^{*})}{g(y^{*})} \\end{gather} \\] Normally, we hope \\(c\\) as small as possible (to make the rejection rate lower). Thus we could let \\[ \\begin{gather} \\boxed{ c=\\frac{f(y^{*})}{g(y ^{*})} } \\end{gather} \\] Standard Normal R.V. Example absolute value of Z \\[ \\begin{gather} f(x) = \\frac{\\mathbf 2}{\\sqrt{2\\pi}}e^{-x^{2}/2} \\qquad 0 < x < \\infty \\end{gather} \\] and let say \\(Y\\) be exponential random variable with \\(\\lambda = 1\\) \\[ \\begin{gather} g(x) = e^{- x} \\end{gather} \\] then, \\[ \\begin{gather} \\frac{d}{dx}\\frac{f(x)}{g(x)} = 0 \\\\\\\\ \\implies \\left(-x+1\\right)\\exp\\left(-\\frac{x^{2}}{2}+x\\right) = 0 \\end{gather} \\] thus \\[ \\begin{gather} c =\\frac{f(1)}{g(1)} = \\frac{\\sqrt {2e}}{\\sqrt\\pi} \\end{gather} \\]","title":"Acceptance-Rejection Method"},{"location":"Probability%20%26%20Statistics/Mid/#mm1+queue","text":"First of all, with the entering rate \\(\\lambda\\) and exiting rate \\(\\mu\\) , we have the server utilization \\(\\rho\\) \\[ \\begin{gather} \\rho = \\frac{\\lambda}{\\mu} \\end{gather} \\] and the server would be stable iff \\(\\lambda < \\mu\\) so that the \\(\\rho < 1\\) graph LR s0((0)) s1((1)) s2((2)) sn-1((n-1)) sn((n)) sn+1((n+1)) s0 --\"&lambda;\"--> s1 s1 --\"&lambda;\"--> s2 s1 --\"&mu;\"--> s0 s2 --\"&mu;\"--> s1 sn-1 --\"&lambda;\"--> sn sn --\"&lambda;\"--> sn+1 sn --\"&mu;\"--> sn-1 sn+1 --\"&mu;\"--> sn The states transition graph have to follow the rule that the outgoing probability must be equal to ingoing probability. Thus it yields the following equations \\[ \\left\\{ \\begin{gather} \\lambda p_0 =\\mu p_1 \\\\\\\\ (\\lambda +\\mu) p_n = \\lambda p_{n-1} + \\mu p_{n+1} \\end{gather}\\right. \\] in which \\(n \\in \u2115\\) , and then we can get \\(p_2\\) as \\[ \\begin{gather} (\\lambda + \\mu) p_1 = \\lambda p_0 + \\mu p_2 \\\\\\\\ \\implies \\lambda p_1 = \\mu p_2 \\end{gather} \\] Recursively, we would have \\[ \\begin{gather} p_n = \\rho \\,p_{n-1} \\end{gather} \\] and since we known that \\(\\displaystyle {\\sum\\limits_i{p_i}=1}\\) , \\[ \\begin{gather} \\sum\\limits_i{p_i}=p_0\\sum\\limits_i{\\rho^{i}} =p_0\\frac{1}{1-\\rho} =1 \\\\\\\\ \\implies p_0 = 1-\\rho \\\\\\\\ \\implies p_n = (1-\\rho)\\rho^{n} \\end{gather} \\] and thus we know that the stable state probability of number of customers \\(n\\) waiting in queue is a geometric random variable with failure times \\(n\\) .","title":"MM1 queue"},{"location":"Probability%20%26%20Statistics/Mid/#number+of+customers+in+system","text":"\\[ \\begin{align} E[N] &= \\sum\\limits_n{n\\cdot p_n} \\\\\\\\ &= (1-\\rho)\\sum\\limits{n\\rho^{n}} \\\\\\\\ &= (1-\\rho)\\sum\\limits_{i=0}^{\\infty}\\sum\\limits_{j=i}^{\\infty}{\\rho^{j}} \\\\\\\\ &= (1-\\rho)\\frac{\\rho}{(1-\\rho)^{2}} \\\\\\\\ &\\boxed{ = \\frac{\\rho}{1-\\rho} } \\end{align} \\]","title":"Number of Customers in System"},{"location":"Probability%20%26%20Statistics/Mid/#average+departure+rate","text":"\\[ \\begin{gather} E[N_D] = \\mu\\rho = \\lambda \\end{gather} \\]","title":"Average Departure Rate"},{"location":"Probability%20%26%20Statistics/Mid/#waiting+time","text":"overall time spent in system \\(W\\) \\[ \\begin{align} E[W] &=\\frac{E[N]}{E[N_D]} \\\\\\\\ &=\\frac{E[N]}{\\lambda} \\\\\\\\ &=\\frac{E[N]/\\mu}{\\rho} \\\\\\\\ &\\boxed{ =\\frac{1}{\\mu(1-\\rho)} } \\end{align} \\] time wait in queue \\(W_q\\) \\[ \\begin{align} E[W_q] &=E[W]-\\frac{1}{\\mu} \\\\\\\\ &=\\frac{1}{\\mu(1-\\rho)}- \\frac{(1-\\rho)}{\\mu(1-\\rho)} \\\\\\\\ &=\\frac{\\rho}{\\mu(1-\\rho)} \\end{align} \\]","title":"Waiting Time"},{"location":"Probability%20%26%20Statistics/Mid/#sample+stats","text":"","title":"Sample Stats."},{"location":"Probability%20%26%20Statistics/Mid/#sample+mean","text":"denotation \\(\\theta\\) \u2013 popluation mean \\(\\sigma^{2}\\) \u2013 population variance \\(\\bar X\\) \u2013 sample mean \\[ \\begin{align} \\theta &= E[X_i] \\\\\\\\ \\sigma^{2} &= \\text{Var}(X_i) \\\\\\\\ \\bar X &\\equiv \\sum\\limits_{i=1}^{n}{\\frac{X_i}{n}} \\\\\\\\ E[\\bar X] &= E \\left[\\sum\\limits_{i=1}^{n}{\\frac{X_i}{n}}\\right] \\\\\\\\ &= \\frac{1}{n}E \\left[\\sum\\limits_{i=1}^{n}{X_i}\\right] \\\\\\\\ &= \\frac{1}{n}n\\theta = \\theta \\end{align} \\] \\[ \\begin{align} E\\left[\\left(\\bar X - \\theta\\right)^{2}\\right] &= \\text{Var}(\\bar X) \\\\\\\\ &= \\text{Var}\\left(\\sum\\limits_{i=1}^{n}{\\frac{X_i}{n}}\\right) \\\\\\\\ &= \\frac{1}{n^{2}}\\text{Var}\\left(\\sum\\limits_{i=1}^{n}{X_i}\\right) \\\\\\\\ &= \\frac{1}{n^{2}}n\\sigma^{2} =\\frac{\\sigma^{2}}{n} \\end{align} \\]","title":"Sample Mean"},{"location":"Probability%20%26%20Statistics/Mid/#binomial+and+negative+binomial","text":"","title":"Binomial and Negative Binomial"},{"location":"Probability%20%26%20Statistics/Mid/#binomial+distribution","text":"distribution of number of success \\(r\\) given number of trails \\(n\\) \\[ \\begin{gather} P(X=r)=\\binom{n}{r}p^{r}(1-p)^{n-r} \\end{gather} \\]","title":"Binomial Distribution"},{"location":"Probability%20%26%20Statistics/Mid/#negative+binomial+distribution","text":"distribution of number of trails \\(n\\) given number of success \\(r\\) namely, distribution of number of failures \\(k\\) given number of success \\(r\\) \\[ \\begin{gather} P(X=n)=\\binom{n-1}{r-1}p^{r}(1-p)^{n-r} \\end{gather} \\]","title":"Negative Binomial Distribution"},{"location":"Probability%20%26%20Statistics/Mid/#geometric+distribution","text":"number of trails \\(n\\) until success \\[ \\begin{gather} P(X=n)=p(1-p)^{n-1} \\end{gather} \\]","title":"Geometric Distribution"},{"location":"Probability%20%26%20Statistics/Mid/#generation+method","text":"CMF \\[ \\begin{gather} F(i) = \\sum\\limits_{n=1}^{i}{p\\,q^{n-1}}=p\\,\\frac{1-q^{i-1}}{1-q}=1-q^{i-1} \\end{gather} \\] let \\[ \\begin{gather} U=G(i) = 1-F(i) = q^{i-1} \\end{gather} \\] thus \\[ \\begin{gather} i-1 = \\log_qU \\\\\\\\ \\implies G^{-1}(U) = \\lfloor {\\log_{q}{U}} \\rfloor + 1 \\end{gather} \\]","title":"Generation Method"},{"location":"Probability%20%26%20Statistics/Mid/#poisson+distribution","text":"\\(\\text{Pois}(\\lambda)\\) Considering events occur with rate \\(r\\) , with time interval \\(t\\) , there would be average number of events \\(rt\\) per interval. say that \\(\\lambda = rt\\) , expected rate of occurrences Split the time interval \\(t\\) in to \\(N\\) pieces and make the sub-interval \\(t'\\) very small(i.e. \\(N\\) very large). \\(t' = t/N \\text{ where } N \\to \\infty\\quad \\text{then } \\quad r\\,t' \\ll 1\\) if \\(rt' \\ll 1\\) , it approximates to do one Bernoulli trails in each time interval \\(t'\\) . \\(\\text{Pois}(\\lambda) \\equiv B(n=N,\\, p=rt'=\\lambda/N)\\equiv B(n \\to \\infty, p \\to 0)\\) \\(E_{X\\sim Pois}\\big[X\\big] = \\lambda = np = rt\\) \\(X \\sim \\text{Pois}(\\lambda )\\) , indicates \\(X\\) is the number of events occurs in unit time interval (and the event rate is \\(\\lambda\\) ). \\[ \\begin{align} P\\{X=i\\} &= \\binom{n}{i}p^{i}\\,(1-p)^{n-i} \\\\\\\\ &= \\binom{\\cancel{n}}{i} \\frac{\\lambda ^{i}}{\\cancel {n^{i}}}\\,(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(1-p)^{n-i} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}(e^{-p})^{n} \\\\\\\\ &= \\frac{\\lambda^{i}}{i!}e^{-\\lambda} \\end{align} \\] hint \\[ \\begin{align} \\lim_{x\\to0}e^{x}=e^{0}\\left(1+\\frac{x}{1!}+\\frac{x^{2}}{2!}\\right)\\approx1+x \\end{align} \\]","title":"Poisson Distribution"},{"location":"Probability%20%26%20Statistics/Mid/#generation+method_1","text":"Generate several exponential random variable \\(E_i \\sim \\text{Exp}(\\lambda )\\) until \\(E_1 +E_2 + \\cdots + E_i \\ge 1 >E_1 + E_2 + \\cdots + E_{i-1}\\)","title":"Generation Method"},{"location":"Probability%20%26%20Statistics/Mid/#exponential+distribution","text":"time between events in Poisson process continuous analogue of the geometric distribution . CDF take \\(Y \\sim \\text{Pois}(\\lambda )\\) split unit time interval to \\(1/N\\) , where \\(N\\) is large then \\(p=\\lambda /N\\) \\[ \\begin{gather} P(X \\ge k) = (1-p)^{kN} = e^{-kNp} = e^{-\\lambda k} \\\\\\\\ F(x) = 1-P(X\\ge x) = 1- e^{-\\lambda x} \\end{gather} \\] PDF \\[ \\begin{gather} f(x) = F'(x) = e^{-\\lambda x} \\end{gather} \\]","title":"Exponential Distribution"},{"location":"Probability%20%26%20Statistics/Mid/#generation+method_2","text":"Since, \\[ \\begin{gather} F(x) = 1 - e^{-\\lambda x } \\\\\\\\ \\implies F^{-1}(U) = \\frac{\\ln (1-U)}{-\\lambda} \\end{gather} \\] identically, \\[ \\begin{gather} F^{-1}(U) = F^{-1}(1-U) = -\\frac{1}{\\lambda}\\ln U \\end{gather} \\]","title":"Generation Method"}]}